[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Data Science",
    "section": "",
    "text": "Preface\nThis is the website for the Advanced Data Science.\nThe website for Introduction to Data Science is here.\nThis book started out as part of the class notes used in the HarvardX Data Science Series1.\nA hardcopy version of the first edition of the book is available from CRC Press2.\nA free PDF of the October 24, 2019 version of the book is available from Leanpub3.\nThe Quarto code used to generate the book is available on GitHub4. Note that, the graphical theme used for plots throughout the book can be recreated using the ds_theme_set() function from dslabs package.\nThis work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC BY-NC-SA 4.0.\nWe make announcements related to the book on Twitter. For updates follow @rafalab.\nA special thanks to my tidyverse guru David Robinson and Amy Gill for dozens of comments, edits, and suggestions. Also, many thanks to Stephanie Hicks who twice served as a co-instructor in my data science classes and Yihui Xie who patiently put up with my many questions about bookdown. Thanks also to Héctor Corrada-Bravo, for advice on how to best teach machine learning. Thanks to Alyssa Frazee for helping create the homework problem that became the Recommendation Systems case study. Also, many thanks to Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund for making the Quarto code for their R for Data Science book open. Finally, thanks to Alex Nones for proofreading the manuscript during its various stages.\nThis book was conceived during the teaching of several applied statistics courses, starting over fifteen years ago. The teaching assistants working with me throughout the years made important indirect contributions to this book. The latest iteration of this course is a HarvardX series coordinated by Heather Sternshein and Zofia Gajdos. We thank them for their contributions. We are also grateful to all the students whose questions and comments helped us improve the book. The courses were partially funded by NIH grant R25GM114818. We are very grateful to the National Institutes of Health for its support.\nA special thanks goes to all those who edited the book via GitHub pull requests or made suggestions by creating an issue or sending an email: nickyfoto (Huang Qiang), desautm (Marc-André Désautels), michaschwab (Michail Schwab), alvarolarreategui (Alvaro Larreategui), jakevc (Jake VanCampen), omerta (Guillermo Lengemann), espinielli (Enrico Spinielli), asimumba(Aaron Simumba), braunschweig (Maldewar), gwierzchowski (Grzegorz Wierzchowski), technocrat (Richard Careaga), atzakas, defeit (David Emerson Feit), shiraamitchell (Shira Mitchell), Nathalie-S, andreashandel (Andreas Handel), berkowitze (Elias Berkowitz), Dean-Webb (Dean Webber), mohayusuf, jimrothstein, mPloenzke (Matthew Ploenzke), NicholasDowand (Nicholas Dow), kant (Darío Hereñú), debbieyuster (Debbie Yuster), tuanchauict (Tuan Chau), phzeller, BTJ01 (BradJ), glsnow (Greg Snow), mberlanda (Mauro Berlanda), wfan9, larswestvang (Lars Westvang), jj999 (Jan Andrejkovic), Kriegslustig (Luca Nils Schmid), odahhani, aidanhorn (Aidan Horn), atraxler (Adrienne Traxler), alvegorova,wycheong (Won Young Cheong), med-hat (Medhat Khalil), kengustafson, Yowza63, ryan-heslin (Ryan Heslin), raffaem, tim8west, David D. Kane, El Mustapha El Abbassi, Vadim Zipunnikov, Anna Quaglieri, Chris Dong, Rick Schoenberg, and Isabella Grabski."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Advanced Data Science",
    "section": "",
    "text": "https://www.edx.org/professional-certificate/harvardx-data-science↩︎\nhttps://www.routledge.com/Introduction-to-Data-Science-Data-Analysis-and-Prediction-Algorithms-with/Irizarry/p/book/9780367357986?utm_source=author&utm_medium=shared_link&utm_campaign=B043135_jm1_5ll_6rm_t081_1al_introductiontodatascienceauthorshare↩︎\nhttps://leanpub.com/datasciencebook↩︎\nhttps://github.com/rafalab/dsbook-part-2↩︎"
  },
  {
    "objectID": "intro.html#who-will-find-this-book-useful",
    "href": "intro.html#who-will-find-this-book-useful",
    "title": "Introduction",
    "section": "Who will find this book useful?",
    "text": "Who will find this book useful?\nThis book is meant to be a textbook for a second course in Data Science. Previous knowledge of R, such as that covered in Introduction to Data Science, is necessary. If you read and understand all the chapters and complete all the exercises in this book, you will be well-positioned to perform advanced data analysis tasks and you will be prepared to learn the more advanced concepts and skills needed to become an expert."
  },
  {
    "objectID": "intro.html#what-is-not-covered-by-this-book",
    "href": "intro.html#what-is-not-covered-by-this-book",
    "title": "Introduction",
    "section": "What is not covered by this book?",
    "text": "What is not covered by this book?\nThis book focuses on the application of statistical and machine learning methods in data analysis. We do not go in depth into the theoretical aspects of the methods, and highly recommend complementing this book with probability and statistics textbooks."
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "Introduction",
    "section": "",
    "text": "https://github.com/rafalab/dsbook-part-2↩︎"
  },
  {
    "objectID": "summaries/intro-summaries.html",
    "href": "summaries/intro-summaries.html",
    "title": "Summary statistics",
    "section": "",
    "text": "We start by describing a simple yet powerful data analysis technique: constructing data summaries. Although the approach does not require mathematical models or probability, the motivation for the summaries we describe will later help us understand both these topics.\nYou have likely noticed that numerical data is often summarized with the average value. For example, the quality of a high school is sometimes summarized with one number: the average score on a standardized test. Occasionally, a second number is reported: the standard deviation. For example, you might read a report stating that scores were 680 plus or minus 50, with 50 the standard deviation. The report has summarized the entirety of scores with just two numbers. Is this appropriate? Is there any important piece of information that we are missing by only looking at this summary rather than the entire list? Here we answer these questions and motivate several useful summary statistics and plots, including the average, standard deviation, median, quartiles, histograms, and density plots."
  },
  {
    "objectID": "summaries/distributions.html#variable-types",
    "href": "summaries/distributions.html#variable-types",
    "title": "1  Distributions",
    "section": "\n1.1 Variable types",
    "text": "1.1 Variable types\nWe will be working with two types of variables: categorical and numeric. Each can be divided into two other groups: categorical can be ordinal or not, whereas numerical variables can be discrete or continuous.\nWhen each entry in a vector comes from one of a small number of groups, we refer to the data as categorical data. Two simple examples are sex (male or female) and US regions (Northeast, South, North Central, West). Some categorical data can be ordered even if they are not numbers, such as spiciness (mild, medium, hot). In statistics textbooks, ordered categorical data are referred to as ordinal data.\nExamples of numerical data are population sizes, murder rates, and heights. Some numerical data can be treated as ordered categorical. We can further divide numerical data into continuous and discrete. Continuous variables are those that can take any value, such as heights, if measured with enough precision. For example, a pair of twins may be 68.12 and 68.11 inches, respectively. Counts, such as number of gun murders per year, are discrete because they have to be round numbers.\nKeep in mind that discrete numeric data can be considered ordinal. Although this is technically true, we usually reserve the term ordinal data for variables belonging to a small number of different groups, with each group having many members. In contrast, when we have many groups with few cases in each group, we typically refer to them as discrete numerical variables. So, for example, the number of packs of cigarettes a person smokes a day, rounded to the closest pack, would be considered ordinal, while the actual number of cigarettes would be considered a numerical variable. But, indeed, there are examples that can be considered both numerical and ordinal.\nThe most basic statistical summary of a list of objects or numbers is its distribution. The simplest way to think of a distribution is as a compact description of a list with many entries. This concept should not be new for readers of this book. For example, with categorical data, the distribution simply describes the proportion of each unique category. Here is an example with US state regions:\n\nprop.table(table(state.region))\n#&gt; state.region\n#&gt;     Northeast         South North Central          West \n#&gt;          0.18          0.32          0.24          0.26\n\nWhen the data is numerical, the task of constructing a summary based on the distribution is more challenging. We introduce an artificial, yet illustrative, motivating problem that will help us introduce the concepts needed to understand distributions."
  },
  {
    "objectID": "summaries/distributions.html#sec-ecdf-intro",
    "href": "summaries/distributions.html#sec-ecdf-intro",
    "title": "1  Distributions",
    "section": "\n1.2 Empirical cumulative distribution functions",
    "text": "1.2 Empirical cumulative distribution functions\nNumerical data that are not categorical also have distributions. In general, when data is not categorical, reporting the frequency of each entry is not an effective summary since most entries are unique. In our case study, while several students reported a height of 68 inches, only one student reported a height of 68.503937007874 inches and only one student reported a height 68.8976377952756 inches. We assume that they converted from 174 and 175 centimeters, respectively.\nStatistics textbooks teach us that a more useful way to define a distribution for numeric data is to define a function that reports the proportion of the data entries \\(x\\) that are below \\(a\\), for all possible values of \\(a\\). This function is called the empirical cumulative distribution function (eCDF) and often denoted with \\(F\\):\n\\[ F(a) = \\mbox{Proportion of data points that are less than or equal to }a\\]\nHere is a plot of \\(F\\) for the male height data:\n\nlibrary(tidyverse)\nheights |&gt; filter(sex == \"Male\") |&gt; \n  ggplot(aes(height)) + \n  stat_ecdf() +\n  labs(x = \"a\", y = \"F(a)\")\n\n\n\n\n\n\n\nSimilar to what the frequency table does for categorical data, the eCDF defines the distribution for numerical data. From the plot, we can see that 16% of the values are below 65, since \\(F(66)=\\) 0.1637931, or that 84% of the values are below 72, since \\(F(72)=\\) 0.841133, and so on. In fact, we can report the proportion of values between any two heights, say \\(a\\) and \\(b\\), by computing \\(F(b) - F(a)\\). This means that if we send this plot above to ET, he will have all the information needed to reconstruct the entire list. Paraphrasing the expression “a picture is worth a thousand words”, in this case, a picture is as informative as 812 numbers.\nNote: the reason we add the word empirical is because, as we will see in Section 4.1), the cumulative distribution function (CDF can be defined mathematically, meaning without any data."
  },
  {
    "objectID": "summaries/distributions.html#histograms",
    "href": "summaries/distributions.html#histograms",
    "title": "1  Distributions",
    "section": "\n1.3 Histograms",
    "text": "1.3 Histograms\nAlthough the eCDF concept is widely discussed in statistics textbooks, the summary plot is actually not very popular in practice. The main reason is that it does not easily convey characteristics of interest such as: at what value is the distribution centered? Is the distribution symmetric? What ranges contain 95% of the values? Histograms are much preferred because they greatly facilitate answering such questions. Histograms sacrifice just a bit of information to produce summaries that are much easier to interpret.\nThe simplest way to make a histogram is to divide the span of our data into non-overlapping bins of the same size. Then, for each bin, we count the number of values that fall in that interval. The histogram plots these counts as bars with the base of the bar defined by the intervals. Here is the histogram for the height data splitting the range of values into one inch intervals: \\((49.5, 50.5],(50.5, 51.5],(51.5,52.5],(52.5,53.5],...,(82.5,83.5]\\)\n\nheights |&gt; filter(sex == \"Male\") |&gt; \n  ggplot(aes(height)) + \n  geom_histogram(binwidth = 1, color = \"black\")\n\n\n\n\n\n\n\nAs you can see in the figure above, a histogram is similar to a barplot, but it differs in that the x-axis is numerical, not categorical.\nIf we send this plot to ET, he will immediately learn some important properties about our data. First, the range of the data is from 50 to 84 with the majority (more than 95%) between 63 and 75 inches. Second, the heights are close to symmetric around 69 inches. Also, by adding up counts, ET could obtain a very good approximation of the proportion of the data in any interval. Therefore, the histogram above is not only easy to interpret, but also provides almost all the information contained in the raw list of 812 heights with about 30 bin counts.\nWhat information do we lose? Note that all values in each interval are treated the same when computing bin heights. So, for example, the histogram does not distinguish between 64, 64.1, and 64.2 inches. Given that these differences are almost unnoticeable to the eye, the practical implications are negligible and we were able to summarize the data to just 23 numbers."
  },
  {
    "objectID": "summaries/distributions.html#smoothed-density",
    "href": "summaries/distributions.html#smoothed-density",
    "title": "1  Distributions",
    "section": "\n1.4 Smoothed density",
    "text": "1.4 Smoothed density\nSmooth density plots are similar to histograms, but the data is not divided into bins. Here is what a smooth density plot looks like for our heights data:\n\nheights |&gt; \n  filter(sex == \"Male\") |&gt; \n  ggplot(aes(height)) + \n  geom_density(alpha = 0.2, fill = \"#00BFC4\")\n\n\n\n\n\n\n\nIn this plot, we no longer have sharp edges at the interval boundaries and many of the local peaks have been removed. Also, the scale of the y-axis changed from counts to density.\nTo understand the smooth densities, we have to understand estimates, a topic we don’t cover until later. However, we provide a heuristic explanation to help you understand the basics.\nThe main new concept you must understand is that we assume that our list of observed values is a subset of a much larger list of unobserved values. In the case of heights, you can imagine that our list of 812 male students comes from a hypothetical list containing all the heights of all the male students in all the world measured very precisely. Let’s say there are 1,000,000 of these measurements. This list of values has a distribution, like any list of values, and this larger distribution is really what we want to report to ET since it is much more general. Unfortunately, we don’t get to see it.\nHowever, we make an assumption that helps us perhaps approximate it. If we had 1,000,000 values, measured very precisely, we could make a histogram with very, very small bins. The assumption is that if we show this, the height of consecutive bins will be similar. This is what we mean by smooth: we don’t have big jumps in the heights of consecutive bins. Below we have a hypothetical histogram with bins of size 1:\n\n\n\n\n\n\n\n\nThe smaller we make the bins, the smoother the histogram gets. Here are the histograms with bin width of 1, 0.5, and 0.1:\n\n\n\n\n\n\n\n\nThe smooth density is basically the curve that goes through the top of the histogram bars when the bins are very, very small. To make the curve not depend on the hypothetical size of the hypothetical list, we compute the curve on frequencies rather than counts:\n\n\n\n\n\n\n\n\nNow, back to reality. We don’t have millions of measurements. Instead, we have 812 and we can’t make a histogram with very small bins.\nWe therefore make a histogram, using bin sizes appropriate for our data and computing frequencies rather than counts, and we draw a smooth curve that goes through the tops of the histogram bars. The following plots demonstrate the steps that lead to a smooth density:\n\n\n\n\n\n\n\n\nHowever, remember that smooth is a relative term. We can actually control the smoothness of the curve that defines the smooth density through an option in the function that computes the smooth density curve. Here are two examples using different degrees of smoothness on the same histogram:\n\np &lt;- heights |&gt; filter(sex == \"Male\") |&gt; \n  ggplot(aes(height)) +\n  geom_histogram(aes(y = after_stat(density)), binwidth = 1, alpha = 0.5) \n\np1 &lt;- p +  geom_line(stat = 'density', adjust = 0.5)\np2 &lt;- p +  geom_line(stat = 'density', adjust = 2) \n\nlibrary(gridExtra)\n#&gt; \n#&gt; Attaching package: 'gridExtra'\n#&gt; The following object is masked from 'package:dplyr':\n#&gt; \n#&gt;     combine\ngrid.arrange(p1,p2, ncol = 2)\n\n\n\n\n\n\n\nWe need to make this choice with care as the resulting summary can change our interpretation of the data. We should select a degree of smoothness that we can defend as being representative of the underlying data. In the case of height, we really do have reason to believe that the proportion of people with similar heights should be the same. For example, the proportion that is 72 inches should be more similar to the proportion that is 71 than to the proportion that is 78 or 65. This implies that the curve should be pretty smooth; that is, the curve should look more like the example on the right than on the left.\nWhile the histogram is an assumption-free summary, the smoothed density is based on some assumptions.\nNote that interpreting the y-axis of a smooth density plot is not straightforward. It is scaled so that the area under the density curve adds up to 1. If you imagine we form a bin with a base 1 unit in length, the y-axis value tells us the proportion of values in that bin. However, this is only true for bins of size 1. For other size intervals, the best way to determine the proportion of data in that interval is by computing the proportion of the total area contained in that interval. For example, here are the proportion of values between 65 and 68:\n\n\n\n\n\n\n\n\nThe proportion of this area is about 0.3, meaning that about 30% of male heights are between 65 and 68 inches.\nBy understanding this, we are ready to use the smooth density as a summary. For this dataset, we would feel quite comfortable with the smoothness assumption, and therefore with sharing this aesthetically pleasing figure with ET, which he could use to understand our male heights data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith the material covered up to here, you can do exercises 1 through 10."
  },
  {
    "objectID": "summaries/distributions.html#sec-normal-distribution",
    "href": "summaries/distributions.html#sec-normal-distribution",
    "title": "1  Distributions",
    "section": "\n1.5 The normal distribution",
    "text": "1.5 The normal distribution\nHistograms and density plots provide excellent summaries of a distribution. But can we summarize even further? We often see the average and standard deviation used as summary statistics: a two-number summary! To understand what these summaries are and why they are so widely used, we need to understand the normal distribution.\nThe normal distribution, also known as the bell curve and as the Gaussian distribution, is one of the most famous mathematical concepts in history. A reason for this is that approximately normal distributions occur in many situations, including gambling winnings, heights, weights, blood pressure, standardized test scores, and experimental measurement errors. There are explanations for this, but we describe these later. Here we focus on how the normal distribution helps us summarize data.\nRather than using data, the normal distribution is defined with a mathematical formula. For any interval \\((a,b)\\), the proportion of values in that interval can be computed using this formula:\n\\[\\mbox{Pr}(a &lt; x \\leq b) = \\int_a^b \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{1}{2}\\left( \\frac{x-\\mu}{\\sigma} \\right)^2} \\, dx\\]\nYou don’t need to memorize or understand the details of the formula. But note that it is completely defined by just two parameters: \\(\\mu\\) and \\(\\sigma\\). The rest of the symbols in the formula represent the interval ends, \\(a\\) and \\(b\\), and known mathematical constants \\(\\pi\\) and \\(e\\). These two parameters, \\(\\mu\\) and \\(\\sigma\\), are referred to as the average (also called the mean) and the standard deviation (SD) of the distribution, respectively (and are the greek letters for \\(m\\) and \\(s\\)).\nThe distribution is symmetric, centered at the average, and most values (about 95%) are within 2 SDs from the average. Here is what the normal distribution looks like when the average is 0 and the SD is 1:\n\n\n\n\n\n\n\n\nThe fact that the distribution is defined by just two parameters implies that if a dataset is approximated by a normal distribution, all the information needed to describe the distribution can be encoded in just two numbers: the average and the standard deviation. We now define these values for an arbitrary list of numbers.\nFor a list of numbers contained in a vector x, the average is defined as:\n\nm &lt;- sum(x) / length(x)\n\nand the SD is defined as:\n\ns &lt;- sqrt(sum((x - m)^2) / length(x))\n\nwhich can be interpreted as the average distance between values and their average.\nLet’s compute the values for the height for males which we will store in the object x:\n\nindex &lt;- heights$sex == \"Male\"\nx &lt;- heights$height[index]\n\nThe pre-built functions mean and sd can be used here:\n\nm &lt;- mean(x)\ns &lt;- sd(x)\n\n\n\n\n\n\n\nFor reasons explained in Section Section 10.2.1, sd(x) divides by length(x)-1 rather than length(x). But note that when length(x) is large, sd(x) and sqrt(sum((x-mu)^2) / length(x)) are practically equal.\n\n\n\nHere is a plot of the smooth density and the normal distribution with mean = 69.3 and SD = 3.6 plotted as a black line with our student height smooth density in blue:\n\n\n\n\n\n\n\n\nThe normal distribution does appear to be quite a good approximation here. We now will see how well this approximation works at predicting the proportion of values within intervals."
  },
  {
    "objectID": "summaries/distributions.html#standard-units",
    "href": "summaries/distributions.html#standard-units",
    "title": "1  Distributions",
    "section": "\n1.6 Standard units",
    "text": "1.6 Standard units\nFor data that is approximately normally distributed, it is convenient to think in terms of standard units. The standard unit of a value tells us how many standard deviations away from the average it is. Specifically, for a value x from a vector X, we define the value of x in standard units as z = (x - m)/s with m and s the average and standard deviation of X, respectively. Why is this convenient?\nFirst look back at the formula for the normal distribution and note that what is being exponentiated is \\(-z^2/2\\) with \\(z\\) equivalent to \\(x\\) in standard units. Because the maximum of \\(e^{-z^2/2}\\) is when \\(z = 0\\), this explains why the maximum of the distribution occurs at the average. It also explains the symmetry since \\(- z^2/2\\) is symmetric around 0. Second, note that if we convert the normally distributed data to standard units, we can quickly know if, for example, a person is about average (\\(z = 0\\)), one of the largest (\\(z \\approx 2\\)), one of the smallest (\\(z \\approx -2\\)), or an extremely rare occurrence (\\(z &gt; 3\\) or \\(z &lt; -3\\)). Remember that it does not matter what the original units are, these rules apply to any data that is approximately normal.\nIn R, we can obtain standard units using the function scale:\n\nz &lt;- scale(x)\n\nNow to see how many men are within 2 SDs from the average, we simply type:\n\nmean(abs(z) &lt; 2)\n#&gt; [1] 0.95\n\nThe proportion is about 95%, which is what the normal distribution predicts! To further confirm that, in fact, the approximation is a good one, we can use quantile-quantile plots."
  },
  {
    "objectID": "summaries/distributions.html#quantile-quantile-plots",
    "href": "summaries/distributions.html#quantile-quantile-plots",
    "title": "1  Distributions",
    "section": "\n1.7 Quantile-quantile plots",
    "text": "1.7 Quantile-quantile plots\nA systematic way to assess how well the normal distribution fits the data is to check if the observed and predicted proportions match. In general, this is the approach of the quantile-quantile plot (QQ-plot).\nFirst let’s define the theoretical quantiles for the normal distribution. In statistics books we use the symbol \\(\\Phi(x)\\) to define the function that gives us the proportion of a standard normal distributed data that are smaller than \\(x\\). So, for example, \\(\\Phi(-1.96) = 0.025\\) and \\(\\Phi(1.96) = 0.975\\). In R, we can evaluate \\(\\Phi\\) using the pnorm function:\n\npnorm(-1.96)\n#&gt; [1] 0.025\n\nThe inverse function \\(\\Phi^{-1}(x)\\) gives us the theoretical quantiles for the normal distribution. So, for example, \\(\\Phi^{-1}(0.975) = 1.96\\). In R, we can evaluate the inverse of \\(\\Phi\\) using the qnorm function.\n\nqnorm(0.975)\n#&gt; [1] 1.96\n\nNote that these calculations are for the standard normal distribution by default (mean = 0, standard deviation = 1), but we can also define these for any normal distribution. We can do this using the mean and sd arguments in the pnorm and qnorm function. For example, we can use qnorm to determine quantiles of a distribution with a specific average and standard deviation\n\nqnorm(0.975, mean = 5, sd = 2)\n#&gt; [1] 8.92\n\nFor the normal distribution, all the calculations related to quantiles are done without data, thus the name theoretical quantiles. But quantiles can be defined for any distribution, including an empirical one. So if we have data in a vector \\(x\\), we can define the quantile associated with any proportion \\(p\\) as the \\(q\\) for which the proportion of values below \\(q\\) is \\(p\\). Using R code, we can define q as the value for which mean(x &lt;= q) = p. Notice that not all \\(p\\) have a \\(q\\) for which the proportion is exactly \\(p\\). There are several ways of defining the best \\(q\\) as discussed in the help for the quantile function.\nTo give a quick example, for the male heights data, we have that:\n\nmean(x &lt;= 69.5)\n#&gt; [1] 0.515\n\nSo about 50% are shorter or equal to 69 inches. This implies that if \\(p = 0.50\\) then \\(q = 69.5\\).\nThe idea of a QQ-plot is that if your data is well approximated by normal distribution then the quantiles of your data should be similar to the quantiles of a normal distribution. To construct a QQ-plot, we do the following:\n\nDefine a vector of \\(m\\) proportions \\(p_1, p_2, \\dots, p_m\\).\nDefine a vector of quantiles \\(q_1, \\dots, q_m\\) for your data for the proportions \\(p_1, \\dots, p_m\\). We refer to these as the sample quantiles.\nDefine a vector of theoretical quantiles for the proportions \\(p_1, \\dots, p_m\\) for a normal distribution with the same average and standard deviation as the data.\nPlot the sample quantiles versus the theoretical quantiles.\n\nLet’s construct a QQ-plot using R code. Start by defining the vector of proportions.\n\np &lt;- seq(0.05, 0.95, 0.05)\n\nTo obtain the quantiles from the data, we can use the quantile function like this:\n\nsample_quantiles &lt;- quantile(x, p)\n\nTo obtain the theoretical normal distribution quantiles with the corresponding average and SD, we use the qnorm function:\n\ntheoretical_quantiles &lt;- qnorm(p, mean = mean(x), sd = sd(x))\n\nTo see if they match or not, we plot them against each other and draw the identity line:\n\nqplot(theoretical_quantiles, sample_quantiles) + geom_abline()\n#&gt; Warning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\nNotice that this code becomes much cleaner if we use standard units:\n\nsample_quantiles &lt;- quantile(z, p)\ntheoretical_quantiles &lt;- qnorm(p) \nqplot(theoretical_quantiles, sample_quantiles) + geom_abline()\n\nThe above code is included to help describe QQ-plots. However, in practice it is easier to use ggplot2 code:\n\nheights |&gt; filter(sex == \"Male\") |&gt;\n  ggplot(aes(sample = scale(height))) + \n  geom_qq() +\n  geom_abline()\n\nWhile for the illustration above we used 20 quantiles, the default from the geom_qq function is to use as many quantiles as data points.\nNote that although here we used qqplots to compare an observed distribution to the mathamatically defeinde normal distribution, QQ-plots can be used to compare any two distributions."
  },
  {
    "objectID": "summaries/distributions.html#percentiles",
    "href": "summaries/distributions.html#percentiles",
    "title": "1  Distributions",
    "section": "\n1.8 Percentiles",
    "text": "1.8 Percentiles\nBefore we move on, let’s define some terms that are commonly used in exploratory data analysis.\nPercentiles are special cases of quantiles that are commonly used. The percentiles are the quantiles you obtain when setting the \\(p\\) at \\(0.01, 0.02, ..., 0.99\\). We call, for example, the case of \\(p = 0.25\\) the 25th percentile, which gives us a number for which 25% of the data is below. The most famous percentile is the 50th, also known as the median.\nFor the normal distribution the median and average are the same, but this is generally not the case.\nAnother special case that receives a name are the quartiles, which are obtained when setting \\(p = 0.25,0.50\\), and \\(0.75\\)."
  },
  {
    "objectID": "summaries/distributions.html#boxplots",
    "href": "summaries/distributions.html#boxplots",
    "title": "1  Distributions",
    "section": "\n1.9 Boxplots",
    "text": "1.9 Boxplots\nTo introduce boxplots we will use a dataset of US murders by state. Suppose we want to summarize the murder rate distribution. Using the techniques we have learned, we can quickly see that the normal approximation does not apply here:\n\n\n\n\n\n\n\n\nIn this case, the histogram above or a smooth density plot would serve as a relatively succinct summary.\nNow suppose those used to receiving just two numbers as summaries ask us for a more compact numerical summary.\nThe boxplot provides a five-number summary composed of the range along with the quartiles (the 25th, 50th, and 75th percentiles). The boxplot often ignore outliers when computing the range and instead plot these as independent points. We provide a detailed explanation of outliers later. Finally, he suggested we plot these numbers as a “box” with “whiskers” like this:\n\n\n\n\n\n\n\n\nwith the box defined by the 25% and 75% percentile and the whiskers showing the range. The distance between these two is called the interquartile range. The two points are considered outliers by the default R function we used. The median is shown with a horizontal line. Today, we call these boxplots.\nFrom just this simple plot, we know that the median is about 2.5, that the distribution is not symmetric, and that the range is 0 to 5 for the great majority of states with two exceptions."
  },
  {
    "objectID": "summaries/distributions.html#sec-stratification",
    "href": "summaries/distributions.html#sec-stratification",
    "title": "1  Distributions",
    "section": "\n1.10 Stratification",
    "text": "1.10 Stratification\nIn data analysis we often divide observations into groups based on the values of one or more variables associated with those observations. For example in the next section we divide the height values into groups based on a sex variable: females and males. We call this procedure stratification and refer to the resulting groups as strata.\nStratification is common in data visualization because we are often interested in how the distribution of variables differs across different subgroups.\nUsing the histogram, density plots, and QQ-plots, we have become convinced that the male height data is well approximated with a normal distribution. In this case, we report back to ET a very succinct summary: male heights follow a normal distribution with an average of 69.3 inches and a SD of 3.6 inches. With this information, ET will have a good idea of what to expect when he meets our male students. However, to provide a complete picture we need to also provide a summary of the female heights.\nWe learned that boxplots are useful when we want to quickly compare two or more distributions. Here are the heights for men and women:\n\nheights |&gt; ggplot(aes(sex, height, fill = sex)) + geom_boxplot()\n\n\n\n\n\n\n\nThe plot immediately reveals that males are, on average, taller than females. The standard deviations appear to be similar. But does the normal approximation also work for the female height data collected by the survey? We expect that they will follow a normal distribution, just like males. However, exploratory plots reveal that the approximation is not as useful:\n\n\n\n\n\n\n\n\nWe see something we did not see for the males: the density plot has a second bump. Also, the QQ-plot shows that the highest points tend to be taller than expected by the normal distribution. Finally, we also see five points in the QQ-plot that suggest shorter than expected heights for a normal distribution. When reporting back to ET, we might need to provide a histogram rather than just the average and standard deviation for the female heights.\nWe have noticed what we didn’t expect to see. If we look at other female height distributions, we do find that they are well approximated with a normal distribution. So why are our female students different? Is our class a requirement for the female basketball team? Are small proportions of females claiming to be taller than they are? Another, perhaps more likely, explanation is that in the form students used to enter their heights, Female was the default sex and some males entered their heights, but forgot to change the sex variable. In any case, data visualization has helped discover a potential flaw in our data.\nRegarding the five smallest values, note that these values are:\n\nheights |&gt; filter(sex == \"Female\") |&gt; \n  top_n(5, desc(height)) |&gt;\n  pull(height)\n#&gt; [1] 51 53 55 52 52\n\nBecause these are reported heights, a possibility is that the student meant to enter 5'1\", 5'2\", 5'3\" or 5'5\"."
  },
  {
    "objectID": "summaries/distributions.html#exercises",
    "href": "summaries/distributions.html#exercises",
    "title": "1  Distributions",
    "section": "\n1.11 Exercises",
    "text": "1.11 Exercises\n1. In the murders dataset, the region is a categorical variable and the following is its distribution:\n\n\n\n\n\n\n\n\nTo the closest 5%, what proportion of the states are in the North Central region?\n2. Which of the following is true:\n\nThe graph above is a histogram.\nThe graph above shows only four numbers with a bar plot.\nCategories are not numbers, so it does not make sense to graph the distribution.\nThe colors, not the height of the bars, describe the distribution.\n\n3. The plot below shows the eCDF for male heights:\n\n\n\n\n\n\n\n\nBased on the plot, what percentage of males are shorter than 75 inches?\n\n100%\n95%\n80%\n72 inches\n\n4. To the closest inch, what height m has the property that 1/2 of the male students are taller than m and 1/2 are shorter?\n\n61 inches\n64 inches\n69 inches\n74 inches\n\n5. Here is an eCDF of the murder rates across states:\n\n\n\n\n\n\n\n\nKnowing that there are 51 states (counting DC) and based on this plot, how many states have murder rates larger than 10 per 100,000 people?\n\n1\n5\n10\n50\n\n6. Based on the eCDF above, which of the following statements are true:\n\nAbout half the states have murder rates above 7 per 100,000 and the other half below.\nMost states have murder rates below 2 per 100,000.\nAll the states have murder rates above 2 per 100,000.\nWith the exception of 4 states, the murder rates are below 5 per 100,000.\n\n7. Below is a histogram of male heights in our heights dataset:\n\n\n\n\n\n\n\n\nBased on this plot, how many males are between 63.5 and 65.5?\n\n10\n24\n47\n100\n\n8. About what percentage are shorter than 60 inches?\n\n1%\n10%\n25%\n50%\n\n9. Based on the density plot below, about what proportion of US states have populations larger than 10 million?\n\n\n\n\n\n\n\n\n\n0.02\n0.15\n0.50\n0.55\n\n10. Below are three density plots. Is it possible that they are from the same dataset?\n\n\n\n\n\n\n\n\nWhich of the following statements is true:\n\nIt is impossible that they are from the same dataset.\nThey are from the same dataset, but the plots are different due to code errors.\nThey are the same dataset, but the first and second plot undersmooth and the third oversmooths.\nThey are the same dataset, but the first is not in the log scale, the second undersmooths, and the third oversmooths.\n\n11. Define variables containing the heights of males and females like this:\n\nlibrary(dslabs)\nmale &lt;- heights$height[heights$sex == \"Male\"]\nfemale &lt;- heights$height[heights$sex == \"Female\"]\n\nHow many measurements do we have for each?\n12. Suppose we can’t make a plot and want to compare the distributions side by side. We can’t just list all the numbers. Instead, we will look at the percentiles. Create a five row table showing female_percentiles and male_percentiles with the 10th, 30th, 50th, 70th, & 90th percentiles for each sex. Then create a data frame with these two as columns.\n13. Study the following boxplots showing population sizes by country:\n\n\n\n\n\n\n\n\nWhich continent has the country with the biggest population size?\n14. What continent has the largest median population size?\n15. What is median population size for Africa to the nearest million?\n16. What proportion of countries in Europe have populations below 14 million?\n\n0.99\n0.75\n0.50\n0.25\n\n17. If we use a log transformation, which continent shown above has the largest interquartile range?\n18. Load the height data set and create a vector x with just the male heights:\n\nlibrary(dslabs)\nx &lt;- heights$height[heights$sex==\"Male\"]\n\nWhat proportion of the data is between 69 and 72 inches (taller than 69, but shorter or equal to 72)? Hint: use a logical operator and mean.\n19. Suppose all you know about the data is the average and the standard deviation. Use the normal approximation to estimate the proportion you just calculated. Hint: start by computing the average and standard deviation. Then use the pnorm function to predict the proportions.\n20. Notice that the approximation calculated in question nine is very close to the exact calculation in the first question. Now perform the same task for more extreme values. Compare the exact calculation and the normal approximation for the interval (79,81]. How many times bigger is the actual proportion than the approximation?\n21. Approximate the distribution of adult men in the world as normally distributed with an average of 69 inches and a standard deviation of 3 inches. Using this approximation, estimate the proportion of adult men that are 7 feet tall or taller, referred to as seven footers. Hint: use the pnorm function.\n22. There are about 1 billion men between the ages of 18 and 40 in the world. Use your answer to the previous question to estimate how many of these men (18-40 year olds) are seven feet tall or taller in the world?\n23. There are about 10 National Basketball Association (NBA) players that are 7 feet tall or higher. Using the answer to the previous two questions, what proportion of the world’s 18-to-40-year-old seven footers are in the NBA?\n14. Repeat the calculations performed in the previous question for Lebron James’ height: 6 feet 8 inches. There are about 150 players that are at least that tall.\n25. In answering the previous questions, we found that it is not at all rare for a seven footer to become an NBA player. What would be a fair critique of our calculations:\n\nPractice and talent are what make a great basketball player, not height.\nThe normal approximation is not appropriate for heights.\nAs seen in question 10, the normal approximation tends to underestimate the extreme values. It’s possible that there are more seven footers than we predicted.\nAs seen in question 10, the normal approximation tends to overestimate the extreme values. It’s possible that there are fewer seven footers than we predicted."
  },
  {
    "objectID": "summaries/robust-summaries.html#outliers",
    "href": "summaries/robust-summaries.html#outliers",
    "title": "2  Robust summaries",
    "section": "\n2.1 Outliers",
    "text": "2.1 Outliers\nWe previously described how boxplots show outliers, but we did not provide a precise definition. Here we discuss outliers, approaches that can help detect them, and summaries that take into account their presence.\nOutliers are very common in real-world data anlysis. Data recording can be complex and it is common to observe data points generated in error. For example, an old monitoring device may read out nonsensical measurements before completely failing. Human error is also a source of outliers, in particular when data entry is done manually. An individual, for instance, may mistakenly enter their height in centimeters instead of inches or put the decimal in the wrong place.\nHow do we distinguish an outlier from measurements that were too big or too small simply due to expected variability? This is not always an easy question to answer, but we try to provide some guidance. Let’s begin with a simple case.\nSuppose a colleague is charged with collecting demography data for a group of males. The data report height in feet and are stored in the object:\n\nlibrary(dslabs)\nstr(outlier_example)\n#&gt;  num [1:500] 5.59 5.8 5.54 6.15 5.83 5.54 5.87 5.93 5.89 5.67 ...\n\nOur colleague uses the fact that heights are usually well approximated by a normal distribution and summarizes the data with average and standard deviation\n\nmean(outlier_example)\n#&gt; [1] 6.1\nsd(outlier_example)\n#&gt; [1] 7.8\n\nand writes a report on the interesting fact that this group of males is much taller than usual. The average height is over six feet tall! Using your data analysis skills, however, you notice something else that is unexpected: the standard deviation is over 7 feet. Adding and subtracting two standard deviations, you note that 95% of this population will have heights between -9.4892954, 21.6969354 feet, which does not make sense. A quick plot reveals the problem:\n\nboxplot(outlier_example)\n\n\n\n\n\n\n\n\n\nThere appears to be at least one value that is nonsensical, since we know that a height of 180 feet is impossible. The boxplot detects this point as an outlier."
  },
  {
    "objectID": "summaries/robust-summaries.html#the-median",
    "href": "summaries/robust-summaries.html#the-median",
    "title": "2  Robust summaries",
    "section": "\n2.2 The median",
    "text": "2.2 The median\nWhen we have an outlier like this, the average can become very large. Mathematically, we can make the average as large as we want by simply changing one number: with 500 data points, we can increase the average by any amount \\(\\Delta\\) by adding \\(\\Delta \\times\\) 500 to a single number. The median, defined as the value for which half the values are smaller and the other half are bigger, is robust to such outliers. No matter how large we make the largest point, the median remains the same.\nWith this data the median is:\n\nmedian(outlier_example)\n#&gt; [1] 5.74\n\nwhich is about 5 feet and 9 inches.\nThe median is what boxplots display as a horizontal line."
  },
  {
    "objectID": "summaries/robust-summaries.html#the-inter-quartile-range-iqr",
    "href": "summaries/robust-summaries.html#the-inter-quartile-range-iqr",
    "title": "2  Robust summaries",
    "section": "\n2.3 The inter quartile range (IQR)",
    "text": "2.3 The inter quartile range (IQR)\nThe box in boxplots is defined by the first and third quartile. These are meant to provide an idea of the variability in the data: 50% of the data is within this range. The difference between the 3rd and 1st quartile (or 75th and 25th percentiles) is referred to as the inter quartile range (IQR). As is the case with the median, this quantity will be robust to outliers as large values do not affect it. We can do some math to see that for normally distributed data, the IQR / 1.349 approximates the standard deviation of the data had an outlier not been present. We can see that this works well in our example since we get a standard deviation estimate of:\n\nIQR(outlier_example) / 1.349\n#&gt; [1] 0.245\n\nwhich is about 3 inches."
  },
  {
    "objectID": "summaries/robust-summaries.html#a-data-driven-definition-of-outliers",
    "href": "summaries/robust-summaries.html#a-data-driven-definition-of-outliers",
    "title": "2  Robust summaries",
    "section": "\n2.4 A data-driven definition of outliers",
    "text": "2.4 A data-driven definition of outliers\nIn R, points falling outside the whiskers of the boxplot are referred to as outliers. This definition of outlier was introduced by John Tukey. The top whisker ends at the 75th percentile plus 1.5 \\(\\times\\) IQR. Similarly the bottom whisker ends at the 25th percentile minus 1.5\\(\\times\\) IQR. If we define the first and third quartiles as \\(Q_1\\) and \\(Q_3\\), respectively, then an outlier is anything outside the range:\n\\[[Q_1 - 1.5 \\times (Q_3 - Q1), Q_3 + 1.5 \\times (Q_3 - Q1)].\\]\nWhen the data is normally distributed, the standard units of these values are:\n\nq3 &lt;- qnorm(0.75)\nq1 &lt;- qnorm(0.25)\niqr &lt;- q3 - q1\nr &lt;- c(q1 - 1.5*iqr, q3 + 1.5*iqr)\nr\n#&gt; [1] -2.7  2.7\n\nUsing the pnorm function, we see that 99.3% of the data falls in this interval.\nKeep in mind that this is not such an extreme event: if we have 1,000 data points that are normally distributed, we expect to see about 7 outside of this range. But these would not be outliers since we expect to see them under the typical variation.\nIf we want an outlier to be rarer, we can increase the 1.5 to a larger number. Tukey also used 3 and called these far out outliers. With a normal distribution, 99.9998% of the data falls in this interval. This translates into about 2 in a million chance of being outside the range. In the geom_boxplot function, this can be controlled by the outlier.size argument, which defaults to 1.5.\nThe 180 inches measurement is well beyond the range of the height data:\n\nmax_height &lt;- quantile(outlier_example, 0.75) + 3*IQR(outlier_example)\nmax_height\n#&gt;  75% \n#&gt; 6.91\n\nIf we take this value out, we can see that the data is in fact normally distributed as expected:\n\nx &lt;- outlier_example[outlier_example &lt; max_height]\nqqnorm(x)\nqqline(x)"
  },
  {
    "objectID": "summaries/robust-summaries.html#median-absolute-deviation",
    "href": "summaries/robust-summaries.html#median-absolute-deviation",
    "title": "2  Robust summaries",
    "section": "\n2.5 Median absolute deviation",
    "text": "2.5 Median absolute deviation\nAnother way to robustly estimate the standard deviation in the presence of outliers is to use the median absolute deviation (MAD). To compute the MAD, we first compute the median, and then for each value we compute the distance between that value and the median. The MAD is defined as the median of these distances. For technical reasons not discussed here, this quantity needs to be multiplied by 1.4826 to assure it approximates the actual standard deviation. The mad function already incorporates this correction. For the height data, we get a MAD of:\n\nmad(outlier_example)\n#&gt; [1] 0.237\n\nwhich is about 3 inches."
  },
  {
    "objectID": "summaries/robust-summaries.html#exercises",
    "href": "summaries/robust-summaries.html#exercises",
    "title": "2  Robust summaries",
    "section": "\n2.6 Exercises",
    "text": "2.6 Exercises\nWe are going to use the HistData package. Load the height data set and create a vector x with just the male heights used in Galton’s data on the heights of parents and their children from his historic research on heredity.\n\nlibrary(HistData)\nx &lt;- Galton$child\n\n1. Compute the average and median of these data.\n2. Compute the median and median absolute deviation of these data.\n3. Now suppose Galton made a mistake when entering the first value and forgot to use the decimal point. You can imitate this error by typing:\n\nx_with_error &lt;- x\nx_with_error[1] &lt;- x_with_error[1]*10\n\nHow many inches does the average grow after this mistake?\n4. How many inches does the SD grow after this mistake?\n5. How many inches does the median grow after this mistake?\n6. How many inches does the MAD grow after this mistake?\n7. How could you use exploratory data analysis to detect that an error was made?\n\nSince it is only one value out of many, we will not be able to detect this.\nWe would see an obvious shift in the distribution.\nA boxplot, histogram, or qq-plot would reveal a clear outlier.\nA scatterplot would show high levels of measurement error.\n\n8. How much can the average accidentally grow with mistakes like this? Write a function called error_avg that takes a value k and returns the average of the vector x after the first entry changed to k. Show the results for k=10000 and k=-10000.\n9. Using the murders dataset in the dslabs package. Compute the murder rate for each state. Make a boxplot comparing the murder rates for each region of the United States.\n10. For the same dataset, compute the median and IQR murder rate for each region.\n11. Add a column to the reported_heights with the year the height was entered. You can use the year function in the lubridate package to extract the year from reported_heights$time_stamp). Change the height column from characters to numbers using parse_number from the readr package. Some of the heights will be converted to NA becuase they were incorrectly entetered and include characters, for example 165cm. These heights were supposed to be reported in inches, but many clearly did not. Convert any entry below 54 or above 72 to NA using the na_if function from dplyr. Once you dod this stratify by sex and year and report the percentage of incorrectly entered heights, represented by the NA.\n12. The heights we have been looking at are not the original heights reported by students. The original reported heights are also included in the dslabs package in the object reported_heights. Note that the height column in this data frame is a character and if we try to create a new column with the numeric version\n\nlibrary(tidyverse)  \nreported_heights &lt;- reported_heights |&gt;\n  mutate(original_heights = height, height = as.numeric(height))\n\nwe get a warnings about NAs. Take a look at the rows that result in NAs and describe why this is happeining. Others used centimeters and others were just trolling.\n13. Remove these entries the result in NAs after attempting to convert heights to numbers. Compute the mean, standard deviation, median, and MAD by sex. What do you notice?\n14. Generate boxplots summarizing the heights for males and females and describe what you see.\n15. Look at the largest 10 heights and provide a hypothesis for what you think is happening.\n16. Review all the nonsensical answers by looking at the data considered to be far out by Tukey and comment on the type of errors you see."
  },
  {
    "objectID": "prob/intro-to-prob.html",
    "href": "prob/intro-to-prob.html",
    "title": "Probability",
    "section": "",
    "text": "In games of chance, probability has a very intuitive definition. However, this is not the case in other contexts. Today probability theory is being used much more broadly with the word probability commonly used in everyday language. Google’s auto-complete of “What are the chances of” give us: “having twins”, “rain today”, “getting struck by lightning”, and “getting cancer”. One of the goals of this part of the book is to help us understand how probability is useful to understand and describe real-world events when performing data analysis. Probability theory is useful any time our data is affected by chance in some way. All of the other chapters in this book build upon probability theory. Knowledge of probability is therefore indispensable for most data analysis challenges.\nBecause knowing how to compute probabilities gives you an edge in games of chance, throughout history many smart individuals, including famous mathematicians such as Cardano, Fermat, and Pascal, spent time and energy thinking through the math of these games. As a result, Probability Theory was born. Probability continues to be highly useful in modern games of chance. For example, in poker, we can compute the probability of winning a hand based on the cards on the table. Also, casinos rely on probability theory to develop games that almost certainly guarantee a profit. We will use casino games to illustrate the basic concepts.\nThis part of the book discusses concepts that can be found in many comprehensive books on probability theory. These books delve into the mathematical theories and formulas behind probability.\nHowever, this book takes a different approach. Instead of diving into the mathematical theories, it uses R to demonstrate these concepts. This helps readers visualize and better understand the principles of probability in practical terms, as they can see the results and outcomes by running code.\nDespite this practical approach, the book does not immediately apply these probability concepts to real-world data. This connection between probability theory and real data will be made in a subsequent section or part of the book.\nIn other words, while you’re learning about probability now, it’ll be a bit longer before you see how these concepts relate directly to real datasets."
  },
  {
    "objectID": "prob/discrete-probability.html#relative-frequency",
    "href": "prob/discrete-probability.html#relative-frequency",
    "title": "\n3  Discrete probability\n",
    "section": "\n3.1 Relative frequency",
    "text": "3.1 Relative frequency\nThe word probability is used in everyday language. Answering questions about probability is often hard, if not impossible. Here we discuss a mathematical definition of probability that does permit us to give precise answers to certain questions.\nFor example, if I have 2 red beads and 3 blue beads inside an urn1 (most probability books use this archaic term, so we do too) and I pick one at random, what is the probability of picking a red one? Our intuition tells us that the answer is 2/5 or 40%. A precise definition can be given by noting that there are five possible outcomes of which two satisfy the condition necessary for the event “pick a red bead”. Since each of the five outcomes has the same chance of occurring, we conclude that the probability is .4 for red and .6 for blue.\nA more tangible way to think about the probability of an event is as the proportion of times the event occurs when we repeat the experiment an infinite number of times, independently, and under the same conditions."
  },
  {
    "objectID": "prob/discrete-probability.html#notation",
    "href": "prob/discrete-probability.html#notation",
    "title": "\n3  Discrete probability\n",
    "section": "\n3.2 Notation",
    "text": "3.2 Notation\nWe use the notation \\(\\mbox{Pr}(A)\\) to denote the probability of event \\(A\\) happening. We use the very general term event to refer to things that can happen when something occurs by chance. In our previous example, the event was “picking a red bead”. In a political poll in which we call 100 likely voters at random, an example of an event is “calling 48 Democrats and 52 Republicans”.\nIn data science applications, we will often deal with continuous variables. These events will often be things like “is this person taller than 6 feet”. In this case, we write events in a more mathematical form: \\(X \\geq 6\\). We will see more of these examples later. Here we focus on categorical data."
  },
  {
    "objectID": "prob/discrete-probability.html#probability-distributions",
    "href": "prob/discrete-probability.html#probability-distributions",
    "title": "\n3  Discrete probability\n",
    "section": "\n3.3 Probability distributions",
    "text": "3.3 Probability distributions\nIf we know the relative frequency of the different categories, defining a distribution for categorical outcomes is relatively straightforward. We simply assign a probability to each category. In cases that can be thought of as beads in an urn, for each bead type, their proportion defines the distribution.\nIf we are randomly calling likely voters from a population that is 44% Democrat, 44% Republican, 10% undecided, and 2% Green Party, these proportions define the probability for each group. The probability distribution is:\n\n\nPr(picking a Republican)\n=\n0.44\n\n\nPr(picking a Democrat)\n=\n0.44\n\n\nPr(picking an undecided)\n=\n0.10\n\n\nPr(picking a Green)\n=\n0.02"
  },
  {
    "objectID": "prob/discrete-probability.html#monte-carlo",
    "href": "prob/discrete-probability.html#monte-carlo",
    "title": "\n3  Discrete probability\n",
    "section": "\n3.4 Monte Carlo",
    "text": "3.4 Monte Carlo\nComputers provide a way to actually perform the simple random experiment described above: pick a bead at random from a bag that contains three blue beads and two red ones. Random number generators permit us to mimic the process of picking at random.\nAn example is the sample function in R. We demonstrate its use in the code below. First, we use the function rep to generate the urn:\n\nbeads &lt;- rep(c(\"red\", \"blue\"), times = c(2,3))\nbeads\n#&gt; [1] \"red\"  \"red\"  \"blue\" \"blue\" \"blue\"\n\nand then use sample to pick a bead at random:\n\nsample(beads, 1)\n#&gt; [1] \"blue\"\n\nThis line of code produces one random outcome. We want to repeat this experiment an infinite number of times, but it is impossible to repeat forever. Instead, we repeat the experiment a large enough number of times to make the results practically equivalent to repeating forever. This is an example of a Monte Carlo simulation.\nMuch of what mathematical and theoretical statisticians study, which we do not cover in this book, relates to providing rigorous definitions of “practically equivalent” as well as studying how close a large number of experiments gets us to what happens in the limit. Later in this section, we provide a practical approach to deciding what is “large enough”.\nTo perform our first Monte Carlo simulation, we use the replicate function, which permits us to repeat the same task any number of times. Here, we repeat the random event \\(B =\\) 10,000 times:\n\nB &lt;- 10000\nevents &lt;- replicate(B, sample(beads, 1))\n\nWe can now see if our definition actually is in agreement with this Monte Carlo simulation approximation. We can use table to see the distribution:\n\ntab &lt;- table(events)\ntab\n#&gt; events\n#&gt; blue  red \n#&gt; 6028 3972\n\nand prop.table gives us the proportions:\n\nprop.table(tab)\n#&gt; events\n#&gt;  blue   red \n#&gt; 0.603 0.397\n\nThe numbers above are the estimated probabilities provided by this Monte Carlo simulation. Statistical theory, not covered here, tells us that as \\(B\\) gets larger, the estimates get closer to 3/5=.6 and 2/5=.4.\nAlthough this is a simple and not very useful example, we will use Monte Carlo simulations to estimate probabilities in cases in which it is harder to compute the exact ones. Before delving into more complex examples, we use simple ones to demonstrate the computing tools available in R.\n\n3.4.1 Setting the random seed\nBefore we continue, we will briefly explain the following important line of code:\n\nset.seed(1986) \n\nThroughout this book, we use random number generators. This implies that many of the results presented can actually change by chance, which then suggests that a frozen version of the book may show a different result than what you obtain when you try to code as shown in the book. This is actually fine since the results are random and change from time to time. However, if you want to ensure that results are exactly the same every time you run them, you can set R’s random number generation seed to a specific number. Above we set it to 1986. We want to avoid using the same seed everytime. A popular way to pick the seed is the year - month - day. For example, we picked 1986 on December 20, 2018: \\(2018 - 12 - 20 = 1986\\).\nYou can learn more about setting the seed by looking at the documentation:\n\n?set.seed\n\nIn the exercises, we may ask you to set the seed to assure that the results you obtain are exactly what we expect them to be.\n\n3.4.2 With and without replacement\nThe function sample has an argument that permits us to pick more than one element from the urn. However, by default, this selection occurs without replacement: after a bead is selected, it is not put back in the bag. Notice what happens when we ask to randomly select five beads:\n\nsample(beads, 5)\n#&gt; [1] \"red\"  \"blue\" \"blue\" \"blue\" \"red\"\nsample(beads, 5)\n#&gt; [1] \"red\"  \"red\"  \"blue\" \"blue\" \"blue\"\nsample(beads, 5)\n#&gt; [1] \"blue\" \"red\"  \"blue\" \"red\"  \"blue\"\n\nThis results in rearrangements that always have three blue and two red beads. If we ask that six beads be selected, we get an error:\n\nsample(beads, 6)\n\nError in sample.int(length(x), size, replace, prob) :    cannot take a sample larger than the population when 'replace = FALSE'\nHowever, the sample function can be used directly, without the use of replicate, to repeat the same experiment of picking 1 out of the 5 beads, continually, under the same conditions. To do this, we sample with replacement: return the bead back to the urn after selecting it. We can tell sample to do this by changing the replace argument, which defaults to FALSE, to replace = TRUE:\n\nevents &lt;- sample(beads, B, replace = TRUE)\nprop.table(table(events))\n#&gt; events\n#&gt;  blue   red \n#&gt; 0.602 0.398\n\nNot surprisingly, we get results very similar to those previously obtained with replicate."
  },
  {
    "objectID": "prob/discrete-probability.html#independence",
    "href": "prob/discrete-probability.html#independence",
    "title": "\n3  Discrete probability\n",
    "section": "\n3.5 Independence",
    "text": "3.5 Independence\nWe say two events are independent if the outcome of one does not affect the other. The classic example is coin tosses. Every time we toss a fair coin, the probability of seeing heads is 1/2 regardless of what previous tosses have revealed. The same is true when we pick beads from an urn with replacement. In the example above, the probability of red is 0.40 regardless of previous draws.\nMany examples of events that are not independent come from card games. When we deal the first card, the probability of getting a King is 1/13 since there are thirteen possibilities: Ace, Deuce, Three, \\(\\dots\\), Ten, Jack, Queen, King, and Ace. Now if we deal a King for the first card, and don’t replace it into the deck, the probabilities of a second card being a King is less because there are only three Kings left: the probability is 3 out of 51. These events are therefore not independent: the first outcome affected the next one.\nTo see an extreme case of non-independent events, consider our example of drawing five beads at random without replacement:\n\nx &lt;- sample(beads, 5)\n\nIf you have to guess the color of the first bead, you will predict blue since blue has a 60% chance. But if I show you the result of the last four outcomes:\n\nx[2:5]\n#&gt; [1] \"blue\" \"blue\" \"blue\" \"red\"\n\nwould you still guess blue? Of course not. Now you know that the probability of red is 1 since the only bead left is red. The events are not independent, so the probabilities change."
  },
  {
    "objectID": "prob/discrete-probability.html#conditional-probabilities",
    "href": "prob/discrete-probability.html#conditional-probabilities",
    "title": "\n3  Discrete probability\n",
    "section": "\n3.6 Conditional probabilities",
    "text": "3.6 Conditional probabilities\nWhen events are not independent, conditional probabilities are useful. We already saw an example of a conditional probability: we computed the probability that a second dealt card is a King given that the first was a King. In probability, we use the following notation:\n\\[\n\\mbox{Pr}(\\mbox{Card 2 is a king} \\mid \\mbox{Card 1 is a king}) = 3/51\n\\]\nWe use the \\(\\mid\\) as shorthand for “given that” or “conditional on”.\nWhen two events, say \\(A\\) and \\(B\\), are independent, we have:\n\\[\n\\mbox{Pr}(A \\mid B) = \\mbox{Pr}(A)\n\\]\nThis is the mathematical way of saying: the fact that \\(B\\) happened does not affect the probability of \\(A\\) happening. In fact, this can be considered the mathematical definition of independence."
  },
  {
    "objectID": "prob/discrete-probability.html#addition-and-multiplication-rules",
    "href": "prob/discrete-probability.html#addition-and-multiplication-rules",
    "title": "\n3  Discrete probability\n",
    "section": "\n3.7 Addition and multiplication rules",
    "text": "3.7 Addition and multiplication rules\n\n3.7.1 Multiplication rule\nIf we want to know the probability of two events, say \\(A\\) and \\(B\\), occurring, we can use the multiplication rule:\n\\[\n\\mbox{Pr}(A \\mbox{ and } B) = \\mbox{Pr}(A)\\mbox{Pr}(B \\mid A)\n\\] Let’s use Blackjack as an example. In Blackjack, you are assigned two random cards. After you see what you have, you can ask for more. The goal is to get closer to 21 than the dealer, without going over. Face cards are worth 10 points and Aces are worth 11 or 1 (you choose).\nSo, in a Blackjack game, to calculate the chances of getting a 21 by drawing an Ace and then a face card, we compute the probability of the first being an Ace and multiply by the probability of drawing a face card or a 10 given that the first was an Ace: \\(1/13 \\times 16/51 \\approx 0.025\\)\nThe multiplication rule also applies to more than two events. We can use induction to expand for more events:\n\\[\n\\mbox{Pr}(A \\mbox{ and } B \\mbox{ and } C) = \\mbox{Pr}(A)\\mbox{Pr}(B \\mid A)\\mbox{Pr}(C \\mid A \\mbox{ and } B)\n\\]\n\n3.7.2 Multiplication rule under independence\nWhen we have independent events, then the multiplication rule becomes simpler:\n\\[\n\\mbox{Pr}(A \\mbox{ and } B \\mbox{ and } C) = \\mbox{Pr}(A)\\mbox{Pr}(B)\\mbox{Pr}(C)\n\\]\nBut we have to be very careful before using this since assuming independence can result in very different and incorrect probability calculations when we don’t actually have independence.\nAs an example, imagine a court case in which the suspect was described as having a mustache and a beard. The defendant has a mustache and a beard and the prosecution brings in an “expert” to testify that 1/10 men have beards and 1/5 have mustaches, so using the multiplication rule we conclude that only \\(1/10 \\times 1/5\\) or 0.02 have both.\nBut to multiply like this we need to assume independence! Say the conditional probability of a man having a mustache conditional on him having a beard is .95. So the correct calculation probability is much higher: \\(1/10 \\times 95/100 = 0.095\\).\nThe multiplication rule also gives us a general formula for computing conditional probabilities:\n\\[\n\\mbox{Pr}(B \\mid A) = \\frac{\\mbox{Pr}(A \\mbox{ and } B)}{ \\mbox{Pr}(A)}\n\\]\nTo illustrate how we use these formulas and concepts in practice, we will use several examples related to card games.\n\n3.7.3 Addition rule\nThe addition rule tells us that:\n\\[\n\\mbox{Pr}(A \\mbox{ or } B) = \\mbox{Pr}(A) + \\mbox{Pr}(B) - \\mbox{Pr}(A \\mbox{ and } B)\n\\]\nThis rule is intuitive: think of a Venn diagram. If we simply add the probabilities, we count the intersection twice so we need to substract one instance."
  },
  {
    "objectID": "prob/discrete-probability.html#combinations-and-permutations",
    "href": "prob/discrete-probability.html#combinations-and-permutations",
    "title": "\n3  Discrete probability\n",
    "section": "\n3.8 Combinations and permutations",
    "text": "3.8 Combinations and permutations\nIn our very first example, we imagined an urn with five beads. As a reminder, to compute the probability distribution of one draw, we simply listed out all the possibilities. There were 5 and so then, for each event, we counted how many of these possibilities were associated with the event. The resulting probability of choosing a blue bead is 3/5 because out of the five possible outcomes, three were blue.\nFor more complicated cases, the computations are not as straightforward. For instance, what is the probability that if I draw five cards without replacement, I get all cards of the same suit, what is known as a “flush” in poker? In a discrete probability course you learn theory on how to make these computations. Here we focus on how to use R code to compute the answers.\nFirst, let’s construct a deck of cards. For this, we will use the expand.grid and paste functions. We use paste to create strings by joining smaller strings. To do this, we take the number and suit of a card and create the card name like this:\n\nnumber &lt;- \"Three\"\nsuit &lt;- \"Hearts\"\npaste(number, suit)\n#&gt; [1] \"Three Hearts\"\n\npaste also works on pairs of vectors performing the operation element-wise:\n\npaste(letters[1:5], as.character(1:5))\n#&gt; [1] \"a 1\" \"b 2\" \"c 3\" \"d 4\" \"e 5\"\n\nThe function expand.grid gives us all the combinations of entries of two vectors. For example, if you have blue and black pants and white, grey, and plaid shirts, all your combinations are:\n\nexpand.grid(pants = c(\"blue\", \"black\"), shirt = c(\"white\", \"grey\", \"plaid\"))\n#&gt;   pants shirt\n#&gt; 1  blue white\n#&gt; 2 black white\n#&gt; 3  blue  grey\n#&gt; 4 black  grey\n#&gt; 5  blue plaid\n#&gt; 6 black plaid\n\nHere is how we generate a deck of cards:\n\nsuits &lt;- c(\"Diamonds\", \"Clubs\", \"Hearts\", \"Spades\")\nnumbers &lt;- c(\"Ace\", \"Deuce\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\", \n             \"Eight\", \"Nine\", \"Ten\", \"Jack\", \"Queen\", \"King\")\ndeck &lt;- expand.grid(number = numbers, suit = suits)\ndeck &lt;- paste(deck$number, deck$suit)\n\nWith the deck constructed, we can double check that the probability of a King in the first card is 1/13 by computing the proportion of possible outcomes that satisfy our condition:\n\nkings &lt;- paste(\"King\", suits)\nmean(deck %in% kings)\n#&gt; [1] 0.0769\n\nNow, how about the conditional probability of the second card being a King given that the first was a King? Earlier, we deduced that if one King is already out of the deck and there are 51 left, then this probability is 3/51. Let’s confirm by listing out all possible outcomes.\nTo do this, we can use the permutations function from the gtools package. For any list of size n, this function computes all the different combinations we can get when we select r items. Here are all the ways we can choose two numbers from a list consisting of 1,2,3:\n\nlibrary(gtools)\npermutations(3, 2)\n#&gt;      [,1] [,2]\n#&gt; [1,]    1    2\n#&gt; [2,]    1    3\n#&gt; [3,]    2    1\n#&gt; [4,]    2    3\n#&gt; [5,]    3    1\n#&gt; [6,]    3    2\n\nNotice that the order matters here: 3,1 is different than 1,3. Also, note that (1,1), (2,2), and (3,3) do not appear because once we pick a number, it can’t appear again.\nOptionally, we can add a vector. If you want to see five random seven digit phone numbers out of all possible phone numbers (without repeats), you can type:\n\nall_phone_numbers &lt;- permutations(10, 7, v = 0:9)\nn &lt;- nrow(all_phone_numbers)\nindex &lt;- sample(n, 5)\nall_phone_numbers[index,]\n#&gt;      [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n#&gt; [1,]    1    3    8    0    6    7    5\n#&gt; [2,]    2    9    1    6    4    8    0\n#&gt; [3,]    5    1    6    0    9    8    2\n#&gt; [4,]    7    4    6    0    2    8    1\n#&gt; [5,]    4    6    5    9    2    8    0\n\nInstead of using the numbers 1 through 10, the default, it uses what we provided through v: the digits 0 through 9.\nTo compute all possible ways we can choose two cards when the order matters, we type:\n\nhands &lt;- permutations(52, 2, v = deck)\n\nThis is a matrix with two columns and 2652 rows. With a matrix we can get the first and second cards like this:\n\nfirst_card &lt;- hands[,1]\nsecond_card &lt;- hands[,2]\n\nNow the cases for which the first hand was a King can be computed like this:\n\nkings &lt;- paste(\"King\", suits)\nsum(first_card %in% kings)\n#&gt; [1] 204\n\nTo get the conditional probability, we compute what fraction of these have a King in the second card:\n\nsum(first_card %in% kings & second_card %in% kings) / \n  sum(first_card %in% kings)\n#&gt; [1] 0.0588\n\nwhich is exactly 3/51, as we had already deduced. Notice that the code above is equivalent to:\n\nmean(first_card %in% kings & second_card %in% kings) / \n  mean(first_card %in% kings)\n#&gt; [1] 0.0588\n\nwhich uses mean instead of sum and is an R version of:\n\\[\n\\frac{\\mbox{Pr}(A \\mbox{ and } B)}{ \\mbox{Pr}(A)}\n\\]\nHow about if the order doesn’t matter? For example, in Blackjack if you get an Ace and a face card in the first draw, it is called a Natural 21 and you win automatically. If we wanted to compute the probability of this happening, we would enumerate the combinations, not the permutations, since the order does not matter.\n\ncombinations(3,2)\n#&gt;      [,1] [,2]\n#&gt; [1,]    1    2\n#&gt; [2,]    1    3\n#&gt; [3,]    2    3\n\nIn the second line, the outcome does not include (2,1) because (1,2) already was enumerated. The same applies to (3,1) and (3,2).\nSo to compute the probability of a Natural 21 in Blackjack, we can do this:\n\naces &lt;- paste(\"Ace\", suits)\n\nfacecard &lt;- c(\"King\", \"Queen\", \"Jack\", \"Ten\")\nfacecard &lt;- expand.grid(number = facecard, suit = suits)\nfacecard &lt;- paste(facecard$number, facecard$suit)\n\nhands &lt;- combinations(52, 2, v = deck)\nmean(hands[,1] %in% aces & hands[,2] %in% facecard)\n#&gt; [1] 0.0483\n\nIn the last line, we assume the Ace comes first. This is only because we know the way combination enumerates possibilities and it will list this case first. But to be safe, we could have written this and produced the same answer:\n\nmean((hands[,1] %in% aces & hands[,2] %in% facecard) |\n       (hands[,2] %in% aces & hands[,1] %in% facecard))\n#&gt; [1] 0.0483\n\n\n3.8.1 Monte Carlo example\nInstead of using combinations to deduce the exact probability of a Natural 21, we can use a Monte Carlo to estimate this probability. In this case, we draw two cards over and over and keep track of how many 21s we get. We can use the function sample to draw two cards without replacements:\n\nhand &lt;- sample(deck, 2)\nhand\n#&gt; [1] \"Queen Clubs\"  \"Seven Spades\"\n\nAnd then check if one card is an Ace and the other a face card or a 10. Going forward, we include 10 when we say face card. Now we need to check both possibilities:\n\n(hands[1] %in% aces & hands[2] %in% facecard) | \n  (hands[2] %in% aces & hands[1] %in% facecard)\n#&gt; [1] FALSE\n\nIf we repeat this 10,000 times, we get a very good approximation of the probability of a Natural 21.\nLet’s start by writing a function that draws a hand and returns TRUE if we get a 21. The function does not need any arguments because it uses objects defined in the global environment.\n\nblackjack &lt;- function(){\n   hand &lt;- sample(deck, 2)\n  (hand[1] %in% aces & hand[2] %in% facecard) | \n    (hand[2] %in% aces & hand[1] %in% facecard)\n}\n\nHere we do have to check both possibilities: Ace first or Ace second because we are not using the combinations function. The function returns TRUE if we get a 21 and FALSE otherwise:\n\nblackjack()\n#&gt; [1] FALSE\n\nNow we can play this game, say, 10,000 times:\n\nB &lt;- 10000\nresults &lt;- replicate(B, blackjack())\nmean(results)\n#&gt; [1] 0.0475"
  },
  {
    "objectID": "prob/discrete-probability.html#examples",
    "href": "prob/discrete-probability.html#examples",
    "title": "\n3  Discrete probability\n",
    "section": "\n3.9 Examples",
    "text": "3.9 Examples\nIn this section, we describe two discrete probability popular examples: the Monty Hall problem and the birthday problem. We use R to help illustrate the mathematical concepts.\n\n3.9.1 Monty Hall problem\nIn the 1970s, there was a game show called “Let’s Make a Deal” and Monty Hall was the host. At some point in the game, contestants were asked to pick one of three doors. Behind one door there was a prize. The other doors had a goat behind them to show the contestant they had lost. After the contestant picked a door, before revealing whether the chosen door contained a prize, Monty Hall would open one of the two remaining doors and show the contestant there was no prize behind that door. Then he would ask “Do you want to switch doors?” What would you do?\nWe can use probability to show that if you stick with the original door choice, your chances of winning a prize remain 1 in 3. However, if you switch to the other door, your chances of winning double to 2 in 3! This seems counterintuitive. Many people incorrectly think both chances are 1 in 2 since you are choosing between 2 options. You can watch a detailed mathematical explanation on Khan Academy2 or read one on Wikipedia3. Below we use a Monte Carlo simulation to see which strategy is better. Note that this code is written longer than it should be for pedagogical purposes.\nLet’s start with the stick strategy:\n\nB &lt;- 10000\nmonty_hall &lt;- function(strategy){\n  doors &lt;- as.character(1:3)\n  prize &lt;- sample(c(\"car\", \"goat\", \"goat\"))\n  prize_door &lt;- doors[prize == \"car\"]\n  my_pick  &lt;- sample(doors, 1)\n  show &lt;- sample(doors[!doors %in% c(my_pick, prize_door)],1)\n  stick &lt;- my_pick\n  stick == prize_door\n  switch &lt;- doors[!doors%in%c(my_pick, show)]\n  choice &lt;- ifelse(strategy == \"stick\", stick, switch)\n  choice == prize_door\n}\nstick &lt;- replicate(B, monty_hall(\"stick\"))\nmean(stick)\n#&gt; [1] 0.342\nswitch &lt;- replicate(B, monty_hall(\"switch\"))\nmean(switch)\n#&gt; [1] 0.668\n\nAs we write the code, we note that the lines starting with my_pick and show have no influence on the last logical operation when we stick to our original choice anyway. From this we should realize that the chance is 1 in 3, what we began with. When we switch, the Monte Carlo estimate confirms the 2/3 calculation. This helps us gain some insight by showing that we are removing a door, show, that is definitely not a winner from our choices. We also see that unless we get it right when we first pick, you win: 1 - 1/3 = 2/3.\n\n3.9.2 Birthday problem\nSuppose you are in a classroom with 50 people. If we assume this is a randomly selected group of 50 people, what is the chance that at least two people have the same birthday? Although it is somewhat advanced, we can deduce this mathematically. We will do this later. Here we use a Monte Carlo simulation. For simplicity, we assume nobody was born on February 29. This actually doesn’t change the answer much.\nFirst, note that birthdays can be represented as numbers between 1 and 365, so a sample of 50 birthdays can be obtained like this:\n\nn &lt;- 50\nbdays &lt;- sample(1:365, n, replace = TRUE)\n\nTo check if in this particular set of 50 people we have at least two with the same birthday, we can use the function duplicated, which returns TRUE whenever an element of a vector is a duplicate. Here is an example:\n\nduplicated(c(1, 2, 3, 1, 4, 3, 5))\n#&gt; [1] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE\n\nThe second time 1 and 3 appear, we get a TRUE. So to check if two birthdays were the same, we simply use the any and duplicated functions like this:\n\nany(duplicated(bdays))\n#&gt; [1] TRUE\n\nIn this case, we see that it did happen. At least two people had the same birthday.\nTo estimate the probability of a shared birthday in the group, we repeat this experiment by sampling sets of 50 birthdays over and over:\n\nB &lt;- 10000\nsame_birthday &lt;- function(n){\n  bdays &lt;- sample(1:365, n, replace = TRUE)\n  any(duplicated(bdays))\n}\nresults &lt;- replicate(B, same_birthday(50))\nmean(results)\n#&gt; [1] 0.969\n\nWere you expecting the probability to be this high?\nPeople tend to underestimate these probabilities. To get an intuition as to why it is so high, think about what happens when the group size is close to 365. At this stage, we run out of days and the probability is one.\nSay we want to use this knowledge to bet with friends about two people having the same birthday in a group of people. When are the chances larger than 50%? Larger than 75%?\nLet’s create a look-up table. We can quickly create a function to compute this for any group size:\n\ncompute_prob &lt;- function(n, B = 10000){\n  results &lt;- replicate(B, same_birthday(n))\n  mean(results)\n}\n\nUsing the function sapply, we can perform element-wise operations on any function:\n\nn &lt;- seq(1,60)\nprob &lt;- sapply(n, compute_prob)\n\nWe can now make a plot of the estimated probabilities of two people having the same birthday in a group of size \\(n\\):\n\nlibrary(tidyverse)\nprob &lt;- sapply(n, compute_prob)\nqplot(n, prob)\n\n\n\n\n\n\n\nNow let’s compute the exact probabilities rather than use Monte Carlo approximations. Not only do we get the exact answer using math, but the computations are much faster since we don’t have to generate experiments.\nTo make the math simpler, instead of computing the probability of it happening, we will compute the probability of it not happening. For this, we use the multiplication rule.\nLet’s start with the first person. The probability that person 1 has a unique birthday is 1. The probability that person 2 has a unique birthday, given that person 1 already took one, is 364/365. Then, given that the first two people have unique birthdays, person 3 is left with 363 days to choose from. We continue this way and find the chances of all 50 people having a unique birthday is:\n\\[\n1 \\times \\frac{364}{365}\\times\\frac{363}{365} \\dots \\frac{365-n + 1}{365}\n\\]\nWe can write a function that does this for any number:\n\nexact_prob &lt;- function(n){\n  prob_unique &lt;- seq(365,365 - n + 1)/365 \n  1 - prod( prob_unique)\n}\neprob &lt;- sapply(n, exact_prob)\nqplot(n, prob) + geom_line(aes(n, eprob), col = \"red\")\n\n\n\n\n\n\n\nThis plot shows that the Monte Carlo simulation provided a very good estimate of the exact probability. Had it not been possible to compute the exact probabilities, we would have still been able to accurately estimate the probabilities."
  },
  {
    "objectID": "prob/discrete-probability.html#infinity-in-practice",
    "href": "prob/discrete-probability.html#infinity-in-practice",
    "title": "\n3  Discrete probability\n",
    "section": "\n3.10 Infinity in practice",
    "text": "3.10 Infinity in practice\nThe theory described here requires repeating experiments over and over forever. In practice we can’t do this. In the examples above, we used \\(B=10,000\\) Monte Carlo experiments and it turned out that this provided accurate estimates. The larger this number, the more accurate the estimate becomes until the approximaton is so good that your computer can’t tell the difference. But in more complex calculations, 10,000 may not be nearly enough. Also, for some calculations, 10,000 experiments might not be computationally feasible. In practice, we won’t know what the answer is, so we won’t know if our Monte Carlo estimate is accurate. We know that the larger \\(B\\), the better the approximation. But how big do we need it to be? This is actually a challenging question and answering it often requires advanced theoretical statistics training.\nOne practical approach we will describe here is to check for the stability of the estimate. The following is an example with the birthday problem for a group of 25 people.\n\nB &lt;- 10^seq(1, 5, len = 100)\ncompute_prob &lt;- function(B, n=25){\n  same_day &lt;- replicate(B, same_birthday(n))\n  mean(same_day)\n}\nprob &lt;- sapply(B, compute_prob)\nplot(log10(B), prob)\n\n\n\n\n\n\n\nIn this plot, we can see that the values start to stabilize (that is, they vary less than .01) around 1000. Note that the exact probability, which we know in this case, is 0.5686997."
  },
  {
    "objectID": "prob/discrete-probability.html#exercises",
    "href": "prob/discrete-probability.html#exercises",
    "title": "\n3  Discrete probability\n",
    "section": "\n3.11 Exercises",
    "text": "3.11 Exercises\n1. One ball will be drawn at random from a box containing: 3 cyan balls, 5 magenta balls, and 7 yellow balls. What is the probability that the ball will be cyan?\n2. What is the probability that the ball will not be cyan?\n3. Instead of taking just one draw, consider taking two draws. You take the second draw without returning the first draw to the box. We call this sampling without replacement. What is the probability that the first draw is cyan and that the second draw is not cyan?\n4. Now repeat the experiment, but this time, after taking the first draw and recording the color, return it to the box and shake the box. We call this sampling with replacement. What is the probability that the first draw is cyan and that the second draw is not cyan?\n5. Two events \\(A\\) and \\(B\\) are independent if \\(\\mbox{Pr}(A \\mbox{ and } B) = \\mbox{Pr}(A) P(B)\\). Under which situation are the draws independent?\n\nYou don’t replace the draw.\nYou replace the draw.\nNeither\nBoth\n\n6. Say you’ve drawn 5 balls from the box, with replacement, and all have been yellow. What is the probability that the next one is yellow?\n7. If you roll a 6-sided die six times, what is the probability of not seeing a 6?\n8. Two teams, say the Celtics and the Cavs, are playing a seven game series. The Cavs are a better team and have a 60% chance of winning each game. What is the probability that the Celtics win at least one game?\n9. Create a Monte Carlo simulation to confirm your answer to the previous problem. Use B &lt;- 10000 simulations. Hint: use the following code to generate the results of the first four games:\n\nceltic_wins &lt;- sample(c(0,1), 4, replace = TRUE, prob = c(0.6, 0.4))\n\nThe Celtics must win one of these 4 games.\n10. Two teams, say the Cavs and the Warriors, are playing a seven game championship series. The first to win four games, therefore, wins the series. The teams are equally good so they each have a 50-50 chance of winning each game. If the Cavs lose the first game, what is the probability that they win the series?\n11. Confirm the results of the previous question with a Monte Carlo simulation.\n12. Two teams, \\(A\\) and \\(B\\), are playing a seven game series. Team \\(A\\) is better than team \\(B\\) and has a \\(p&gt;0.5\\) chance of winning each game. Given a value \\(p\\), the probability of winning the series for the underdog team \\(B\\) can be computed with the following function based on a Monte Carlo simulation:\n\nprob_win &lt;- function(p){\n  B &lt;- 10000\n  result &lt;- replicate(B, {\n    b_win &lt;- sample(c(1,0), 7, replace = TRUE, prob = c(1-p, p))\n    sum(b_win)&gt;=4\n  })\n  mean(result)\n}\n\nUse the function sapply to compute the probability, call it Pr, of winning for p &lt;- seq(0.5, 0.95, 0.025). Then plot the result.\n13. Repeat the exercise above, but now keep the probability fixed at p &lt;- 0.75 and compute the probability for different series lengths: best of 1 game, 3 games, 5 games,… Specifically, N &lt;- seq(1, 25, 2). Hint: use this function:\n\nprob_win &lt;- function(N, p=0.75){\n  B &lt;- 10000\n  result &lt;- replicate(B, {\n    b_win &lt;- sample(c(1,0), N, replace = TRUE, prob = c(1-p, p))\n    sum(b_win)&gt;=(N+1)/2\n  })\n  mean(result)\n}"
  },
  {
    "objectID": "prob/discrete-probability.html#footnotes",
    "href": "prob/discrete-probability.html#footnotes",
    "title": "\n3  Discrete probability\n",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Urn_problem↩︎\nhttps://www.khanacademy.org/math/precalculus/prob-comb/dependent-events-precalc/v/monty-hall-problem↩︎\nhttps://en.wikipedia.org/wiki/Monty_Hall_problem↩︎"
  },
  {
    "objectID": "prob/continuous-probability.html#sec-cdf-intro",
    "href": "prob/continuous-probability.html#sec-cdf-intro",
    "title": "\n4  Continuous probability\n",
    "section": "\n4.1 Cumulative distribution functions",
    "text": "4.1 Cumulative distribution functions\nWe used the heights of adult male students as an example\n\nlibrary(tidyverse)\nlibrary(dslabs)\nx &lt;- heights %&gt;% filter(sex==\"Male\") %&gt;% pull(height)\n\nand defined the empirical cumulative distribution function (eCDF) as\n\nF &lt;- function(a) mean(x&lt;=a)\n\nwhich, for any value a, gives the proportion of values in the list x that are smaller or equal than a.\nLet’s connect the eCDF to probability by asking: if I pick one of the male students at random, what is the chance that he is taller than 70.5 inches? Because every student has the same chance of being picked, the answer to this is equivalent to the proportion of students that are taller than 70.5 inches. Using the eCDF we obtain an answer by typing:\n\n1 - F(70)\n#&gt; [1] 0.377\n\nThe CDF is a version of the eCDF that assigns theoretical probabilities for each \\(a\\) rather than proportions computed from data. Although, as we just demonstrated, proportions computed from data can be used to define probabilities for a random variable. Specifically, the CDF for a random outcome \\(X\\) defines, for any number \\(a\\), the probability of observing a value larger than \\(a\\).\n\\[ F(a) = \\mbox{Pr}(X \\leq a) \\]\nOnce a CDF is defined, we can use it to compute the probability of any subset of values. For instance, the probability of a student being between height a and height b is:\n\\[\n\\mbox{Pr}(a &lt; X \\leq b) = F(b)-F(a)\n\\]\nBecause we can compute the probability for any possible event this way, the CDF defines the probability distribution."
  },
  {
    "objectID": "prob/continuous-probability.html#probability-density-function",
    "href": "prob/continuous-probability.html#probability-density-function",
    "title": "\n4  Continuous probability\n",
    "section": "\n4.2 Probability density function",
    "text": "4.2 Probability density function\nA mathematical result that is actually very useful in practice is that for most CDFs we can define a function, call it \\(f(x)\\), that permits us to construct the CDF using Calculus, like this:\n\\[\nF(b) - F(a) = \\int_a^b f(x)\\,dx\n\\] \\(f(x)\\) is referred to as the probability density function. The intuition is that even for continuous outcomes we can define tiny intervals, that are almost as small as points, that have positive probabilities. If we think of the size of these intervals as the base of a rectangle,the probability density function \\(f\\) determines the height of the rectangle so that the summing up the area of these rectangles approximate the probability \\(F(b) - F(a)\\). This sum can be written as Reimann sum that is approximated by an integral:\n\n\n\n\n\n\n\n\nAn example of such a continuous distribution is the normal distribution. As we saw in Section 1.5, the probability density function is given by:\n\\[f(x) = e^{-\\frac{1}{2}\\left( \\frac{x-m}{s} \\right)^2} \\]\nThe cumulative distribution for the normal distribution is defined by a mathematical formula which in R can be obtained with the function pnorm. We say that a random quantity is normally distributed with average m and standard deviation s if its probability distribution is defined by:\n\nF(a) = pnorm(a, m, s)\n\nThis is useful because if we are willing to use the normal approximation for, say, height, we don’t need the entire dataset to answer questions such as: what is the probability that a randomly selected student is taller then 70 inches? We just need the average height and standard deviation:\n\nm &lt;- mean(x)\ns &lt;- sd(x)\n1 - pnorm(70.5, m, s)\n#&gt; [1] 0.371"
  },
  {
    "objectID": "prob/continuous-probability.html#theoretical-distributions-as-approximations",
    "href": "prob/continuous-probability.html#theoretical-distributions-as-approximations",
    "title": "\n4  Continuous probability\n",
    "section": "\n4.3 Theoretical distributions as approximations",
    "text": "4.3 Theoretical distributions as approximations\nThe normal distribution is derived mathematically: we do not need data to define it. For practicing data scientists, almost everything we do involves data. Data is always, technically speaking, discrete. For example, we could consider our height data categorical with each specific height a unique category. The probability distribution is defined by the proportion of students reporting each height. Here is a plot of that probability distribution:\n\n\n\n\n\n\n\n\nWhile most students rounded up their heights to the nearest inch, others reported values with more precision. One student reported his height to be 69.6850393700787, which is 177 centimeters. The probability assigned to this height is 0.0012315 or 1 in 812. The probability for 70 inches is much higher at 0.1059113, but does it really make sense to think of the probability of being exactly 70 inches as being different than 69.6850393700787? Clearly it is much more useful for data analytic purposes to treat this outcome as a continuous numeric variable, keeping in mind that very few people, or perhaps none, are exactly 70 inches, and that the reason we get more values at 70 is because people round to the nearest inch.\nWith continuous distributions, the probability of a singular value is not even defined. For example, it does not make sense to ask what is the probability that a normally distributed value is 70. Instead, we define probabilities for intervals. We thus could ask what is the probability that someone is between 69.5 and 70.5.\nIn cases like height, in which the data is rounded, the normal approximation is particularly useful if we deal with intervals that include exactly one round number. For example, the normal distribution is useful for approximating the proportion of students reporting values in intervals like the following three:\n\nmean(x &lt;= 68.5) - mean(x &lt;= 67.5)\n#&gt; [1] 0.115\nmean(x &lt;= 69.5) - mean(x &lt;= 68.5)\n#&gt; [1] 0.119\nmean(x &lt;= 70.5) - mean(x &lt;= 69.5)\n#&gt; [1] 0.122\n\nNote how close we get with the normal approximation:\n\npnorm(68.5, m, s) - pnorm(67.5, m, s) \n#&gt; [1] 0.103\npnorm(69.5, m, s) - pnorm(68.5, m, s) \n#&gt; [1] 0.11\npnorm(70.5, m, s) - pnorm(69.5, m, s) \n#&gt; [1] 0.108\n\nHowever, the approximation is not as useful for other intervals. For instance, notice how the approximation breaks down when we try to estimate:\n\nmean(x &lt;= 70.9) - mean(x&lt;=70.1)\n#&gt; [1] 0.0222\n\nwith\n\npnorm(70.9, m, s) - pnorm(70.1, m, s)\n#&gt; [1] 0.0836\n\nIn general, we call this situation discretization. Although the true height distribution is continuous, the reported heights tend to be more common at discrete values, in this case, due to rounding. As long as we are aware of how to deal with this reality, the normal approximation can still be a very useful tool."
  },
  {
    "objectID": "prob/continuous-probability.html#the-probability-density",
    "href": "prob/continuous-probability.html#the-probability-density",
    "title": "\n4  Continuous probability\n",
    "section": "\n4.4 The probability density",
    "text": "4.4 The probability density\nFor categorical distributions, we can define the probability of a category. For example, a roll of a die, let’s call it \\(X\\), can be 1, 2, 3, 4, 5 or 6. The probability of 4 is defined as:\n\\[\n\\mbox{Pr}(X=4) = 1/6\n\\]\nThe CDF can then easily be defined: \\[\nF(4) = \\mbox{Pr}(X\\leq 4) =  \\mbox{Pr}(X = 4) +  \\mbox{Pr}(X = 3) +  \\mbox{Pr}(X = 2) +  \\mbox{Pr}(X = 1)\n\\]\nAlthough for continuous distributions the probability of a single value \\(\\mbox{Pr}(X=x)\\) is not defined, there is a theoretical definition that has a similar interpretation. The probability density at \\(x\\) is defined as the function \\(f(a)\\) such that:\n\\[\nF(a) = \\mbox{Pr}(X\\leq a) = \\int_{-\\infty}^a f(x)\\, dx\n\\]\nFor those that know calculus, remember that the integral is related to a sum: it is the sum of bars with widths approximating 0. If you don’t know calculus, you can think of \\(f(x)\\) as a curve for which the area under that curve up to the value \\(a\\), gives you the probability \\(\\mbox{Pr}(X\\leq a)\\).\nFor example, to use the normal approximation to estimate the probability of someone being taller than 76 inches, we use:\n\n1 - pnorm(76, m, s)\n#&gt; [1] 0.0321\n\nwhich mathematically is the grey area below:\n\n\n\n\n\n\n\n\nThe curve you see is the probability density for the normal distribution. In R, we get this using the function dnorm.\nAlthough it may not be immediately obvious why knowing about probability densities is useful, understanding this concept will be essential to those wanting to fit models to data for which predefined functions are not available."
  },
  {
    "objectID": "prob/continuous-probability.html#monte-carlo",
    "href": "prob/continuous-probability.html#monte-carlo",
    "title": "\n4  Continuous probability\n",
    "section": "\n4.5 Monte Carlo",
    "text": "4.5 Monte Carlo\nR provides functions to generate normally distributed outcomes. Specifically, the rnorm function takes three arguments: size, average (defaults to 0), and standard deviation (defaults to 1) and produces random numbers. Here is an example of how we could generate data that looks like our reported heights:\n\nn &lt;- length(x)\nm &lt;- mean(x)\ns &lt;- sd(x)\nsimulated_heights &lt;- rnorm(n, m, s)\n\nNot surprisingly, the distribution looks normal:\n\n\n\n\n\n\n\n\nThis is one of the most useful functions in R as it will permit us to generate data that mimics natural events and answers questions related to what could happen by chance by running Monte Carlo simulations.\nIf, for example, we pick 800 males at random, what is the distribution of the tallest person? How rare is a seven footer in a group of 800 males? The following Monte Carlo simulation helps us answer that question:\n\nB &lt;- 10000\ntallest &lt;- replicate(B, {\n  simulated_data &lt;- rnorm(800, m, s)\n  max(simulated_data)\n})\n\nHaving a seven footer is quite rare:\n\nmean(tallest &gt;= 7*12)\n#&gt; [1] 0.0156\n\nHere is the resulting distribution:\n\n\n\n\n\n\n\n\nNote that it does not look normal."
  },
  {
    "objectID": "prob/continuous-probability.html#continuous-distributions",
    "href": "prob/continuous-probability.html#continuous-distributions",
    "title": "\n4  Continuous probability\n",
    "section": "\n4.6 Continuous distributions",
    "text": "4.6 Continuous distributions\nThe normal distribution is not the only useful theoretical distribution. Other continuous distributions that we may encounter are the student-t, Chi-square, exponential, gamma, beta, and beta-binomial. R provides functions to compute the density, the quantiles, the cumulative distribution functions and to generate Monte Carlo simulations. R uses a convention that lets us remember the names, namely using the letters d, q, p, and r in front of a shorthand for the distribution. We have already seen the functions dnorm, pnorm, and rnorm for the normal distribution. The functions qnorm gives us the quantiles. We can therefore draw a distribution like this:\n\nx &lt;- seq(-4, 4, length.out = 100)\nqplot(x, f, geom = \"line\", data = data.frame(x, f = dnorm(x)))\n\nFor the student-t, described later in Section Section 10.2.3, the shorthand t is used so the functions are dt for the density, qt for the quantiles, pt for the cumulative distribution function, and rt for Monte Carlo simulation."
  },
  {
    "objectID": "prob/continuous-probability.html#exercises",
    "href": "prob/continuous-probability.html#exercises",
    "title": "\n4  Continuous probability\n",
    "section": "\n4.7 Exercises",
    "text": "4.7 Exercises\n1. Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 5 feet or shorter?\n2. Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 6 feet or taller?\n3. Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is between 61 and 67 inches?\n4. Repeat the exercise above, but convert everything to centimeters. That is, multiply every height, including the standard deviation, by 2.54. What is the answer now?\n5. Notice that the answer to the question does not change when you change units. This makes sense since the standard deviations from the average for an entry in a list are not affected by what units we use. In fact, if you look closely, you notice that 61 and 67 are both 1 SD away from the average. Compute the probability that a randomly picked, normally distributed random variable is within 1 SD from the average.\n6. To see the math that explains why the answers to questions 3, 4, and 5 are the same, suppose we have a random variable with average \\(m\\) and standard error \\(s\\). Suppose we ask the probability of \\(X\\) being smaller or equal to \\(a\\). Remember that, by definition, \\(a\\) is \\((a - m)/s\\) standard deviations \\(s\\) away from the average \\(m\\). The probability is:\n\\[\n\\mbox{Pr}(X \\leq a)\n\\]\nNow we subtract \\(\\mu\\) to both sides and then divide both sides by \\(\\sigma\\):\n\\[\n\\mbox{Pr}\\left(\\frac{X-m}{s} \\leq \\frac{a-m}{s} \\right)\n\\]\nThe quantity on the left is a standard normal random variable. It has an average of 0 and a standard error of 1. We will call it \\(Z\\):\n\\[\n\\mbox{Pr}\\left(Z \\leq \\frac{a-m}{s} \\right)\n\\]\nSo, no matter the units, the probability of \\(X\\leq a\\) is the same as the probability of a standard normal variable being less than \\((a - m)/s\\). If mu is the average and sigma the standard error, which of the following R code would give us the right answer in every situation:\n\nmean(X&lt;=a)\npnorm((a - m)/s)\npnorm((a - m)/s, m, s)\npnorm(a)\n\n7. Imagine the distribution of male adults is approximately normal with an expected value of 69 and a standard deviation of 3. How tall is the male in the 99th percentile? Hint: use qnorm.\n8. The distribution of IQ scores is approximately normally distributed. The average is 100 and the standard deviation is 15. Suppose you want to know the distribution of the highest IQ across all graduating classes if 10,000 people are born each in your school district. Run a Monte Carlo simulation with B=1000 generating 10,000 IQ scores and keeping the highest. Make a histogram."
  },
  {
    "objectID": "prob/random-variables-sampling-models-clt.html#random-variables",
    "href": "prob/random-variables-sampling-models-clt.html#random-variables",
    "title": "\n5  Random variables\n",
    "section": "\n5.1 Random variables",
    "text": "5.1 Random variables\nRandom variables are numeric outcomes resulting from random processes. We can easily generate random variables using some of the simple examples we have shown. For example, define X to be 1 if a bead is blue and red otherwise:\n\nbeads &lt;- rep( c(\"red\", \"blue\"), times = c(2,3))\nX &lt;- ifelse(sample(beads, 1) == \"blue\", 1, 0)\n\nHere X is a random variable: every time we select a new bead the outcome changes randomly. See below:\n\nifelse(sample(beads, 1) == \"blue\", 1, 0)\n#&gt; [1] 1\nifelse(sample(beads, 1) == \"blue\", 1, 0)\n#&gt; [1] 0\nifelse(sample(beads, 1) == \"blue\", 1, 0)\n#&gt; [1] 0\n\nSometimes it’s 1 and sometimes it’s 0."
  },
  {
    "objectID": "prob/random-variables-sampling-models-clt.html#sampling-models",
    "href": "prob/random-variables-sampling-models-clt.html#sampling-models",
    "title": "\n5  Random variables\n",
    "section": "\n5.2 Sampling models",
    "text": "5.2 Sampling models\nMany data generation procedures, those that produce the data we study, can be modeled quite well as draws from an urn. For instance, we can model the process of polling likely voters as drawing 0s (Republicans) and 1s (Democrats) from an urn containing the 0 and 1 code for all likely voters. In epidemiological studies, we often assume that the subjects in our study are a random sample from the population of interest. The data related to a specific outcome can be modeled as a random sample from an urn containing the outcome for the entire population of interest. Similarly, in experimental research, we often assume that the individual organisms we are studying, for example worms, flies, or mice, are a random sample from a larger population. Randomized experiments can also be modeled by draws from an urn given the way individuals are assigned into groups: when getting assigned, you draw your group at random. Sampling models are therefore ubiquitous in data science. Casino games offer a plethora of examples of real-world situations in which sampling models are used to answer specific questions. We will therefore start with such examples.\nSuppose a very small casino hires you to consult on whether they should set up roulette wheels. To keep the example simple, we will assume that 1,000 people will play and that the only game you can play on the roulette wheel is to bet on red or black. The casino wants you to predict how much money they will make or lose. They want a range of values and, in particular, they want to know what’s the chance of losing money. If this probability is too high, they will pass on installing roulette wheels.\nWe are going to define a random variable \\(S\\) that will represent the casino’s total winnings. Let’s start by constructing the urn. A roulette wheel has 18 red pockets, 18 black pockets and 2 green ones. So playing a color in one game of roulette is equivalent to drawing from this urn:\n\ncolor &lt;- rep(c(\"Black\", \"Red\", \"Green\"), c(18, 18, 2))\n\nThe 1,000 outcomes from 1,000 people playing are independent draws from this urn. If red comes up, the gambler wins and the casino loses a dollar, so we draw a -$1. Otherwise, the casino wins a dollar and we draw a $1. To construct our random variable \\(S\\), we can use this code:\n\nn &lt;- 1000\nX &lt;- sample(ifelse(color == \"Red\", -1, 1),  n, replace = TRUE)\nX[1:10]\n#&gt;  [1] -1  1  1 -1 -1 -1  1  1  1  1\n\nBecause we know the proportions of 1s and -1s, we can generate the draws with one line of code, without defining color:\n\nX &lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19))\n\nWe call this a sampling model since we are modeling the random behavior of roulette with the sampling of draws from an urn. The total winnings \\(S\\) is simply the sum of these 1,000 independent draws:\n\nX &lt;- sample(c(-1, 1), n, replace = TRUE, prob = c(9/19, 10/19))\nS &lt;- sum(X)\nS\n#&gt; [1] 22"
  },
  {
    "objectID": "prob/random-variables-sampling-models-clt.html#the-probability-distribution-of-a-random-variable",
    "href": "prob/random-variables-sampling-models-clt.html#the-probability-distribution-of-a-random-variable",
    "title": "\n5  Random variables\n",
    "section": "\n5.3 The probability distribution of a random variable",
    "text": "5.3 The probability distribution of a random variable\nIf you run the code above, you see that \\(S\\) changes every time. This is, of course, because \\(S\\) is a random variable. The probability distribution of a random variable tells us the probability of the observed value falling at any given interval. So, for example, if we want to know the probability that we lose money, we are asking the probability that \\(S\\) is in the interval \\(S&lt;0\\).\nNote that if we can define a cumulative distribution function \\(F(a) = \\mbox{Pr}(S\\leq a)\\), then we will be able to answer any question related to the probability of events defined by our random variable \\(S\\), including the event \\(S&lt;0\\). We call this \\(F\\) the random variable’s distribution function.\nWe can estimate the distribution function for the random variable \\(S\\) by using a Monte Carlo simulation to generate many realizations of the random variable. With this code, we run the experiment of having 1,000 people play roulette, over and over, specifically \\(B = 10,000\\) times:\n\nn &lt;- 1000\nB &lt;- 10000\nroulette_winnings &lt;- function(n){\n  X &lt;- sample(c(-1,1), n, replace = TRUE, prob = c(9/19, 10/19))\n  sum(X)\n}\nS &lt;- replicate(B, roulette_winnings(n))\n\nNow we can ask the following: in our simulations, how often did we get sums less than or equal to a?\n\nmean(S &lt;= a)\n\nThis will be a very good approximation of \\(F(a)\\) and we can easily answer the casino’s question: how likely is it that we will lose money? We can see it is quite low:\n\nmean(S &lt; 0)\n#&gt; [1] 0.0456\n\nWe can visualize the distribution of \\(S\\) by creating a histogram showing the probability \\(F(b)-F(a)\\) for several intervals \\((a,b]\\):\n\n\n\n\n\n\n\n\nWe see that the distribution appears to be approximately normal. A qq-plot will confirm that the normal approximation is close to a perfect approximation for this distribution. If, in fact, the distribution is normal, then all we need to define the distribution is the average and the standard deviation. Because we have the original values from which the distribution is created, we can easily compute these with mean(S) and sd(S). The blue curve you see added to the histogram above is a normal density with this average and standard deviation.\nThis average and this standard deviation have special names. They are referred to as the expected value and standard error of the random variable \\(S\\). We will say more about these in the next section.\nStatistical theory provides a way to derive the distribution of random variables defined as independent random draws from an urn. Specifically, in our example above, we can show that \\((S+n)/2\\) follows a binomial distribution. We therefore do not need to run for Monte Carlo simulations to know the probability distribution of \\(S\\). We did this for illustrative purposes.\nWe can use the function dbinom and pbinom to compute the probabilities exactly. For example, to compute \\(\\mbox{Pr}(S &lt; 0)\\) we note that:\n\\[\\mbox{Pr}(S &lt; 0) = \\mbox{Pr}((S+n)/2 &lt; (0+n)/2)\\]\nand we can use the pbinom to compute \\[\\mbox{Pr}(S \\leq 0)\\]\n\nn &lt;- 1000\npbinom(n/2, size = n, prob = 10/19)\n#&gt; [1] 0.0511\n\nBecause this is a discrete probability function, to get \\(\\mbox{Pr}(S &lt; 0)\\) rather than \\(\\mbox{Pr}(S \\leq 0)\\), we write:\n\npbinom(n/2 - 1, size = n, prob = 10/19)\n#&gt; [1] 0.0448\n\nFor the details of the binomial distribution, you can consult any basic probability book or even Wikipedia3.\nHere we do not cover these details. Instead, we will discuss an incredibly useful approximation provided by mathematical theory that applies generally to sums and averages of draws from any urn: the Central Limit Theorem (CLT)."
  },
  {
    "objectID": "prob/random-variables-sampling-models-clt.html#distributions-versus-probability-distributions",
    "href": "prob/random-variables-sampling-models-clt.html#distributions-versus-probability-distributions",
    "title": "\n5  Random variables\n",
    "section": "\n5.4 Distributions versus probability distributions",
    "text": "5.4 Distributions versus probability distributions\nBefore we continue, let’s make an important distinction and connection between the distribution of a list of numbers and a probability distribution. In the visualization chapter, we described how any list of numbers \\(x_1,\\dots,x_n\\) has a distribution. The definition is quite straightforward. We define \\(F(a)\\) as the function that tells us what proportion of the list is less than or equal to \\(a\\). Because they are useful summaries when the distribution is approximately normal, we define the average and standard deviation. These are defined with a straightforward operation of the vector containing the list of numbers x:\n\nm &lt;- sum(x)/length(x)\ns &lt;- sqrt(sum((x - m)^2) / length(x))\n\nA random variable \\(X\\) has a distribution function. To define this, we do not need a list of numbers. It is a theoretical concept. In this case, we define the distribution as the \\(F(a)\\) that answers the question: what is the probability that \\(X\\) is less than or equal to \\(a\\)? There is no list of numbers.\nHowever, if \\(X\\) is defined by drawing from an urn with numbers in it, then there is a list: the list of numbers inside the urn. In this case, the distribution of that list is the probability distribution of \\(X\\) and the average and standard deviation of that list are the expected value and standard error of the random variable.\nAnother way to think about it that does not involve an urn is to run a Monte Carlo simulation and generate a very large list of outcomes of \\(X\\). These outcomes are a list of numbers. The distribution of this list will be a very good approximation of the probability distribution of \\(X\\). The longer the list, the better the approximation. The average and standard deviation of this list will approximate the expected value and standard error of the random variable."
  },
  {
    "objectID": "prob/random-variables-sampling-models-clt.html#notation-for-random-variables",
    "href": "prob/random-variables-sampling-models-clt.html#notation-for-random-variables",
    "title": "\n5  Random variables\n",
    "section": "\n5.5 Notation for random variables",
    "text": "5.5 Notation for random variables\nIn statistical textbooks, upper case letters are used to denote random variables and we follow this convention here. Lower case letters are used for observed values. You will see some notation that includes both. For example, you will see events defined as \\(X \\leq x\\). Here \\(X\\) is a random variable, making it a random event, and \\(x\\) is an arbitrary value and not random. So, for example, \\(X\\) might represent the number on a die roll and \\(x\\) will represent an actual value we see 1, 2, 3, 4, 5, or 6. So in this case, the probability of \\(X=x\\) is 1/6 regardless of the observed value \\(x\\). This notation is a bit strange because, when we ask questions about probability, \\(X\\) is not an observed quantity. Instead, it’s a random quantity that we will see in the future. We can talk about what we expect it to be, what values are probable, but not what it is. But once we have data, we do see a realization of \\(X\\). So data scientists talk of what could have been after we see what actually happened."
  },
  {
    "objectID": "prob/random-variables-sampling-models-clt.html#the-expected-value-and-standard-error",
    "href": "prob/random-variables-sampling-models-clt.html#the-expected-value-and-standard-error",
    "title": "\n5  Random variables\n",
    "section": "\n5.6 The expected value and standard error",
    "text": "5.6 The expected value and standard error\nWe have described sampling models for draws. We will now go over the mathematical theory that lets us approximate the probability distributions for the sum of draws. Once we do this, we will be able to help the casino predict how much money they will make. The same approach we use for the sum of draws will be useful for describing the distribution of averages and proportion which we will need to understand how polls work.\nThe first important concept to learn is the expected value. In statistics books, it is common to use letter \\(\\mbox{E}\\) like this:\n\\[\\mbox{E}[X]\\]\nto denote the expected value of the random variable \\(X\\).\nA random variable will vary around its expected value in a way that if you take the average of many, many draws, the average of the draws will approximate the expected value, getting closer and closer the more draws you take. This makes the expected value a useful quantity to compute.\nFor discrete random variable with possible outcomes \\(x_1,\\dots,x_n\\) the expected value is defined as\n\\[\n\\mbox{E}[X] = \\sum_{i=1}^n x_i \\,\\mbox{Pr}(X = x_i)\n\\] If \\(X\\) is a continuous random variable, with range of values \\(a\\) to \\(b\\) and probability density function \\(f(x)\\), this sum turns into an integral:\n\\[\n\\mbox{E}[X] = \\int_a^b x f(x)\n\\]\nNote that in the case that we are picking values from an un urn where each value \\(x_i\\) has an equal chance \\(1/n\\) of being selected the above equation is simply the average of the \\(x_i\\)s\n\\[\n\\mbox{E}[X] = \\frac{1}{n}\\sum_{i=1}^n x_i\n\\]\nIn the urn used to model betting on red in roulette, we have 20 one dollars and 18 negative one dollars so the expected value is:\n\\[\n\\mbox{E}[X] = (20 + -18)/38\n\\]\nwhich is about 5 cents. You may think it is a bit counter-intuitive to say that \\(X\\) varies around 0.05, when the only values it takes is 1 and -1. One way to make sense of the expected value in this context is by realizing that if we play the game over and over, the casino wins, on average, 5 cents per game. A Monte Carlo simulation confirms this:\n\nB &lt;- 10^6\nx &lt;- sample(c(-1, 1), B, replace = TRUE, prob = c(9/19, 10/19))\nmean(x)\n#&gt; [1] 0.0517\n\nIn general, if the urn has two possible outcomes, say \\(a\\) and \\(b\\), with proportions \\(p\\) and \\(1-p\\) respectively, the average is:\n\\[\\mbox{E}[X] = ap + b(1-p)\\]\nTo see this, notice that if there are \\(n\\) beads in the urn, then we have \\(np\\) \\(a\\)s and \\(n(1-p)\\) \\(b\\)s and because the average is the sum, \\(n\\times a \\times p + n\\times b \\times (1-p)\\), divided by the total \\(n\\), we get that the average is \\(ap + b(1-p)\\).\nNow the reason we define the expected value is because this mathematical definition turns out to be useful for approximating the probability distributions of sum, which then is useful for describing the distribution of averages and proportions. The first useful fact is that the expected value of the sum of the draws is the number of draws \\(\\times\\) the average of the numbers in the urn.\nSo if 1,000 people play roulette, the casino expects to win, on average, about 1,000 \\(\\times\\) $0.05 = $50. But this is an expected value. How different can one observation be from the expected value? The casino really needs to know this. What is the range of possibilities? If negative numbers are too likely, they will not install roulette wheels. Statistical theory once again answers this question. The standard error (SE) gives us an idea of the size of the variation around the expected value. In statistics books, it’s common to use:\n\\[\\mbox{SE}[X]\\]\nto denote the standard error of a random variable.\nFor discrete random variable with possible outcomes \\(x_1,\\dots,x_n\\) the standard error is defined as\n\\[\n\\mbox{SE}[X] = \\sqrt{\\sum_{i=1}^n \\left(x_i - E[X]\\right)^2 \\,\\mbox{Pr}(X = x_i)},\n\\] which you can think of as the expected average distance of \\(X\\) from the expected value.\nIf \\(X\\) is a continuous random variable, with range of values \\(a\\) to \\(b\\) and probability density function \\(f(x)\\), this sum turns into an integral:\n\\[\n\\mbox{SE}[X] = \\sqrt{\\int_a^b \\left(x-\\mbox{E}[X]\\right)^2 f(x)\\,\\mathrm{d}x}\n\\]\nNote that in the case that we are picking values from an un urn where each value \\(x_i\\) has an equal chance \\(1/n\\) of being selected the above equation is simply the standard deviation of of the \\(x_i\\)s\n\\[\n\\mbox{SE}[X] = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2} \\mbox{ with } \\bar{x} =  \\frac{1}{n}\\sum_{i=1}^n x_i\n\\] Using the definition of standard deviation, we can derive, with a bit of math, that if an urn contains two values \\(a\\) and \\(b\\) with proportions \\(p\\) and \\((1-p)\\), respectively, the standard deviation is:\n\\[\\mid b - a \\mid \\sqrt{p(1-p)}.\\]\nSo in our roulette example, the standard deviation of the values inside the urn is: \\(\\mid 1 - (-1) \\mid \\sqrt{10/19 \\times 9/19}\\) or:\n\n2 * sqrt(90)/19\n#&gt; [1] 0.999\n\nThe standard error tells us the typical difference between a random variable and its expectation. Since one draw is obviously the sum of just one draw, we can use the formula above to calculate that the random variable defined by one draw has an expected value of 0.05 and a standard error of about 1. This makes sense since we either get 1 or -1, with 1 slightly favored over -1.\nA widely used mathematical results is tha if our draws are independent, then the standard error of the sum is given by the equation:\n\\[\n\\sqrt{\\mbox{number of draws}} \\times \\mbox{ standard deviation of the numbers in the urn}\n\\]\nUsing this formula, the sum of 1,000 people playing has standard error of about $32:\n\nn &lt;- 1000\nsqrt(n) * 2 * sqrt(90)/19\n#&gt; [1] 31.6\n\nAs a result, when 1,000 people bet on red, the casino is expected to win $50 with a standard error of $32. It therefore seems like a safe bet. But we still haven’t answered the question: how likely is it to lose money? Here the CLT will help.\n\n\n\n\n\n\nThe exact probability for the casino winnings can be computed exactly, rather than an approximation, using the binomial distribution. However, here we focus on the CLT, which can be generally applied to sums of random variables in a way that the binomial distribution can’t."
  },
  {
    "objectID": "prob/random-variables-sampling-models-clt.html#central-limit-theorem",
    "href": "prob/random-variables-sampling-models-clt.html#central-limit-theorem",
    "title": "\n5  Random variables\n",
    "section": "\n5.7 Central Limit Theorem",
    "text": "5.7 Central Limit Theorem\nThe Central Limit Theorem (CLT) tells us that when the number of draws, also called the sample size, is large, the probability distribution of the sum of the independent draws is approximately normal. Because sampling models are used for so many data generation processes, the CLT is considered one of the most important mathematical insights in history.\nPreviously, we discussed that if we know that the distribution of a list of numbers is approximated by the normal distribution, all we need to describe the list are the average and standard deviation. We also know that the same applies to probability distributions. If a random variable has a probability distribution that is approximated with the normal distribution, then all we need to describe the probability distribution are the average and standard deviation, referred to as the expected value and standard error.\nWe previously ran this Monte Carlo simulation:\n\nn &lt;- 1000\nB &lt;- 10000\nroulette_winnings &lt;- function(n){\n  X &lt;- sample(c(-1,1), n, replace = TRUE, prob = c(9/19, 10/19))\n  sum(X)\n}\nS &lt;- replicate(B, roulette_winnings(n))\n\nThe Central Limit Theorem (CLT) tells us that the sum \\(S\\) is approximated by a normal distribution. Using the formulas above, we know that the expected value and standard error are:\n\nn * (20 - 18)/38 \n#&gt; [1] 52.6\nsqrt(n)*2*sqrt(90)/19 \n#&gt; [1] 31.6\n\nThe theoretical values above match those obtained with the Monte Carlo simulation:\n\nmean(S)\n#&gt; [1] 52.2\nsd(S)\n#&gt; [1] 31.7\n\nUsing the CLT, we can skip the Monte Carlo simulation and instead compute the probability of the casino losing money using this approximation:\n\nmu &lt;- n * (20 - 18)/38\nse &lt;-  sqrt(n)*2*sqrt(90)/19 \npnorm(0, mu, se)\n#&gt; [1] 0.0478\n\nwhich is also in very good agreement with our Monte Carlo result:\n\nmean(S &lt; 0)\n#&gt; [1] 0.0458\n\n\n5.7.1 How large is large in the Central Limit Theorem?\nThe CLT works when the number of draws is large. But large is a relative term. In many circumstances as few as 30 draws is enough to make the CLT useful. In some specific instances, as few as 10 is enough. However, these should not be considered general rules. Note, for example, that when the probability of success is very small, we need much larger sample sizes.\nBy way of illustration, let’s consider the lottery. In the lottery, the chances of winning are less than 1 in a million. Thousands of people play so the number of draws is very large. Yet the number of winners, the sum of the draws, range between 0 and 4. This sum is certainly not well approximated by a normal distribution, so the CLT does not apply, even with the very large sample size. This is generally true when the probability of a success is very low. In these cases, the Poisson distribution is more appropriate.\nYou can examine the properties of the Poisson distribution using dpois and ppois. You can generate random variables following this distribution with rpois. However, we do not cover the theory here. You can learn about the Poisson distribution in any probability textbook and even Wikipedia4"
  },
  {
    "objectID": "prob/random-variables-sampling-models-clt.html#statistical-properties-of-averages",
    "href": "prob/random-variables-sampling-models-clt.html#statistical-properties-of-averages",
    "title": "\n5  Random variables\n",
    "section": "\n5.8 Statistical properties of averages",
    "text": "5.8 Statistical properties of averages\nThere are several useful mathematical results that we used above and often employ when working with data. We list them below.\n1. The expected value of the sum of random variables is the sum of each random variable’s expected value. We can write it like this:\n\\[\n\\mbox{E}[X_1+X_2+\\dots+X_n] =  \\mbox{E}[X_1] + \\mbox{E}[X_2]+\\dots+\\mbox{E}[X_n]\n\\]\nIf the \\(X\\) are independent draws from the urn, then they all have the same expected value. Let’s call it \\(\\mu\\) and thus:\n\\[\n\\mbox{E}[X_1+X_2+\\dots+X_n]=  n\\mu\n\\]\nwhich is another way of writing the result we show above for the sum of draws.\n2. The expected value of a non-random constant times a random variable is the non-random constant times the expected value of a random variable. This is easier to explain with symbols:\n\\[\n\\mbox{E}[aX] =  a\\times\\mbox{E}[X]\n\\]\nTo see why this is intuitive, consider change of units. If we change the units of a random variable, say from dollars to cents, the expectation should change in the same way. A consequence of the above two facts is that the expected value of the average of independent draws from the same urn is the expected value of the urn, call it \\(\\mu\\) again:\n\\[\n\\mbox{E}[(X_1+X_2+\\dots+X_n) / n]=   \\mbox{E}[X_1+X_2+\\dots+X_n] / n = n\\mu/n = \\mu\n\\]\n3. The square of the standard error of the sum of independent random variables is the sum of the square of the standard error of each random variable. This one is easier to understand in math form:\n\\[\n\\mbox{SE}[X_1+X_2+\\dots+X_n] = \\sqrt{\\mbox{SE}[X_1]^2 + \\mbox{SE}[X_2]^2+\\dots+\\mbox{SE}[X_n]^2  }\n\\]\nThe square of the standard error is referred to as the variance in statistical textbooks. Note that this particular property is not as intuitive as the previous three and more in depth explanations can be found in statistics textbooks.\n4. The standard error of a non-random constant times a random variable is the non-random constant times the random variable’s standard error. As with the expectation:\n\\[\n\\mbox{SE}[aX] =  a \\times \\mbox{SE}[X]\n\\]\nTo see why this is intuitive, again think of units.\nA consequence of 3 and 4 is that the standard error of the average of independent draws from the same urn is the standard deviation of the urn divided by the square root of \\(n\\) (the number of draws), call it \\(\\sigma\\):\n\\[\n\\begin{aligned}\n\\mbox{SE}[(X_1+X_2+\\dots+X_n) / n] &=   \\mbox{SE}[X_1+X_2+\\dots+X_n]/n \\\\\n&= \\sqrt{\\mbox{SE}[X_1]^2+\\mbox{SE}[X_2]^2+\\dots+\\mbox{SE}[X_n]^2}/n \\\\\n&= \\sqrt{\\sigma^2+\\sigma^2+\\dots+\\sigma^2}/n\\\\\n&= \\sqrt{n\\sigma^2}/n\\\\\n&= \\sigma / \\sqrt{n}    \n\\end{aligned}\n\\]\n5. If \\(X\\) is a normally distributed random variable, then if \\(a\\) and \\(b\\) are non-random constants, \\(aX + b\\) is also a normally distributed random variable. All we are doing is changing the units of the random variable by multiplying by \\(a\\), then shifting the center by \\(b\\).\nNote that statistical textbooks use the Greek letters \\(\\mu\\) and \\(\\sigma\\) to denote the expected value and standard error, respectively. This is because \\(\\mu\\) is the Greek letter for \\(m\\), the first letter of mean, which is another term used for expected value. Similarly, \\(\\sigma\\) is the Greek letter for \\(s\\), the first letter of standard error.\n\n\n\n\n\n\nThe assumption of independence is important\n\n\n\nLet’s make the explanation more concise and clear:\nThe given equation reveals crucial insights for practical scenarios. Specifically, it suggests that the standard error can be minimized by increasing the sample size, \\(n\\), and we can quantify this reduction. However, this principle holds true only when the variables \\(X_1, X_2, ... X_n\\) are independent. If they are not, the estimated standard error can be significantly off.\nIn Section Section 13.2, we introduce the concept of correlation, which quantifies the degree to which variables are interdependent. If the correlation coefficient among the ( X ) variables is ( ), the standard error of their average is:\n\\[\n\\mbox{SE}\\left(\\bar{X}\\right) = \\sigma \\sqrt{\\frac{1 + (n-1) \\rho}{n}}\n\\]\nThe key observation here is that as \\(\\rho\\) approaches its upper limit of 1, the standard error increases. Notably, in the situation where \\(\\rho = 1\\), the standard error, \\(\\mbox{SE}(\\bar{X})\\), equals \\(\\sigma\\), and it becomes unaffected by the sample size \\(n\\).\n\n\n\n5.8.1 Law of large numbers\nAn important implication of the result 5 above is that the standard error of the average becomes smaller and smaller as \\(n\\) grows larger. When \\(n\\) is very large, then the standard error is practically 0 and the average of the draws converges to the average of the urn. This is known in statistical textbooks as the law of large numbers or the law of averages.\n\n\n\n\n\n\nMisinterpretation of the law of averages\n\n\n\nThe law of averages is sometimes misinterpreted. For example, if you toss a coin 5 times and see a head each time, you might hear someone argue that the next toss is probably a tail because of the law of averages: on average we should see 50% heads and 50% tails. A similar argument would be to say that red “is due” on the roulette wheel after seeing black come up five times in a row. These events are independent so the chance of a coin landing heads is 50% regardless of the previous 5. This is also the case for the roulette outcome. The law of averages applies only when the number of draws is very large and not in small samples. After a million tosses, you will definitely see about 50% heads regardless of the outcome of the first five tosses. Another funny misuse of the law of averages is in sports when TV sportscasters predict a player is about to succeed because they have failed a few times in a row."
  },
  {
    "objectID": "prob/random-variables-sampling-models-clt.html#exercises",
    "href": "prob/random-variables-sampling-models-clt.html#exercises",
    "title": "\n5  Random variables\n",
    "section": "\n5.9 Exercises",
    "text": "5.9 Exercises\n1. In American Roulette you can also bet on green. There are 18 reds, 18 blacks and 2 greens (0 and 00). What are the chances the green comes out?\n2. The payout for winning on green is $17 dollars. This means that if you bet a dollar and it lands on green, you get $17. Create a sampling model using sample to simulate the random variable \\(X\\) for your winnings. Hint: see the example below for how it should look like when betting on red.\n\nx &lt;- sample(c(1,-1), 1, prob = c(9/19, 10/19))\n\n3. Compute the expected value of \\(X\\).\n4. Compute the standard error of \\(X\\).\n5. Now create a random variable \\(S\\) that is the sum of your winnings after betting on green 1000 times. Hint: change the argument size and replace in your answer to question 2. Start your code by setting the seed to 1 with set.seed(1).\n6. What is the expected value of \\(S\\)?\n7. What is the standard error of \\(S\\)?\n8. What is the probability that you end up winning money? Hint: use the CLT.\n9. Create a Monte Carlo simulation that generates 1,000 outcomes of \\(S\\). Compute the average and standard deviation of the resulting list to confirm the results of 6 and 7. Start your code by setting the seed to 1 with set.seed(1).\n10. Now check your answer to 8 using the Monte Carlo result.\n11. The Monte Carlo result and the CLT approximation are close, but not that close. What could account for this?\n\n1,000 simulations is not enough. If we do more, they match.\nThe CLT does not work as well when the probability of success is small. In this case, it was 1/19. If we make the number of roulette plays bigger, they will match better.\nThe difference is within rounding error.\nThe CLT only works for averages.\n\n12. Now create a random variable \\(Y\\) that is your average winnings per bet after playing off your winnings after betting on green 1,000 times.\n13. What is the expected value of \\(Y\\)?\n14. What is the standard error of \\(Y\\)?\n15. What is the probability that you end up with winnings per game that are positive? Hint: use the CLT.\n16. Create a Monte Carlo simulation that generates 2,500 outcomes of \\(Y\\). Compute the average and standard deviation of the resulting list to confirm the results of 13 and 14. Start your code by setting the seed to 1 with set.seed(1).\n17. Now compare your answer to 15 using the Monte Carlo result.\n18. The Monte Carlo result and the CLT approximation are now much closer. What could account for this?\n\nWe are now computing averages instead of sums.\n2,500 Monte Carlo simulations is not better than 1,000.\nThe CLT works better when the sample size is larger. We increased from 1,000 to 2,500.\nIt is not closer. The difference is within rounding error.\n\n19. More complex versions of the sampling models we have discussed are also used by banks to decide interest rates and insurance companies to decide on premiums. To understand this, suppose you run a small bank that has a history of identifying potential homeowners that can be trusted to make payments. In fact, historically, in a given year, only 2% of your customers default, meaning that they don’t pay back the money that you lent them. Suppose your bank will give out $n=$1,000 loans for $180,000 this year. Also, after adding up all costs, suppose your bank loses \\(l\\)=$200,000 per foreclosure. For simplicity, we assume this includes all operational costs. What is the expected profit \\(S\\) for you bank under this scenario?\n20. Note that the total loss defined by the final sum in the previous exercise is a random variable. Every time you run the sampling model code, you get a different number of people default resulting in a different loss. Code a sampling model for the random variable representing your banks profit \\(S\\) under scenario described in 19.\n21. The previous exercise demonstrates that if you simply loan money to everybody without interest, you will end up losing money due to the 2% that defaults. Although you know 2% of your clients will probably default, you don’t know which ones, so you can’t remove them. Yet by charging everybody just a bit extra in interest, you can make up the losses incurred due to that 2% and also cover your operating costs. What quantity \\(x\\) would you have to charge each borrower so that your bank’s expected profit is 0? Assume that you don’t get \\(x\\) from the borrowers that default. Also note \\(x\\) is not the interest rate but the total you add. Can we refer to \\(x\\) divided by the size (\\(x/180000\\)) as the interest rate.\n22. Rewrite the sample modelfrom 20 and run a Monte Carlo simulation to get an idea of the distribution of your profit when you charge interest rates.\n23. We don’t really need a Monte Carlo simulation though. Using what we have learned, the CLT tells us that because our losses are a sum of independent draws, its distribution is approximately normal. What are the expected value and standard errors of the profit \\(S\\)? Write these as functions of the probability of foreclosure \\(p\\), the number of loans \\(n\\), the loss per foreclosure \\(l\\), and the quantity you charge each borrower \\(x\\).\n24. If you set \\(x\\) to assure your bank breaks even (expected profit is 0), what is the probability that your bank loses money?\n25. Suppose that if your bank has negative profit it has to close. You therefore need to increase \\(x\\) to minimize this risk. However, if you set the interest rates too high, your clients will go to another bank. So let’s say that we want our chances of losing money to be 1 in 100, what does the \\(x\\) quantity need to be now? Hint: We want \\(\\mbox{Pr}(S&lt;0) = 0.01\\). Note that you can add subtract constants to both side of an inequality and the probability does not change: \\(\\mbox{Pr}(S&lt;0) = \\mbox{Pr}(S+k&lt;0+k)\\), Similarly, with division of positive constants: \\(\\mbox{Pr}(S+k&lt;0+k) = \\mbox{Pr}((S+k)/m &lt;k/m)\\). Use this fact and the CLT to transform the left side of the inequality in \\(\\mbox{Pr}(S&lt;0)\\) into a standard normal.\n26. Our interest rate now goes up. But it is still a very competitive interest rate. For the \\(x\\) you obtained in 25, what is expected profit per loan and the expected total profit?\n27. Run run a Monte Carlo simulation to double check the theoretical approximation used in 25 and 26.\n28. One of your employees points out that since the bank is making a profit per loan, the bank should give out more loans! Why just \\(n\\)? You explain that finding those \\(n\\) clients was hard. You need a group that is predictable and that keeps the chances of defaults low. He then points out that even if the probability of default is higher, as long as our expected value is positive, you can minimize your chances of losses by increasing \\(n\\) and relying on the law of large numbers. Suppose the default probability is twice as high, or 4%, and you set the interest rate to 5%, or \\(x=\\)$9,000, what is your expected profit per loan?\n29. How much do we have to increase \\(n\\) by to assure the probability of losing money is still less than 0.01?\n30. Confirm the result in 29 with a Monte Carlo simulations.\n31. According to this equation, giving out more loans increases your expected profit and lowers the chances of losing money! Giving out more loans seems like a no brainier. As a result, your colleague decides to leave your bank and start his own high-risk mortgage company. A few months later, your colleague’s bank has gone bankrupt. A book is written and eventually the movies The Big Short and Margin Call are made relating the mistake your friend, and many others, made. What happened?\nYour colleague’s scheme was mainly based on this mathematical formula \\(\\mbox{SE}\\left(\\bar{X}\\right) = \\sigma / \\sqrt{n}\\). By making \\(n\\) large, we minimize the standard error of our per-loan profit. However, for this rule to hold, the \\(X\\)s must be independent draws: one person defaulting must be independent of others defaulting.\nTo construct a more realistic simulation than the original one your colleague ran, let’s assume there is a global event that affects everybody with high-risk mortgages and changes their probability. We will assume that with 50-50 chance, all the probabilities go up or down slightly to somewhere between 0.03 and 0.05. But it happens to everybody at once, not just one person. These draws are no longer independent so our equation does not apply. Write a Monte Carlo simulation for your total profit with this model.\n32. Use the simulation results to report the expected profit, the probability of losing money, and the probability of losing more than $10,000,000. Study the distribution of profit and discuss how making the wrong assumption lead to a catastrophic result,"
  },
  {
    "objectID": "prob/random-variables-sampling-models-clt.html#footnotes",
    "href": "prob/random-variables-sampling-models-clt.html#footnotes",
    "title": "\n5  Random variables\n",
    "section": "",
    "text": "https://en.wikipedia.org/w/index.php?title=Financial_crisis_of_2007%E2%80%932008↩︎\nhttps://en.wikipedia.org/w/index.php?title=Security_(finance)↩︎\nhttps://en.wikipedia.org/w/index.php?title=Binomial_distribution↩︎\nhttps://en.wikipedia.org/w/index.php?title=Poisson_distribution↩︎"
  },
  {
    "objectID": "inference/intro-inference.html#footnotes",
    "href": "inference/intro-inference.html#footnotes",
    "title": "Statistical inference",
    "section": "",
    "text": "https://www.youtube.com/watch?v=TbKkjm-gheY↩︎"
  },
  {
    "objectID": "inference/parameters-estimates.html#the-sampling-model-for-polls",
    "href": "inference/parameters-estimates.html#the-sampling-model-for-polls",
    "title": "\n6  Parameters and Estimates\n",
    "section": "\n6.1 The sampling model for polls",
    "text": "6.1 The sampling model for polls\nWe start by connecting probability theory to the task of using polls to learn about a population.\nAlthough typically the results of these polls are kept private, similar polls are conducted by news organizations because results tend to be of interest to the general public and made public. We will eventually be looking at such data.\nReal Clear Politics1 is an example of a news aggregator that organizes and publishes poll results. For example, they present the following poll results reporting estimates of the popular vote for the 2016 presidential election2:\n\n\n\n\nPoll\nDate\nSample\nMoE\nClinton\nTrump\nSpread\n\n\n\nRCP Average\n10/31 - 11/7\n--\n--\n47.2\n44.3\nClinton +2.9\n\n\nBloomberg\n11/4 - 11/6\n799 LV\n3.5\n46.0\n43.0\nClinton +3\n\n\nEconomist\n11/4 - 11/7\n3669 LV\n--\n49.0\n45.0\nClinton +4\n\n\nIBD\n11/3 - 11/6\n1026 LV\n3.1\n43.0\n42.0\nClinton +1\n\n\nABC\n11/3 - 11/6\n2220 LV\n2.5\n49.0\n46.0\nClinton +3\n\n\nFOX News\n11/3 - 11/6\n1295 LV\n2.5\n48.0\n44.0\nClinton +4\n\n\nMonmouth\n11/3 - 11/6\n748 LV\n3.6\n50.0\n44.0\nClinton +6\n\n\nCBS News\n11/2 - 11/6\n1426 LV\n3.0\n47.0\n43.0\nClinton +4\n\n\nLA Times\n10/31 - 11/6\n2935 LV\n4.5\n43.0\n48.0\nTrump +5\n\n\nNBC News\n11/3 - 11/5\n1282 LV\n2.7\n48.0\n43.0\nClinton +5\n\n\nNBC News\n10/31 - 11/6\n30145 LV\n1.0\n51.0\n44.0\nClinton +7\n\n\nMcClatchy\n11/1 - 11/3\n940 LV\n3.2\n46.0\n44.0\nClinton +2\n\n\nReuters\n10/31 - 11/4\n2244 LV\n2.2\n44.0\n40.0\nClinton +4\n\n\nGravisGravis\n10/31 - 10/31\n5360 RV\n1.3\n50.0\n50.0\nTie\n\n\n\n\n\n\nLet’s make some observations about the table above. First, note that different polls, all taken days before the election, report a different spread: the estimated difference between support for the two candidates. Notice also that the reported spreads hover around what ended up being the actual result: Clinton won the popular vote by 2.1%. We also see a column titled MoE which stands for margin of error.\nTo help us understand the connection between polls and what we have learned, let’s construct a similar situation to the one pollsters face. To mimic the challenge real pollsters face in terms of competing with other pollsters for media attention, we will use an urn full of beads to represent voters and pretend we are competing for a $25 dollar prize. The challenge is to guess the spread between the proportion of blue and red beads in this urn (in this case, a pickle jar):\n\n\n\n\n\n\n\n\nBefore making a prediction, you can take a sample (with replacement) from the urn. To mimic the fact that running polls is expensive, it costs you $0.10 per each bead you sample. Therefore, if your sample size is 250, and you win, you will break even since you will pay $25 to collect your $25 prize. Your entry into the competition can be an interval. If the interval you submit contains the true proportion, you get half what you paid and pass to the second phase of the competition. In the second phase, the entry with the smallest interval is selected as the winner.\nThe dslabs package includes a function that shows a random draw from this urn:\n\nlibrary(tidyverse)\nlibrary(dslabs)\ntake_poll(25)\n\n\n\n\n\n\n\n\n\nThink about how you would construct your interval based on the data shown above.\nWe have just described a simple sampling model for opinion polls. The beads inside the urn represent the individuals that will vote on election day. Those that will vote for the Republican candidate are represented with red beads and the Democrats with the blue beads. For simplicity, assume there are no other colors. That is, that there are just two parties: Republican and Democratic."
  },
  {
    "objectID": "inference/parameters-estimates.html#populations-samples-parameters-and-estimates",
    "href": "inference/parameters-estimates.html#populations-samples-parameters-and-estimates",
    "title": "\n6  Parameters and Estimates\n",
    "section": "\n6.2 Populations, samples, parameters, and estimates",
    "text": "6.2 Populations, samples, parameters, and estimates\nWe want to predict the proportion of blue beads in the urn. Let’s call this quantity \\(p\\), which then tells us the proportion of red beads \\(1-p\\), and the spread \\(p - (1-p)\\), which simplifies to \\(2p - 1\\).\nIn statistical textbooks, the beads in the urn are called the population. The proportion of blue beads in the population \\(p\\) is called a parameter. The 25 beads we see in the previous plot are called a sample. The task of statistical inference is to predict the parameter \\(p\\) using the observed data in the sample.\nCan we do this with the 25 observations above? It is certainly informative. For example, given that we see 13 red and 12 blue beads, it is unlikely that \\(p\\) &gt; .9 or \\(p\\) &lt; .1. But are we ready to predict with certainty that there are more red beads than blue in the jar?\nWe want to construct an estimate of \\(p\\) using only the information we observe. An estimate should be thought of as a summary of the observed data that we think is informative about the parameter of interest. It seems intuitive to think that the proportion of blue beads in the sample \\(0.48\\) must be at least related to the actual proportion \\(p\\). But do we simply predict \\(p\\) to be 0.48? First, remember that the sample proportion is a random variable. If we run the command take_poll(25) four times, we get a different answer each time, since the sample proportion is a random variable.\n\n\n\n\n\n\n\n\nNote that in the four random samples shown above, the sample proportions range from 0.44 to 0.60. By describing the distribution of this random variable, we will be able to gain insights into how good this estimate is and how we can make it better.\n\n6.2.1 The sample average\nConducting an opinion poll is being modeled as taking a random sample from an urn. We are proposing the use of the proportion of blue beads in our sample as an estimate of the parameter \\(p\\). Once we have this estimate, we can easily report an estimate for the spread \\(2p-1\\), but for simplicity we will illustrate the concepts for estimating \\(p\\). We will use our knowledge of probability to defend our use of the sample proportion and quantify how close we think it is from the population proportion \\(p\\).\nWe start by defining the random variable \\(X\\) as: \\(X=1\\) if we pick a blue bead at random and \\(X=0\\) if it is red. This implies that the population is a list of 0s and 1s. If we sample \\(N\\) beads, then the average of the draws \\(X_1, \\dots, X_N\\) is equivalent to the proportion of blue beads in our sample. This is because adding the \\(X\\)s is equivalent to counting the blue beads and dividing this count by the total \\(N\\) is equivalent to computing a proportion. We use the symbol \\(\\bar{X}\\) to represent this average. In general, in statistics textbooks a bar on top of a symbol means the average. The theory we just learned about the sum of draws becomes useful because the average is a sum of draws multiplied by the constant \\(1/N\\):\n\\[\\bar{X} = \\frac{1}{N} \\sum_{i=1}^N X_i\\]\nFor simplicity, let’s assume that the draws are independent: after we see each sampled bead, we return it to the urn. In this case, what do we know about the distribution of the sum of draws? First, we know that the expected value of the sum of draws is \\(N\\) times the average of the values in the urn. We know that the average of the 0s and 1s in the urn must be \\(p\\), the proportion of blue beads.\nHere we encounter an important difference with what we did in the Probability chapter: we don’t know what is in the urn. We know there are blue and red beads, but we don’t know how many of each. This is what we want to find out: we are trying to estimate \\(p\\).\n\n6.2.2 Parameters\nJust like we use variables to define unknowns in systems of equations, in statistical inference we define parameters to define unknown parts of our models. In the urn model which we are using to mimic an opinion poll, we do not know the proportion of blue beads in the urn. We define the parameters \\(p\\) to represent this quantity. \\(p\\) is the average of the urn because if we take the average of the 1s (blue) and 0s (red), we get the proportion of blue beads. Since our main goal is figuring out what is \\(p\\), we are going to estimate this parameter.\nThe ideas presented here on how we estimate parameters, and provide insights into how good these estimates are, extrapolate to many data science tasks. For example, we may want to determine the difference in health improvement between patients receiving treatment and a control group. We may ask, what are the health effects of smoking on a population? What are the differences in racial groups of fatal shootings by police? What is the rate of change in life expectancy in the US during the last 10 years? All these questions can be framed as a task of estimating a parameter from a sample."
  },
  {
    "objectID": "inference/parameters-estimates.html#polling-versus-forecasting",
    "href": "inference/parameters-estimates.html#polling-versus-forecasting",
    "title": "\n6  Parameters and Estimates\n",
    "section": "\n6.3 Polling versus forecasting",
    "text": "6.3 Polling versus forecasting\nBefore we continue, let’s make an important clarification related to the practical problem of forecasting the election. If a poll is conducted four months before the election, it is estimating the \\(p\\) for that moment and not for election day. The \\(p\\) for election night might be different since people’s opinions fluctuate through time. The polls provided the night before the election tend to be the most accurate since opinions don’t change that much in a day. However, forecasters try to build tools that model how opinions vary across time and try to predict the election night results taking into consideration the fact that opinions fluctuate. We will describe some approaches for doing this in a later section."
  },
  {
    "objectID": "inference/parameters-estimates.html#properties-of-our-estimate-expected-value-and-standard-error",
    "href": "inference/parameters-estimates.html#properties-of-our-estimate-expected-value-and-standard-error",
    "title": "\n6  Parameters and Estimates\n",
    "section": "\n6.4 Properties of our estimate: expected value and standard error",
    "text": "6.4 Properties of our estimate: expected value and standard error\nTo understand how good our estimate is, we will describe the statistical properties of the random variable defined above: the sample proportion \\(\\bar{X}\\). Remember that \\(\\bar{X}\\) is the sum of independent draws so the rules we covered in the probability chapter apply.\nUsing what we have learned, the expected value of the sum \\(N\\bar{X}\\) is \\(N \\times\\) the average of the urn, \\(p\\). So dividing by the non-random constant \\(N\\) gives us that the expected value of the average \\(\\bar{X}\\) is \\(p\\). We can write it using our mathematical notation:\n\\[\n\\mbox{E}(\\bar{X}) = p\n\\]\nWe can also use what we learned to figure out the standard error: the standard error of the sum is \\(\\sqrt{N} \\times\\) the standard deviation of the urn. Can we compute the standard error of the urn? We learned a formula that tells us that it is \\((1-0) \\sqrt{p (1-p)}\\) = \\(\\sqrt{p (1-p)}\\). Because we are dividing the sum by \\(N\\), we arrive at the following formula for the standard error of the average:\n\\[\n\\mbox{SE}(\\bar{X}) = \\sqrt{p(1-p)/N}\n\\]\nThis result reveals the power of polls. The expected value of the sample proportion \\(\\bar{X}\\) is the parameter of interest \\(p\\) and we can make the standard error as small as we want by increasing \\(N\\). The law of large numbers tells us that with a large enough poll, our estimate converges to \\(p\\).\nIf we take a large enough poll to make our standard error about 1%, we will be quite certain about who will win. But how large does the poll have to be for the standard error to be this small?\nOne problem is that we do not know \\(p\\), so we can’t compute the standard error. However, for illustrative purposes, let’s assume that \\(p=0.51\\) and make a plot of the standard error versus the sample size \\(N\\):\n\n\n\n\n\n\n\n\nFrom the plot we see that we would need a poll of over 10,000 people to get the standard error that low. We rarely see polls of this size due in part to costs. From the Real Clear Politics table, we learn that the sample sizes in opinion polls range from 500-3,500 people. For a sample size of 1,000 and \\(p=0.51\\), the standard error is:\n\nsqrt(p*(1 - p))/sqrt(1000)\n#&gt; [1] 0.0158\n\nor 1.5 percentage points. So even with large polls, for close elections, \\(\\bar{X}\\) can lead us astray if we don’t realize it is a random variable. Nonetheless, we can actually say more about how close we get the \\(p\\) and we do that in Section Chapter 7."
  },
  {
    "objectID": "inference/parameters-estimates.html#exercises",
    "href": "inference/parameters-estimates.html#exercises",
    "title": "\n6  Parameters and Estimates\n",
    "section": "\n6.5 Exercises",
    "text": "6.5 Exercises\n1. Suppose you poll a population in which a proportion \\(p\\) of voters are Democrats and \\(1-p\\) are Republicans. Your sample size is \\(N=25\\). Consider the random variable \\(S\\) which is the total number of Democrats in your sample. What is the expected value of this random variable? Hint: it’s a function of \\(p\\).\n2. What is the standard error of \\(S\\) ? Hint: it’s a function of \\(p\\).\n3. Consider the random variable \\(S/N\\). This is equivalent to the sample average, which we have been denoting as \\(\\bar{X}\\). What is the expected value of the \\(\\bar{X}\\)? Hint: it’s a function of \\(p\\).\n4. What is the standard error of \\(\\bar{X}\\)? Hint: it’s a function of \\(p\\).\n5. Write a line of code that gives you the standard error se for the problem above for several values of \\(p\\), specifically for p &lt;- seq(0, 1, length = 100). Make a plot of se versus p.\n6. Copy the code above and put it inside a for-loop to make the plot for \\(N=25\\), \\(N=100\\), and \\(N=1000\\).\n7. If we are interested in the difference in proportions, \\(\\mu = p - (1-p)\\), our estimate is \\(\\hat{\\mu} = \\bar{X} - (1-\\bar{X})\\). Use the rules we learned about sums of random variables and scaled random variables to derive the expected value of \\(\\hat{\\mu}\\).\n8. What is the standard error of \\(\\hat{\\mu}\\)?\n9. If the actual \\(p=.45\\), it means the Republicans are winning by a relatively large margin since \\(\\mu = -.1\\), which is a 10% margin of victory. In this case, what is the standard error of \\(2\\hat{X}-1\\) if we take a sample of \\(N=25\\)?\n10. Given the answer to 9, which of the following best describes your strategy of using a sample size of \\(N=25\\)?\n\nThe expected value of our estimate \\(2\\bar{X}-1\\) is \\(\\mu\\), so our prediction will be right on.\nOur standard error is larger than the difference, so the chances of \\(2\\bar{X}-1\\) being positive and throwing us off were not that small. We should pick a larger sample size.\nThe difference is 10% and the standard error is about 0.2, therefore much smaller than the difference.\nBecause we don’t know \\(p\\), we have no way of knowing that making \\(N\\) larger would actually improve our standard error."
  },
  {
    "objectID": "inference/parameters-estimates.html#footnotes",
    "href": "inference/parameters-estimates.html#footnotes",
    "title": "\n6  Parameters and Estimates\n",
    "section": "",
    "text": "http://www.realclearpolitics.com↩︎\nhttp://www.realclearpolitics.com/epolls/2016/president/us/general_election_trump_vs_clinton-5491.html↩︎"
  },
  {
    "objectID": "inference/clt.html#a-monte-carlo-simulation",
    "href": "inference/clt.html#a-monte-carlo-simulation",
    "title": "7  Central Limit Theorem",
    "section": "\n7.1 A Monte Carlo simulation",
    "text": "7.1 A Monte Carlo simulation\nSuppose we want to use a Monte Carlo simulation to corroborate the tools we have built using probability theory. To create the simulation, we would write code like this:\n\nB &lt;- 10000\nN &lt;- 1000\nx_hat &lt;- replicate(B, {\n  x &lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))\n  mean(x)\n})\n\nThe problem is, of course, we don’t know p. We could construct an urn like the one pictured above and run an analog (without a computer) simulation. It would take a long time, but you could take 10,000 samples, count the beads and keep track of the proportions of blue. We can use the function take_poll(n=1000) instead of drawing from an actual urn, but it would still take time to count the beads and enter the results.\nOne thing we therefore do to corroborate theoretical results is to pick one or several values of p and run the simulations. Let’s set p=0.45. We can then simulate a poll:\n\np &lt;- 0.45\nN &lt;- 1000\n\nx &lt;- sample(c(0, 1), size = N, replace = TRUE, prob = c(1 - p, p))\nx_hat &lt;- mean(x)\n\nIn this particular sample, our estimate is x_hat. We can use that code to do a Monte Carlo simulation:\n\nB &lt;- 10000\nx_hat &lt;- replicate(B, {\n  x &lt;- sample(c(0, 1), size = N, replace = TRUE, prob = c(1 - p, p))\n  mean(x)\n})\n\nTo review, the theory tells us that \\(\\bar{X}\\) is approximately normally distributed, has expected value \\(p=\\) 0.45 and standard error \\(\\sqrt{p(1-p)/N}\\) = 0.0157321. The simulation confirms this:\n\nmean(x_hat)\n#&gt; [1] 0.45\nsd(x_hat)\n#&gt; [1] 0.0158\n\nA histogram and qq-plot confirm that the normal approximation is accurate as well:\n\n\n\n\n\n\n\n\nOf course, in real life we would never be able to run such an experiment because we don’t know \\(p\\). But we could run it for various values of \\(p\\) and \\(N\\) and see that the theory does indeed work well for most values. You can easily do this by re-running the code above after changing p and N."
  },
  {
    "objectID": "inference/clt.html#the-spread",
    "href": "inference/clt.html#the-spread",
    "title": "7  Central Limit Theorem",
    "section": "\n7.2 The spread",
    "text": "7.2 The spread\nThe competition is to predict the spread, not the proportion \\(p\\). However, because we are assuming there are only two parties, we know that the spread is \\(\\mu = p - (1-p) = 2p - 1\\). As a result, everything we have done can easily be adapted to an estimate of \\(\\mu\\). Once we have our estimate \\(\\bar{X}\\) and \\(\\hat{\\mbox{SE}}(\\bar{X})\\), we estimate the spread with \\(2\\bar{X} - 1\\) and, since we are multiplying by 2, the standard error is \\(2\\hat{\\mbox{SE}}(\\bar{X})\\). Note that subtracting 1 does not add any variability so it does not affect the standard error.\nFor our 25 item sample above, our estimate \\(p\\) is .48 with margin of error .20 and our estimate of the spread is 0.04 with margin of error .40. Again, not a very useful sample size. However, the point is that once we have an estimate and standard error for \\(p\\), we have it for the spread \\(\\mu\\).\n\n\n\n\n\n\nWe use \\(\\mu\\) the denote the spread here and in the next chapters because this is the typical notation used in statistical textbooks for the parameter of interest. The reason we use \\(\\mu\\) is because a populuation mean is often the parameter of interest and \\(\\mu\\) is the Greek letter for m."
  },
  {
    "objectID": "inference/clt.html#bias-why-not-run-a-very-large-poll",
    "href": "inference/clt.html#bias-why-not-run-a-very-large-poll",
    "title": "7  Central Limit Theorem",
    "section": "\n7.3 Bias: why not run a very large poll?",
    "text": "7.3 Bias: why not run a very large poll?\nFor realistic values of \\(p\\), say from 0.35 to 0.65, if we run a very large poll with 100,000 people, theory tells us that we would predict the election perfectly since the largest possible margin of error is around 0.3%:\n\n#&gt; Warning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\nOne reason is that running such a poll is very expensive. Another possibly more important reason is that theory has its limitations. Polling is much more complicated than picking beads from an urn. Some people might lie to pollsters and others might not have phones. But perhaps the most important way an actual poll differs from an urn model is that we actually don’t know for sure who is in our population and who is not. How do we know who is going to vote? Are we reaching all possible voters? Hence, even if our margin of error is very small, it might not be exactly right that our expected value is \\(p\\). We call this bias. Historically, we observe that polls are indeed biased, although not by that much. The typical bias appears to be about 1-2%. This makes election forecasting a bit more interesting and we will talk about how to model this in a later chapter."
  },
  {
    "objectID": "inference/clt.html#exercises",
    "href": "inference/clt.html#exercises",
    "title": "7  Central Limit Theorem",
    "section": "\n7.4 Exercises",
    "text": "7.4 Exercises\n1. Write an urn model function that takes the proportion of Democrats \\(p\\) and the sample size \\(N\\) as arguments and returns the sample average if Democrats are 1s and Republicans are 0s. Call the function take_sample.\n2. Now assume p &lt;- 0.45 and that your sample size is \\(N=100\\). Take a sample 10,000 times and save the vector of mean(X) - p into an object called errors. Hint: use the function you wrote for exercise 1 to write this in one line of code.\n3. The vector errors contains, for each simulated sample, the difference between the actual \\(p\\) and our estimate \\(\\bar{X}\\). We refer to this difference as the error. Compute the average and make a histogram of the errors generated in the Monte Carlo simulation and select which of the following best describes their distributions:\n\nmean(errors)\nhist(errors)\n\n\nThe errors are all about 0.05.\nThe errors are all about -0.05.\nThe errors are symmetrically distributed around 0.\nThe errors range from -1 to 1.\n\n4. The error \\(\\bar{X}-p\\) is a random variable. In practice, the error is not observed because we do not know \\(p\\). Here we observe it because we constructed the simulation. What is the average size of the error if we define the size by taking the absolute value \\(\\mid \\bar{X} - p \\mid\\) ?\n5. The standard error is related to the typical size of the error we make when predicting. We say size because we just saw that the errors are centered around 0, so thus the average error value is 0. For mathematical reasons related to the Central Limit Theorem, we actually use the standard deviation of errors rather than the average of the absolute values to quantify the typical size. What is this standard deviation of the errors?\n6. The theory we just learned tells us what this standard deviation is going to be because it is the standard error of \\(\\bar{X}\\). What does theory tell us is the standard error of \\(\\bar{X}\\) for a sample size of 100?\n7. In practice, we don’t know \\(p\\), so we construct an estimate of the theoretical prediction based by plugging in \\(\\bar{X}\\) for \\(p\\). Compute this estimate. Set the seed at 1 with set.seed(1).\n8. Note how close the standard error estimates obtained from the Monte Carlo simulation (exercise 5), the theoretical prediction (exercise 6), and the estimate of the theoretical prediction (exercise 7) are. The theory is working and it gives us a practical approach to knowing the typical error we will make if we predict \\(p\\) with \\(\\bar{X}\\). Another advantage that the theoretical result provides is that it gives an idea of how large a sample size is required to obtain the precision we need. Earlier we learned that the largest standard errors occur for \\(p=0.5\\). Create a plot of the largest standard error for \\(N\\) ranging from 100 to 5,000. Based on this plot, how large does the sample size have to be to have a standard error of about 1%?\n\n100\n500\n2,500\n4,000\n\n9. For sample size \\(N=100\\), the central limit theorem tells us that the distribution of \\(\\bar{X}\\) is:\n\npractically equal to \\(p\\).\napproximately normal with expected value \\(p\\) and standard error \\(\\sqrt{p(1-p)/N}\\).\napproximately normal with expected value \\(\\bar{X}\\) and standard error \\(\\sqrt{\\bar{X}(1-\\bar{X})/N}\\).\nnot a random variable.\n\n10. Based on the answer from exercise 8, the error \\(\\bar{X} - p\\) is:\n\npractically equal to 0.\napproximately normal with expected value \\(0\\) and standard error \\(\\sqrt{p(1-p)/N}\\).\napproximately normal with expected value \\(p\\) and standard error \\(\\sqrt{p(1-p)/N}\\).\nnot a random variable.\n\n11. To corroborate your answer to exercise 9, make a qq-plot of the errors you generated in exercise 2 to see if they follow a normal distribution.\n12. If \\(p=0.45\\) and \\(N=100\\) as in exercise 2, use the CLT to estimate the probability that \\(\\bar{X}&gt;0.5\\). You can assume you know \\(p=0.45\\) for this calculation.\n13. Assume you are in a practical situation and you don’t know \\(p\\). Take a sample of size \\(N=100\\) and obtain a sample average of \\(\\bar{X} = 0.51\\). What is the CLT approximation for the probability that your error is equal to or larger than 0.01?"
  },
  {
    "objectID": "inference/confidence-intervals.html#a-monte-carlo-simulation",
    "href": "inference/confidence-intervals.html#a-monte-carlo-simulation",
    "title": "\n8  Confidence intervals\n",
    "section": "\n8.1 A Monte Carlo simulation",
    "text": "8.1 A Monte Carlo simulation\nWe can run a Monte Carlo simulation to confirm that, in fact, a 95% confidence interval includes \\(p\\) 95% of the time.\n\nN &lt;- 1000\nB &lt;- 10000\ninside &lt;- replicate(B, {\n  x &lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))\n  x_hat &lt;- mean(x)\n  se_hat &lt;- sqrt(x_hat * (1 - x_hat) / N)\n  between(p, x_hat - 1.96 * se_hat, x_hat + 1.96 * se_hat)\n})\nmean(inside)\n#&gt; [1] 0.948\n\nThe following plot shows the first 100 confidence intervals. In this case, we created the simulation so the black line denotes the parameter we are trying to estimate:\n\n\n\n\n\n\n\n\n::: {.callout-note title = “The correct language”}\nWhen using the theory we described above, it is important to remember that it is the intervals that are random, not \\(p\\). In the plot above, we can see the random intervals moving around and \\(p\\), represented with the vertical line, staying in the same place. The proportion of blue in the urn \\(p\\) is not. So the 95% relates to the probability that this random interval falls on top of \\(p\\). Saying the \\(p\\) has a 95% chance of being between this and that is technically an incorrect statement because \\(p\\) is not random. :::"
  },
  {
    "objectID": "inference/confidence-intervals.html#exercises",
    "href": "inference/confidence-intervals.html#exercises",
    "title": "\n8  Confidence intervals\n",
    "section": "\n8.2 Exercises",
    "text": "8.2 Exercises\nFor these exercises, we will use actual polls from the 2016 election. You can load the data from the dslabs package.\n\nlibrary(dslabs)\n\nSpecifically, we will use all the national polls that ended within one week before the election.\n\nlibrary(tidyverse)\npolls &lt;- polls_us_election_2016 |&gt; \n  filter(enddate &gt;= \"2016-10-31\" & state == \"U.S.\") \n\n1. For the first poll, you can obtain the samples size and estimated Clinton percentage with:\n\nN &lt;- polls$samplesize[1]\nx_hat &lt;- polls$rawpoll_clinton[1]/100\n\nAssume there are only two candidates and construct a 95% confidence interval for the election night proportion \\(p\\).\n2. Now use dplyr to add a confidence interval as two columns, call them lower and upper, to the object poll. Then use select to show the pollster, enddate, x_hat,lower, upper variables. Hint: define temporary columns x_hat and se_hat.\n3. The final tally for the popular vote was Clinton 48.2% and Trump 46.1%. Add a column, call it hit, to the previous table stating if the confidence interval included the true proportion \\(p=0.482\\) or not.\n4. For the table you just created, what proportion of confidence intervals included \\(p\\)?\n5. If these confidence intervals are constructed correctly, and the theory holds up, what proportion should include \\(p\\)?\n6. A much smaller proportion of the polls than expected produce confidence intervals containing \\(p\\). If you look closely at the table, you will see that most polls that fail to include \\(p\\) are underestimating. The reason for this is undecided voters, individuals polled that do not yet know who they will vote for or do not want to say. Because, historically, undecideds divide evenly between the two main candidates on election day, it is more informative to estimate the spread or the difference between the proportion of two candidates \\(\\mu\\), which in this election was \\(0. 482 - 0.461 = 0.021\\). Assume that there are only two parties and that \\(\\mu = 2p - 1\\), redefine polls as below and re-do exercise 1, but for the difference.\n\npolls &lt;- polls_us_election_2016 |&gt; \n  filter(enddate &gt;= \"2016-10-31\" & state == \"U.S.\")  |&gt;\n  mutate(mu_hat = rawpoll_clinton / 100 - rawpoll_trump / 100)\n\n7. Now repeat exercise 3, but for the difference.\n8. Now repeat exercise 4, but for the difference.\n9. Although the proportion of confidence intervals goes up substantially, it is still lower than 0.95. In the next chapter, we learn the reason for this. To motivate this, make a plot of the error, the difference between each poll’s estimate and the actual \\(mu=0.021\\). Stratify by pollster.\n10. Redo the plot that you made for exercise 9, but only for pollsters that took five or more polls."
  },
  {
    "objectID": "inference/hypothesis-testing.html#p-values",
    "href": "inference/hypothesis-testing.html#p-values",
    "title": "9  Hypothesis testing",
    "section": "\n9.1 p-values",
    "text": "9.1 p-values\nSuppose we take a random sample of \\(N=100\\) and we observe \\(52\\) blue beads, which gives us \\(\\bar{X} = 0.52\\). This seems to be pointing to the existence of more blue than red beads since 0.52 is larger than 0.5. However, we know there is chance involved in this process and we could get a 52 even when the actual \\(p=0.5\\). We call the assumption that \\(p = 0.5\\) a null hypothesis. The null hypothesis is the skeptic’s hypothesis.\nWe have observed a random variable \\(\\bar{X} = 0.52\\) and the p-value is the answer to the question: how likely is it to see a value this large, when the null hypothesis is true? If the p-value is small enough we reject the null hypothesis and say that the results are statistically significant.\n\nThe p-value of 0.05 as a threshold for statistical significance is conventionally used in many areas of research. A cutoff of 0.01 is also used to define highly significance. The choice of 0.05 is somewhat arbitrary and was popularized by the British statistician Ronald Fisher in the 1920s. We do not recommend using the cutoff without justification and try to avoid the phrase statistically significance.\n\nTo obtain a p-value for our example we write:\n\\[\\mbox{Pr}(\\mid \\bar{X} - 0.5 \\mid &gt; 0.02 ) \\]\nassuming the \\(p=0.5\\). Under the null hypothesis we know that:\n\\[\n\\sqrt{N}\\frac{\\bar{X} - 0.5}{\\sqrt{0.5(1-0.5)}}\n\\]\nis standard normal. We therefore can compute the probability above, which is the p-value.\n\\[\\mbox{Pr}\\left(\\sqrt{N}\\frac{\\mid \\bar{X} - 0.5\\mid}{\\sqrt{0.5(1-0.5)}} &gt; \\sqrt{N} \\frac{0.02}{ \\sqrt{0.5(1-0.5)}}\\right)\\]\n\nN &lt;- 100\nz &lt;- sqrt(N)*0.02/0.5\n1 - (pnorm(z) - pnorm(-z))\n#&gt; [1] 0.689\n\nIn this case, there is actually a large chance of seeing 52 or larger under the null hypothesis.\nKeep in mind that there is a close connection between p-values and confidence intervals. If a 95% confidence interval of the spread does not include 0, we know that the p-value must be smaller than 0.05.\nTo learn more about p-values, you can consult any statistics textbook. However, in general, we prefer reporting confidence intervals over p-values since it gives us an idea of the size of the estimate. If we just report the p-value we provide no information about the significance of the finding in the context of the problem.\n\nWe can show mathematically that if a \\((1-\\alpha)\\times 100\\)% confidence interval does not contain the null hypothesis value, the null hypothesis is rejected with a p-value as smaller or smaller than \\(\\alpha\\). So statistical significance can be determined from confidence intervals. However, unlike the confidence interval, the p-value does not provide an estimate of the magnitude of the effect. For this reason we recommend avoiding p-values whenever you can compute a confidence interval."
  },
  {
    "objectID": "inference/hypothesis-testing.html#power",
    "href": "inference/hypothesis-testing.html#power",
    "title": "9  Hypothesis testing",
    "section": "\n9.2 Power",
    "text": "9.2 Power\nPollsters are not successful at providing correct confidence intervals, but rather at predicting who will win. When we took a 25 bead sample size, the confidence interval for the spread:\n\nN &lt;- 25\nx_hat &lt;- 0.48\n(2 * x_hat - 1) + c(-1.96, 1.96) * 2 * sqrt(x_hat * (1 - x_hat) / N)\n#&gt; [1] -0.432  0.352\n\nincludes 0. If this were a poll and we were forced to make a declaration, we would have to say it was a “toss-up”.\nA problem with our poll results is that given the sample size and the value of \\(p\\), we would have to sacrifice on the probability of an incorrect call to create an interval that does not include 0.\nThis does not mean that the election is close. It only means that we have a small sample size. In statistical textbooks this is called lack of power. In the context of polls, power is the probability of detecting spreads different from 0.\nBy increasing our sample size, we lower our standard error and therefore have a much better chance of detecting the direction of the spread."
  },
  {
    "objectID": "inference/hypothesis-testing.html#exercises",
    "href": "inference/hypothesis-testing.html#exercises",
    "title": "9  Hypothesis testing",
    "section": "\n9.3 Exercises",
    "text": "9.3 Exercises\n\nGenerate a sample of size \\(N=50\\) from an urn model with 50% blue beads:\n\n\nN &lt;- 50 \np &lt;- 0.5\nx &lt;- rbinom(N, 1, 0.5)\n\nThen compute a p-value testing if \\(p=0.5\\). Repeat this 10,000 times and report how often do we incorrectly is the p-value lower than 0.05? How often is it lower than 0.01?\n\nMake a histogram of the p-values you generated in exercise 1. Which of the following seems to be true:\n\n\nThe p-values are all 0.05\nThe p-values are normally distributed, CLT seems to hold.\nThe p-values are uniformly distributed\nThe p-values\n\n\n(Advanced) Demonstrate mathematically why see the histogram you see.\nGenerate a sample of size \\(N=50\\) from an urn model with 52% blue beads:\n\n\nN &lt;- 50 \np &lt;- 0.52\nx &lt;- rbinom(N, 1, 0.5)\n\nThen compute a p-value testing if \\(p=0.5\\). Repeat this 10,000 times and report how often do we incorrectly is the p-value larger than 0.05? Note that you are computing 1 - power.\n\nRepeat exercise for but for the following values:\n\n\nvalues &lt;- expand.grid(N = c(25, 50, 100, 500, 1000), p = seq(0.51 ,0.75, 0.01))\n\nPlot power as a function of \\(N\\) with a different color curve for each value of p."
  },
  {
    "objectID": "inference/models.html#case-study-poll-aggregators",
    "href": "inference/models.html#case-study-poll-aggregators",
    "title": "10  Data-driven models",
    "section": "\n10.1 Case study: poll aggregators",
    "text": "10.1 Case study: poll aggregators\nAs we described earlier, a few weeks before the 2012 election Nate Silver was giving Obama a 90% chance of winning. How was Mr. Silver so confident? We will use a Monte Carlo simulation to illustrate the insight Mr. Silver had and others missed. To do this, we generate results for 12 polls taken the week before the election. We mimic sample sizes from actual polls and construct and report 95% confidence intervals for each of the 12 polls. We save the results from this simulation in a data frame and add a poll ID column.\n\nlibrary(tidyverse)\nlibrary(dslabs)\nmu &lt;- 0.039\nNs &lt;- c(1298, 533, 1342, 897, 774, 254, 812, 324, 1291, 1056, 2172, 516)\np &lt;- (mu + 1) / 2\n\npolls &lt;- map_df(Ns, function(N) {\n  x &lt;- sample(c(0, 1), size = N, replace = TRUE, prob = c(1 - p, p))\n  x_hat &lt;- mean(x)\n  se_hat &lt;- sqrt(x_hat * (1 - x_hat) / N)\n  list(estimate = 2 * x_hat - 1, \n    low = 2*(x_hat - 1.96*se_hat) - 1, \n    high = 2*(x_hat + 1.96*se_hat) - 1,\n    sample_size = N)\n}) |&gt; mutate(poll = seq_along(Ns))\n\nHere is a visualization showing the intervals the pollsters would have reported for the difference between Obama and Romney:\n\n\n\n\n\n\n\n\nNot surprisingly, all 12 polls report confidence intervals that include the election night result (dashed line). However, all 12 polls also include 0 (solid black line) as well. Therefore, if asked individually for a prediction, the pollsters would have to say: it’s a toss-up. Below we describe a key insight they are missing.\nPoll aggregators, such as Nate Silver, realized that by combining the results of different polls you could greatly improve precision. By doing this, we are effectively conducting a poll with a huge sample size. We can therefore report a smaller 95% confidence interval and a more precise prediction.\nAlthough as aggregators we do not have access to the raw poll data, we can use mathematics to reconstruct what we would have obtained had we made one large poll with:\n\nsum(polls$sample_size)\n#&gt; [1] 11269\n\nparticipants. Basically, we construct an estimate of the spread, let’s call it \\(\\mu\\), with a weighted average in the following way:\n\nmu_hat &lt;- polls |&gt; \n  summarize(avg = sum(estimate*sample_size) / sum(sample_size)) |&gt; \n  pull(avg)\n\nOnce we have an estimate of \\(\\mu\\), we can construct an estimate for the proportion voting for Obama, which we can then use to estimate the standard error. Once we do this, we see that our margin of error is 0.0184545.\nThus, we can predict that the spread will be 3.1 plus or minus 1.8, which not only includes the actual result we eventually observed on election night, but is quite far from including 0. Once we combine the 12 polls, we become quite certain that Obama will win the popular vote.\n\n\n\n\n\n\n\n\nHowevever, this was just a simulation to illustrate the idea. Let’s look at real data from 2016 presidential election. Specifically, the following subset of the polls_us_election_2016 data in dslabs which includes results for national polls, as well as state polls, taken during the year prior to the election and organized by FiveThirtyEight. For this first example, we will filter the data to include national polls conducted during the week before the election. We also remove polls that FiveThirtyEight has determined not to be reliable and graded with a “B” or less. Some polls have not been graded and we include those:\n\nlibrary(dslabs)\npolls &lt;- polls_us_election_2016 |&gt; \n  filter(state == \"U.S.\" & enddate &gt;= \"2016-10-31\" &\n           (grade %in% c(\"A+\",\"A\",\"A-\",\"B+\") | is.na(grade)))\n\nWe add a spread estimate:\n\npolls &lt;- polls |&gt; \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)\n\nFor this example, we will assume that there are only two parties and call \\(p\\) the proportion voting for Clinton and \\(1-p\\) the proportion voting for Trump. We are interested in the spread \\(2p-1\\). Let’s call the spread \\(\\mu\\) (for difference).\nWe have 49 estimates of the spread. The theory we learned from sampling models tells us that these estimates are a random variable with a probability distribution that is approximately normal. The expected value is the election night spread \\(\\mu\\) and the standard error is \\(2\\sqrt{p (1 - p) / N}\\). Assuming the urn model we described earlier is a good one, we can use this information to construct a confidence interval based on the aggregated data. The estimated spread is:\n\nmu_hat &lt;- polls |&gt; \n  summarize(mu_hat = sum(spread * samplesize) / sum(samplesize)) |&gt; \n  pull(mu_hat)\n\nand the standard error is:\n\np_hat &lt;- (mu_hat + 1)/2 \nmoe &lt;- 1.96 * 2 * sqrt(p_hat * (1 - p_hat) / sum(polls$samplesize))\nmoe\n#&gt; [1] 0.00662\n\nSo we report a spread of 1.43% with a margin of error of 0.66%. On election night, we discover that the actual percentage was 2.1%, which is outside a 95% confidence interval. What happened?\nA histogram of the reported spreads shows a problem:\n\npolls |&gt; ggplot(aes(spread)) + geom_histogram(color = \"black\", binwidth = .01)\n\n\n\n\n\n\n\nThe data does not appear to be normally distributed and the standard error appears to be larger than 0.0066232. The theory is not working here and in the next section we describe a useful data-driven model."
  },
  {
    "objectID": "inference/models.html#sample-avg-model",
    "href": "inference/models.html#sample-avg-model",
    "title": "10  Data-driven models",
    "section": "\n10.2 Beyond the simple sampling model",
    "text": "10.2 Beyond the simple sampling model\nNotice that data come various pollsters and some are taking several polls a week:\n\npolls |&gt; group_by(pollster) |&gt; summarize(n())\n#&gt; # A tibble: 15 × 2\n#&gt;   pollster                                                   `n()`\n#&gt;   &lt;fct&gt;                                                      &lt;int&gt;\n#&gt; 1 ABC News/Washington Post                                       7\n#&gt; 2 Angus Reid Global                                              1\n#&gt; 3 CBS News/New York Times                                        2\n#&gt; 4 Fox News/Anderson Robbins Research/Shaw & Company Research     2\n#&gt; 5 IBD/TIPP                                                       8\n#&gt; # ℹ 10 more rows\n\nLet’s visualize the data for the pollsters that are regularly polling:\n\n\n\n\n\n\n\n\nThis plot reveals an unexpected result. First, consider that the standard error predicted by theory for each poll is between 0.018 and 0.033:\n\npolls |&gt; group_by(pollster) |&gt; \n  filter(n() &gt;= 6) |&gt;\n  summarize(se = 2 * sqrt(p_hat * (1 - p_hat) / median(samplesize)))\n#&gt; # A tibble: 5 × 2\n#&gt;   pollster                     se\n#&gt;   &lt;fct&gt;                     &lt;dbl&gt;\n#&gt; 1 ABC News/Washington Post 0.0265\n#&gt; 2 IBD/TIPP                 0.0333\n#&gt; 3 Ipsos                    0.0225\n#&gt; 4 The Times-Picayune/Lucid 0.0196\n#&gt; 5 USC Dornsife/LA Times    0.0183\n\nThis agrees with the within poll variation we see. However, there appears to be differences across the polls. Note, for example, how the USC Dornsife/LA Times pollster is predicting a 4% win for Trump, while Ipsos is predicting a win larger than 5% for Clinton. The theory we learned says nothing about different pollsters producing polls with different expected values: all the polls should have the same expected value. FiveThirtyEight refers to these differences as house effects. We also call them pollster bias. Nothing in our simple urn model provides an explanation for these pollster-to-pollster differences. This model misspesification led to an overconfident interval that ended up not inclding the election nigth result. So, rather than modeling the process generating these values with an urn model, we instead model the pollster results directly. To do this, we start by collecting some data. Specifically, for each pollster we look at the last reported result before the election:\n\none_poll_per_pollster &lt;- polls |&gt; group_by(pollster) |&gt; \n  filter(enddate == max(enddate)) |&gt;\n  ungroup()\n\nHere is a histogram of the data for these 15 pollsters:\n\nqplot(spread, data = one_poll_per_pollster, binwidth = 0.01)\n#&gt; Warning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\nAlthough we are no longer using a model with red (Repbulicans) and blue (Democrate) beads in an urn, our new model can also be thought of as an urn mode but containing poll results from all possible pollsters and think of our $N=$15 data points \\(X_1,\\dots X_N\\) a as a random sample from this urn. To develop a useful model, we assume that the expected value of our urn is the actual spread \\(\\mu=2p-1\\), which implies that the sample average has expected value \\(\\mu\\).\nNow, because instead of 0s and 1s, our urn contains continuous numbers, the standard deviation of the urn is no longer \\(\\sqrt{p(1-p)}\\). Rather than voter sampling variability, the standard error now includes the pollster-to-pollster variability. Our new urn also includes the sampling variability from the polling. Regardless, this standard deviation is now an unknown parameter. In statistics textbooks, the Greek symbol \\(\\sigma\\) is used to represent this parameter.\nSo our new statistical model is that \\(X_1, \\dots, X_N\\) are a random sample with expected \\(\\mu\\) and standard deviation \\(\\sigma\\). The distribution, for now, is unspecified. But we consider \\(N\\) to be large enough to assume that the sample average \\(\\bar{X} = \\sum_{i=1}^N X_i\\) follows a normal distribution with expected value \\(\\mu\\) and standard error \\(\\sigma / \\sqrt{N}\\). We write\n\\[\n\\bar{X} \\sim \\mbox{N}(\\mu, \\sigma / \\sqrt{N})\n\\] Here the \\(\\sim\\) symbol tells us the random variable on the left of the symbol follows the distribution on the right. We use the notation \\(N(a,b)\\) to represent the normal distribution with mean \\(a\\) and standard deviation \\(b\\).\n\nThis model for the sample average will be used again the next chapter.\n\n\n10.2.1 Estimating the standard deviation\nThe model we have specfied has two unknown parameters: the expected value \\(\\mu\\) and the standard deviation \\(\\sigma\\). We know that the sample average \\(\\bar{X}\\) will be our estimte of \\(\\mu\\). But what about \\(\\sigma\\)?\nOur task is to estimate \\(\\mu\\). Because we model the observed values \\(X_1,\\dots X_N\\) as a random sample from the urn, for a large enough sample size \\(N\\), the probability distribution of the sample average \\(\\bar{X}\\) is approximately normal with expected value \\(\\mu\\) and standard error \\(\\sigma/\\sqrt{N}\\). If we are willing to consider \\(N=15\\) large enough, we can use this to construct confidence intervals.\nTheory tells us that we can estimate the urn model \\(\\sigma\\) with the sample standard deviation defined as\n\\[\ns = \\sqrt{ \\frac{1}{N-1} \\sum_{i=1}^N (X_i - \\bar{X})^2 }\n\\]\nNote that unlike for the population standard deviation definition, we now divide by \\(N-1\\). This makes \\(s\\) a better estimate of \\(\\sigma\\). There is a mathematical explanation for this, which is explained in most statistics textbooks, but we don’t cover it here.\nThe sd function in R computes the sample standard deviation:\n\nsd(one_poll_per_pollster$spread)\n#&gt; [1] 0.0242\n\n\n10.2.2 Computing a confidence interval\nWe are now ready to form a new confidence interval based on our new data-driven model:\n\nresults &lt;- one_poll_per_pollster |&gt; \n  summarize(avg = mean(spread), \n            se = sd(spread) / sqrt(length(spread))) |&gt; \n  mutate(start = avg - 1.96 * se, \n         end = avg + 1.96 * se) \nround(results * 100, 1)\n#&gt;   avg  se start end\n#&gt; 1 2.9 0.6   1.7 4.1\n\nOur confidence interval is wider now since it incorporates the pollster variability. It does include the election night result of 2.1%. Also, note that it was small enough not to include 0, which means we were confident Clinton would win the popular vote.\n\n10.2.3 The t-distribution\nAbove we made use of the CLT with a sample size of 15. Because we are estimating a second parameters \\(\\sigma\\), further variability is introduced into our confidence interval which results in intervals that are too small. For very large sample sizes this extra variability is negligible, but, in general, for values smaller than 30 we need to be cautious about using the CLT. However, if the data in the urn is known to follow a normal distribution, then we actually have mathematical theory that tells us how much bigger we need to make the intervals to account for the estimation of \\(\\sigma\\). Using this theory, we can construct confidence intervals for any \\(N\\). But again, this works only if the data in the urn is known to follow a normal distribution. So for the 0, 1 data of our previous urn model, this theory definitely does not apply.\n\n\n\n\n\n\nNote that 30 is a very general rule of thumb based on the case when the data come from a normal distribution. There are cases when a large sample size is needed as well as cases when smaller sample sizes are good enough.\n\n\n\nThe statistic on which confidence intervals for \\(\\mu\\) are based is\n\\[\nZ = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{N}}\n\\]\nCLT tells us that Z is approximately normally distributed with expected value 0 and standard error 1. But in practice we don’t know \\(\\sigma\\) so we use:\n\\[\nt = \\frac{\\bar{X} - \\mu}{s/\\sqrt{N}}\n\\]\nThis is referred to a t-statistic. By substituting \\(\\sigma\\) with \\(s\\) we introduce some variability. The theory tells us that \\(t\\) follows a student t-distribution with \\(N-1\\) degrees of freedom. The degrees of freedom is a parameter that controls the variability via fatter tails:\n\n\n\n\n\n\n\n\nIf we are willing to assume the pollster effect data is normally distributed, based on the sample data \\(X_1, \\dots, X_N\\),\n\none_poll_per_pollster |&gt;\n  ggplot(aes(sample = spread)) + stat_qq()\n\n\n\n\n\n\n\nthen \\(t\\) follows a t-distribution with \\(N-1\\) degrees of freedom. So perhaps a better confidence interval for \\(\\mu\\) is:\n\nz &lt;- qt(0.975,  nrow(one_poll_per_pollster) - 1)\none_poll_per_pollster |&gt; \n  summarize(avg = mean(spread), moe = z*sd(spread)/sqrt(length(spread))) |&gt; \n  mutate(start = avg - moe, end = avg + moe) \n#&gt; # A tibble: 1 × 4\n#&gt;      avg    moe  start    end\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 0.0290 0.0134 0.0156 0.0424\n\nA bit larger than the one using normal is\n\nqt(0.975, 14)\n#&gt; [1] 2.14\n\nis bigger than\n\nqnorm(0.975)\n#&gt; [1] 1.96\n\nThis results in slightly larger confidence interval than we obtained before:\n\n#&gt;   start end\n#&gt; 1   1.6 4.2\n\nNote that using the t-distribution and the t-statistic is the basis for t-tests, widely used approach for computing p-values. To learn more about t-tests, you can consult any statistics textbook.\nThe t-distribution can also be used to model errors in bigger deviations that are more likely than with the normal distribution, as seen in the densities we previously saw. Fivethirtyeight uses the t-distribution to generate errors that better model the deviations we see in election data. For example, in Wisconsin the average of six polls was 7% in favor of Clinton with a standard deviation of 1%, but Trump won by 0.7%. Even after taking into account the overall bias, this 7.7% residual is more in line with t-distributed data than the normal distribution.\n\npolls_us_election_2016 |&gt;\n  filter(state == \"Wisconsin\" &\n           enddate &gt;= \"2016-10-31\" & \n           (grade %in% c(\"A+\", \"A\", \"A-\", \"B+\") | is.na(grade))) |&gt;\n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) |&gt;\n  mutate(state = as.character(state)) |&gt;\n  left_join(results_us_election_2016, by = \"state\") |&gt;\n  mutate(actual = clinton/100 - trump/100) |&gt;\n  summarize(actual = first(actual), avg = mean(spread), \n            sd = sd(spread), n = n()) |&gt;\n  select(actual, avg, sd, n)\n#&gt;   actual    avg     sd n\n#&gt; 1 -0.007 0.0711 0.0104 6"
  },
  {
    "objectID": "inference/models.html#exercises",
    "href": "inference/models.html#exercises",
    "title": "10  Data-driven models",
    "section": "\n10.3 Exercises",
    "text": "10.3 Exercises\nWe have been using urn models to motivate the use of probability models. Most data science applications are not related to data obtained from urns. More common are data that come from individuals. The reason probability plays a role here is because the data come from a random sample. The random sample is taken from a population and the urn serves as an analogy for the population.\nLet’s revisit the heights dataset. Suppose we consider the males in our course the population.\n\nlibrary(dslabs)\nx &lt;- heights |&gt; filter(sex == \"Male\") |&gt;\n  pull(height)\n\n1. Mathematically speaking, x is our population. Using the urn analogy, we have an urn with the values of x in it. What are the average and standard deviation of our population?\n2. Call the population average computed above \\(\\mu\\) and the standard deviation \\(\\sigma\\). Now take a sample of size 50, with replacement, and construct an estimate for \\(\\mu\\) and \\(\\sigma\\).\n3. What does the theory tell us about the sample average \\(\\bar{X}\\) and how it is related to \\(\\mu\\)?\n\nIt is practically identical to \\(\\mu\\).\nIt is a random variable with expected value \\(\\mu\\) and standard error \\(\\sigma/\\sqrt{N}\\).\nIt is a random variable with expected value \\(\\mu\\) and standard error \\(\\sigma\\).\nContains no information.\n\n4. So how is this useful? We are going to use an oversimplified yet illustrative example. Suppose we want to know the average height of our male students, but we only get to measure 50 of the 708. We will use \\(\\bar{X}\\) as our estimate. We know from the answer to exercise 3 that the standard estimate of our error \\(\\bar{X}-\\mu\\) is \\(\\sigma/\\sqrt{N}\\). We want to compute this, but we don’t know \\(\\sigma\\). Based on what is described in this section, show your estimate of \\(\\sigma\\).\n5. Now that we have an estimate of \\(\\sigma\\), let’s call our estimate \\(s\\). Construct a 95% confidence interval for \\(\\mu\\).\n6. Now run a Monte Carlo simulation in which you compute 10,000 confidence intervals as you have just done. What proportion of these intervals include \\(\\mu\\)?\n7. Use the qnorm and qt functions to generate quantiles. Compare these quantiles for different degrees of freedom for the t-distribution. Use this to motivate the sample size of 30 rule of thumb."
  },
  {
    "objectID": "inference/bayes.html#bayes-theorem",
    "href": "inference/bayes.html#bayes-theorem",
    "title": "11  Bayesian statistics",
    "section": "\n11.1 Bayes theorem",
    "text": "11.1 Bayes theorem\nWe start by describing Bayes theorem. We do this using a hypothetical cystic fibrosis test as an example. Suppose a test for cystic fibrosis has an accuracy of 99%. We will use the following notation:\n\\[\n\\mbox{Prob}(+ \\mid D=1)=0.99, \\mbox{Prob}(- \\mid D=0)=0.99\n\\]\nwith \\(+\\) meaning a positive test and \\(D\\) representing if you actually have the disease (1) or not (0).\nSuppose we select a random person and they test positive. What is the probability that they have the disease? We write this as \\(\\mbox{Prob}(D=1 \\mid +)?\\) The cystic fibrosis rate is 1 in 3,900 which implies that \\(\\mbox{Prob}(D=1)=0.00025\\). To answer this question, we will use Bayes theorem, which in general tells us that:\n\\[\n\\mbox{Pr}(A \\mid B)  =  \\frac{\\mbox{Pr}(B \\mid A)\\mbox{Pr}(A)}{\\mbox{Pr}(B)}\n\\]\nThis equation applied to our problem becomes:\n\\[\n\\begin{aligned}\n\\mbox{Pr}(D=1 \\mid +) & =  \\frac{ P(+ \\mid D=1) \\cdot P(D=1)} {\\mbox{Pr}(+)} \\\\\n& =  \\frac{\\mbox{Pr}(+ \\mid D=1)\\cdot P(D=1)} {\\mbox{Pr}(+ \\mid D=1) \\cdot P(D=1) + \\mbox{Pr}(+ \\mid D=0) \\mbox{Pr}( D=0)}\n\\end{aligned}\n\\]\nPlugging in the numbers we get:\n\\[\n\\frac{0.99 \\cdot 0.00025}{0.99 \\cdot 0.00025 + 0.01 \\cdot (.99975)}  =  0.02\n\\]\nThis says that despite the test having 0.99 accuracy, the probability of having the disease given a positive test is only 0.02. This may appear counter-intuitive to some, but the reason this is the case is because we have to factor in the very rare probability that a person, chosen at random, has the disease. To illustrate this, we run a Monte Carlo simulation.\n\n11.1.1 Bayes theorem simulation\nThe following simulation is meant to help you visualize Bayes theorem. We start by randomly selecting 100,000 people from a population in which the disease in question has a 1 in 4,000 prevalence.\n\nprev &lt;- 0.00025\nN &lt;- 100000\noutcome &lt;- sample(c(\"Disease\",\"Healthy\"), N, replace = TRUE, \n                  prob = c(prev, 1 - prev))\n\nNote that there are very few people with the disease:\n\nN_D &lt;- sum(outcome == \"Disease\")\nN_D\n#&gt; [1] 23\nN_H &lt;- sum(outcome == \"Healthy\")\nN_H\n#&gt; [1] 99977\n\nAlso, there are many without the disease, which makes it more probable that we will see some false positives given that the test is not perfect. Now each person gets the test, which is correct 99% of the time:\n\naccuracy &lt;- 0.99\ntest &lt;- vector(\"character\", N)\ntest[outcome == \"Disease\"]  &lt;- sample(c(\"+\", \"-\"), N_D, replace = TRUE, \n                                    prob = c(accuracy, 1 - accuracy))\ntest[outcome == \"Healthy\"]  &lt;- sample(c(\"-\", \"+\"), N_H, replace = TRUE, \n                                    prob = c(accuracy, 1 - accuracy))\n\nBecause there are so many more controls than cases, even with a low false positive rate we get more controls than cases in the group that tested positive:\n\ntable(outcome, test)\n#&gt;          test\n#&gt; outcome       -     +\n#&gt;   Disease     0    23\n#&gt;   Healthy 99012   965\n\nFrom this table, we see that the proportion of positive tests that have the disease is 23 out of 988. We can run this over and over again to see that, in fact, the probability converges to about 0.022."
  },
  {
    "objectID": "inference/bayes.html#priors-posteriors-and-and-credible-intervals",
    "href": "inference/bayes.html#priors-posteriors-and-and-credible-intervals",
    "title": "11  Bayesian statistics",
    "section": "\n11.2 Priors, posteriors and and credible intervals",
    "text": "11.2 Priors, posteriors and and credible intervals\nIn the previous chapter we an estimate and margin of error for the difference in popular votes between Hillary Clinton and Donald Trump, which we denoted with \\(\\mu\\). The estimate was between 2 and 3 percent and the confidence interval did not include 0. A forecaster would use this to predict Hillary Clinton would win the popular vote. But to make a probabilistic statement about winning the election, we need to use a Bayesian.\nWe start the Bayesian approach by quantifying our knowledge before seeing any data. This is done using a probability distribution refereed to as a prior. For our example we could write:\n\\[\n\\mu \\sim N(\\theta, \\tau)\n\\]\nWe can think of \\(\\theta\\) as our best guess for the popular vote difference had we not seen any polling data and we can think of \\(\\tau\\) as quantifying how certain we feel about this guess. Generally, if we have expert knowledge related to \\(\\mu\\), we can try to quantify it with the prior disribution. In the case of election polls, experts use fundamentals, which include, for example, the state of the economy, to develop prior distributions. The data is used to update our initial guess or prior belief. This can be done mathematically if we define the distribution for the observed data, for any given \\(\\mu\\). In our particular example we would write down a model the average of our polls, which is the same as before:\n\\[\n\\bar{X} \\mid \\mu \\sim N(\\mu, \\sigma/\\sqrt{N})\n\\]\nAs before, \\(\\sigma\\) describes randomness due to sampling and the pollster effects. In the Bayesian contexts, this is referred to as the sampling distribution. Note that we write the conditional \\(\\bar{X} \\mid \\mu\\) becuase \\(\\mu\\) is now considered a random variable.\nWe do not show the derivations here, but we can now use Calculus and a version fo Bayes Theorem foto derive the distribution of \\(\\mu\\) conditional of the data, refered to as the posterior distribution. Specifcially we can show the \\(\\mu \\mid \\bar{X}\\) follows a normal distribution with expected value:\n\\[\n\\begin{aligned}\n\\mbox{E}(\\mu \\mid \\bar{X}) &= B \\theta + (1-B) \\bar{X}\\\\\n&= \\theta + (1-B)(\\bar{X}-\\theta)\\\\\n\\mbox{with } B &= \\frac{\\sigma^2/N}{\\sigma^2/N+\\tau^2}\n\\end{aligned}\n\\] and standard error :\n\\[\n\\mbox{SE}(\\mu \\mid \\bar{X})^2 = \\frac{1}{1/\\sigma^2+1/\\tau^2}.\n\\]\nNote that the expected value is a weighted average of our prior guess \\(\\theta\\) and the observed data \\(\\bar{X}\\). The weight depends on how certain we are about our prior belief, quantified by \\(\\tau\\), and the precision \\(\\sigma/N\\) of the summary of our observed data. This weighted average is sometimes referred to as shrinking because it shrinks estimates towards a prior value.\nThese quantities useful for updating our beliefs. Specifically, we use the posterior distribution not only to compute the expected value of \\(\\mu\\) given the observed data, but for any probability \\(\\alpha\\) we can construct intervals, centered at our estimate and with \\(\\alpha\\) chance of ocurring.\nTo compute a posterior distribution and construct a credible interval, we define a prior distribution with mean 0% and standard error 3.5% which can be interpreted as: before seing polling data, we don’t think any candidate has the advantage and a difference of up to 7% either way is possible. We compute the posterior distribution using the equations above:\n\ntheta &lt;- 0\ntau &lt;- 0.035\nsigma &lt;- results$se\nx_bar &lt;- results$avg\nB &lt;- sigma^2 / (sigma^2 + tau^2)\n\nposterior_mean &lt;- B*theta + (1 - B)*x_bar\nposterior_se &lt;- sqrt(1/(1/sigma^2 + 1/tau^2))\n\nposterior_mean\n#&gt; [1] 0.0281\nposterior_se\n#&gt; [1] 0.00615\n\nBecause we know the posterior distribution in normal, we can consturct a credible interval like this:\n\nposterior_mean + c(-1, 1) * qnorm(0.975) * posterior_se\n#&gt; [1] 0.0160 0.0401\n\nFurthermore, we can now make the probabilitic statement we could not make with the frequentists approach by computing the posterior probability of Hillary winning the popular vote. Specifically, \\(\\mbox{Pr}(\\mu&gt;0 \\mid \\bar{X})\\) can be computed like this:\n\n1 - pnorm(0, posterior_mean, posterior_se)\n#&gt; [1] 1\n\nThis says we are 100% sure Clinton will win the popular vote, which seems too overconfident. Also, it is not in agreement with FiveThirtyEight’s 81.4%. What explains this difference? There is a level of uncertainty that we are not yet describing, and we will get back to that in Chapter Chapter 12."
  },
  {
    "objectID": "inference/bayes.html#exercises",
    "href": "inference/bayes.html#exercises",
    "title": "11  Bayesian statistics",
    "section": "\n11.3 Exercises",
    "text": "11.3 Exercises\n1. In 1999, in England, Sally Clark1 was found guilty of the murder of two of her sons. Both infants were found dead in the morning, one in 1996 and another in 1998. In both cases, she claimed the cause of death was sudden infant death syndrome (SIDS). No evidence of physical harm was found on the two infants so the main piece of evidence against her was the testimony of Professor Sir Roy Meadow, who testified that the chances of two infants dying of SIDS was 1 in 73 million. He arrived at this figure by finding that the rate of SIDS was 1 in 8,500 and then calculating that the chance of two SIDS cases was 8,500 \\(\\times\\) 8,500 \\(\\approx\\) 73 million. Which of the following do you agree with?\n\nSir Meadow assumed that the probability of the second son being affected by SIDS was independent of the first son being affected, thereby ignoring possible genetic causes. If genetics plays a role then: \\(\\mbox{Pr}(\\mbox{second case of SIDS} \\mid \\mbox{first case of SIDS}) &gt; \\mbox{P}r(\\mbox{first case of SIDS})\\).\nNothing. The multiplication rule always applies in this way: \\(\\mbox{Pr}(A \\mbox{ and } B) =\\mbox{Pr}(A)\\mbox{Pr}(B)\\)\n\nSir Meadow is an expert and we should trust his calculations.\nNumbers don’t lie.\n\n2. Let’s assume that there is in fact a genetic component to SIDS and the probability of \\(\\mbox{Pr}(\\mbox{second case of SIDS} \\mid \\mbox{first case of SIDS}) = 1/100\\), is much higher than 1 in 8,500. What is the probability of both of her sons dying of SIDS?\n3. Many press reports stated that the expert claimed the probability of Sally Clark being innocent as 1 in 73 million. Perhaps the jury and judge also interpreted the testimony this way. This probability can be written as the probability of a mother is a son-murdering psychopath given that two of her children are found dead with no evidence of physical harm. According to Bayes’ rule, what is this?\n4. Assume that the chance of a son-murdering psychopath finding a way to kill her children, without leaving evidence of physical harm, is:\n\\[\n\\mbox{Pr}(A \\mid B) = 0.50\n\\]\nwith A = two of her children are found dead with no evidence of physical harm and B = a mother is a son-murdering psychopath = 0.50. Assume that the rate of son-murdering psychopaths mothers is 1 in 1,000,000. According to Bayes’ theorem, what is the probability of \\(\\mbox{Pr}(B \\mid A)\\) ?\n5/. After Sally Clark was found guilty, the Royal Statistical Society issued a statement saying that there was “no statistical basis” for the expert’s claim. They expressed concern at the “misuse of statistics in the courts”. Eventually, Sally Clark was acquitted in June 2003. What did the expert miss?\n\nHe made an arithmetic error.\nHe made two mistakes. First, he misused the multiplication rule and did not take into account how rare it is for a mother to murder her children. After using Bayes’ rule, we found a probability closer to 0.5 than 1 in 73 million.\nHe mixed up the numerator and denominator of Bayes’ rule.\nHe did not use R.\n\n6. Florida is one of the most closely watched states in the U.S. election because it has many electoral votes, and the election is generally close, and Florida tends to be a swing state that can vote either way. Create the following table with the polls taken during the last two weeks:\n\nlibrary(tidyverse)\nlibrary(dslabs)\npolls &lt;- polls_us_election_2016 |&gt; \n  filter(state == \"Florida\" & enddate &gt;= \"2016-11-04\" ) |&gt; \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)\n\nTake the average spread of these polls. The CLT tells us this average is approximately normal. Calculate an average and provide an estimate of the standard error. Save your results in an object called results.\n7. Now assume a Bayesian model that sets the prior distribution for Florida’s election night spread \\(\\mu\\) to be Normal with expected value \\(\\theta\\) and standard deviation \\(\\tau\\). What are the interpretations of \\(\\theta\\) and \\(\\tau\\)?\n\n\n\\(\\theta\\) and \\(\\tau\\) are arbitrary numbers that let us make probability statements about \\(\\mu\\).\n\n\\(\\theta\\) and \\(\\tau\\) summarize what we would predict for Florida before seeing any polls. Based on past elections, we would set \\(\\mu\\) close to 0 because both Republicans and Democrats have won, and \\(\\tau\\) at about \\(0.02\\), because these elections tend to be close.\n\n\\(\\theta\\) and \\(\\tau\\) summarize what we want to be true. We therefore set \\(\\theta\\) at \\(0.10\\) and \\(\\tau\\) at \\(0.01\\).\nThe choice of prior has no effect on Bayesian analysis.\n\n8. The CLT tells us that our estimate of the spread \\(\\hat{\\mu}\\) has normal distribution with expected value \\(\\mu\\) and standard deviation \\(\\sigma\\) calculated in problem 6. Use the formulas we showed for the posterior distribution to calculate the expected value of the posterior distribution if we set \\(\\theta = 0\\) and \\(\\tau = 0.01\\).\n9. Now compute the standard deviation of the posterior distribution.\n10. Using the fact that the posterior distribution is normal, create an interval that has a 95% probability of occurring centered at the posterior expected value. Note that we call these credible intervals.\n11. According to this analysis, what was the probability that Trump wins Florida?\n12. Now use sapply function to change the prior variance from seq(0.005, 0.05, len = 100) and observe how the probability changes by making a plot."
  },
  {
    "objectID": "inference/bayes.html#footnotes",
    "href": "inference/bayes.html#footnotes",
    "title": "11  Bayesian statistics",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Sally_Clark↩︎"
  },
  {
    "objectID": "inference/hierarchical-models.html#case-study-election-forecasting",
    "href": "inference/hierarchical-models.html#case-study-election-forecasting",
    "title": "12  Hierarchichal Models",
    "section": "\n12.1 Case study: election forecasting",
    "text": "12.1 Case study: election forecasting\nSince the 2008 elections, organizations other than FiveThirtyEight have started their own election forecasting groups that also aggregate polling data and uses statistical models to make predictions. However, in 2016 forecasters underestimated Trump’s chances of winning greatly. The day before the election the New York Times reported1 the following probabilities for Hillary Clinton winning the presidency:\n\n\n\n\n\nNYT\n538\nHuffPost\nPW\nPEC\nDK\nCook\nRoth\n\n\nWin Prob\n85%\n71%\n98%\n89%\n&gt;99%\n92%\nLean Dem\nLean Dem\n\n\n\n\nFor example, the Princeton Election Consortium (PEC) gave Trump less than 1% chance of winning, while the Huffington Post gave him a 2% chance. In contrast, FiveThirtyEight had Trump’s probability of winning at 29%, substantially higher than the others. In fact, four days before the election FiveThirtyEight published an article titled Trump Is Just A Normal Polling Error Behind Clinton2.\nSo why did FiveThirtyEight’s model fair so much better than others? How could PEC and Huffington Post get it so wrong if they were using the same data? In this chapter we describe how FiveThirtyEight used a hierarchical model to correctly account for key sources of variability and outperform all other forecasters. For illustrative purposes we will cotinue examining our popular vote example. In the final section we then describe the more complex approach used to forecast the electoral college result."
  },
  {
    "objectID": "inference/hierarchical-models.html#the-general-bias",
    "href": "inference/hierarchical-models.html#the-general-bias",
    "title": "12  Hierarchichal Models",
    "section": "\n12.2 The general bias",
    "text": "12.2 The general bias\nIn the previous chapter we computed the posterior probability of Hillary Clinton winning the popular vote with a standard Bayesian analysis and found it to be very close to 100%. However, FiveThirtyEight gave her a 81.4% chance3. What explains this difference? Below we describe the general bias, another source of variability, included in the FiveThirtyEight model, that accounts for the difference.\nAfter elections are over, one can look at the difference between pollster predictions and actual result. An important observation that our initial models did not take into account is that it is common to see a general bias that affects most pollsters in the same way making the observed data correlated. There is no agreed upon explanation for this, but we do observe it in historical data: in one election, the average of polls favors Democrats by 2%, then in the following election they favor Republicans by 1%, then in the next election there is no bias, then in the following one Republicans are favored by 3%, and so on. In 2016, the polls were biased in favor of the Democrats by 1-2%.\nHowever, although we know this bias term affects our polls, we have no way of knowing what this bias is until election night. So we can’t correct our polls accordingly. What we can do is include a term in our model that accounts for the variability."
  },
  {
    "objectID": "inference/hierarchical-models.html#mathematical-representations-of-the-hierarchical-model",
    "href": "inference/hierarchical-models.html#mathematical-representations-of-the-hierarchical-model",
    "title": "12  Hierarchichal Models",
    "section": "\n12.3 Mathematical representations of the hierarchical model",
    "text": "12.3 Mathematical representations of the hierarchical model\nSuppose we are collecting data from one pollster and we assume there is no general bias. The pollster collects several polls with a sample size of \\(N\\), so we observe several measurements of the spread \\(X_1, \\dots, X_J\\). Suppose the real proportion for Hillary is \\(p\\) and the difference is \\(\\mu\\). The urn model theory tells us that these random variables are normally distributed with expected value \\(\\mu\\) and standard error \\(2 \\sqrt{p(1-p)/N}\\):\n\\[\nX_j \\sim \\mbox{N}\\left(\\mu, \\sqrt{p(1-p)/N}\\right)\n\\]\nWe use the index \\(j\\) to represent the different polls conducted by this pollster. Here is a simulation for six polls assuming the spread is 2.1 and \\(N\\) is 2,000:\n\nset.seed(3)\nJ &lt;- 6\nN &lt;- 2000\nmu &lt;- .021\np &lt;- (mu + 1)/2\nX &lt;- rnorm(J, mu, 2 * sqrt(p * (1 - p) / N))\n\nNow suppose we have \\(J=6\\) polls from each of \\(I=5\\) different pollsters. For simplicity, let’s say all polls had the same sample size \\(N\\). The urn model tell us the distribution is the same for all pollsters so to simulate data, we use the same model for each:\n\nI &lt;- 5\nJ &lt;- 6\nN &lt;- 2000\nX &lt;- sapply(1:I, function(i){\n  rnorm(J, mu, 2 * sqrt(p * (1 - p) / N))\n})\n\nAs expected, the simulated data does not really seem to capture the features of the actual data because it does not account for pollster-to-pollster variability:\n\n\n\n\n\n\n\n\nTo fix this, we need to represent the two levels of variability and we need two indexes, one for pollster and one for the polls each pollster takes. We use \\(X_{ij}\\) with \\(i\\) representing the pollster and \\(j\\) representing the \\(j\\)-th poll from that pollster. The model is now augmented to include pollster effects \\(h_i\\), referred to as house effects by FiveThirtyEight, with standard deviation \\(\\sigma_h\\):\n\\[\n\\begin{aligned}\nh_i &\\sim \\mbox{N}\\left(0, \\sigma_h\\right)\\\\\nX_{i,j} \\mid h_i &\\sim \\mbox{N}\\left(\\mu + h_i, \\sqrt{p(1-p)/N}\\right)\n\\end{aligned}\n\\]\nTo simulate data from a specific pollster, we first need to draw an \\(h_i\\) and the generate individual poll data after adding this effect. Here is how we would do it for one specific pollster. We assume \\(\\sigma_h\\) is 0.025:\n\nI &lt;- 5\nJ &lt;- 6\nN &lt;- 2000\nmu &lt;- .021\np &lt;- (mu + 1) / 2\nh &lt;- rnorm(I, 0, 0.025)\nX &lt;- sapply(1:I, function(i){\n  mu + h[i] + rnorm(J, 0, 2 * sqrt(p * (1 - p) / N))\n})\n\nThe simulated data now looks more like the actual data:\n\n\n\n\n\n\n\n\nNote that \\(h_i\\) is common to all the observed spreads from a specific pollster. Different pollsters have a different \\(h_i\\), which explains why we can see the groups of points shift up and down from pollster to pollster.\nNow, in the model above, we assume the average house effect is 0. We think that for every pollster biased in favor of our party, there is another one in favor of the other and assume the standard deviation is \\(\\sigma_h\\). But historically we see that every election has a general bias affecting all polls. We can observe this with the 2016 data, but if we collect historical data, we see that the average of polls misses by more than models like the one above predict. To see this, we would take the average of polls for each election year and compare it to the actual value. If we did this, we would see a difference with a standard deviation of between 2-3%. To incorporate this into the model, we can add another level account for this variability:\n\\[\n\\begin{aligned}\nb &\\sim \\mbox{N}\\left(0, \\sigma_b\\right)\\\\\nh_j \\mid \\, b &\\sim \\mbox{N}\\left(b, \\sigma_h\\right)\\\\\nX_{i,j} | \\, h_j, b &\\sim \\mbox{N}\\left(\\mu + h_j, \\sqrt{p(1-p)/N}\\right)\n\\end{aligned}\n\\]\nThis model accounts for three levels of variability: 1) variability in the bias observed from election to election, quantified by \\(\\sigma_b\\), 2) pollster-to-pollster or house effect variability, quantified by \\(\\sigma_h\\), and 3) poll sampling variability, which we can derive to be \\(\\sqrt(p(1-p)/N)\\).\nNote that not including a term like \\(b\\) in the models, is what led many forecasters to be overconfident. This random variable changes from election to election, but for any given election, it is the same for all pollsters and polls within on election (note it does not have an index). This implies we can’t estimate \\(\\sigma_h\\) with data from just one election. It also implies that the random variables \\(X_{i,j}\\) for a fixed election year share the same \\(b\\) and are therefore correlated.\nOne way to interpret \\(b\\) is as the difference between the average of all polls from all pollsters and the actual result of the election. Because we don’t know the actual result until after the election, we can’t estimate \\(b\\) until after the election."
  },
  {
    "objectID": "inference/hierarchical-models.html#computing-a-posterior-probability",
    "href": "inference/hierarchical-models.html#computing-a-posterior-probability",
    "title": "12  Hierarchichal Models",
    "section": "\n12.4 Computing a posterior probability",
    "text": "12.4 Computing a posterior probability\n\n\n\n\n\n\nSome of the results presented in this section rely on calculations of the statistical properties of summaries based on correlated random variables. The learn about the related mathematical details we skip in this book, please consult a textbook on hierarchical models.\n\n\n\nNow let’s fit the model above to data. We will use the same data used in the previous chapters and saved in one_poll_per_pollster.\n\npolls &lt;- polls_us_election_2016 |&gt; \n  filter(state == \"U.S.\" & enddate &gt;= \"2016-10-31\" &\n           (grade %in% c(\"A+\",\"A\",\"A-\",\"B+\") | is.na(grade))) |&gt; \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)\n\none_poll_per_pollster &lt;- polls |&gt; group_by(pollster) |&gt; \n  filter(enddate == max(enddate)) |&gt;\n  ungroup()\n\nHere we have just one poll per pollster so we will drop the \\(j\\) index and represent the data as before with \\(X_1, \\dots, X_I\\). As a reminder we have data from \\(I=15\\) pollsters. Based on the model assumptions described above, we can mathematically show that the average \\(\\bar{X}\\)\n\nx_bar &lt;- mean(one_poll_per_pollster$spread)\n\nhas expected value \\(\\mu\\), thus it provides an unbiased estimate of the outcome of interest. However, how precise is this estimate? Can we use the observed stample standard deviation to construct an estimate of the standard error of \\(\\bar{X}\\)?\nIt turns out that, because the \\(X_i\\) are correlated, estimating the standard error is more complex than what we have described up to now. Specifically, using advanced statistical calculations not shown here, we can show that the typical variance (standard error squared) estimate\n\ns2 &lt;- with(one_poll_per_pollster, sd(spread)^2 / length(spread))\n\nwill consistently underestimate the true standard error by about \\(\\sigma_b^2\\). And, as mentioned earlier, to estimate \\(\\sigma_b\\), we need data from several elections. By collecting and analyzing polling data from several elections, FiveThirtyEight estimates this variability and find that \\(\\sigma_b \\approx 0.025\\). We can therefore greatly improve our standard error estimate by adding this quantity:\n\nsigma_b &lt;- 0.025\nse &lt;- sqrt(s2 + sigma_b^2)\n\nIf we redo the Bayesian calculation taking this variability into account, we get a result much closer to FiveThirtyEight’s:\n\nmu &lt;- 0\ntau &lt;- 0.035\nB &lt;- se^2 / (se^2 + tau^2)\nposterior_mean &lt;- B*mu + (1-B)*x_bar\nposterior_se &lt;- sqrt( 1/ (1/se^2 + 1/tau^2))\n\n1 - pnorm(0, posterior_mean, posterior_se)\n#&gt; [1] 0.817\n\nNote that by accounting for the general bias term, our Bayesian analysis now produces a posterior probability similar to that reported by FiveThirtyEight.\n\n\n\n\n\n\nNote that we are simplifying FiveThirtyEight’s calculations related to the general bias \\(b\\). For example, one of the many ways their analysis is more complex than the one presented here, is that they permit \\(b\\) vary across regions of the country. This helps because historically, we have observed geographical patterns in voting behaviors."
  },
  {
    "objectID": "inference/hierarchical-models.html#predicting-the-electoral-college",
    "href": "inference/hierarchical-models.html#predicting-the-electoral-college",
    "title": "12  Hierarchichal Models",
    "section": "\n12.5 Predicting the electoral college",
    "text": "12.5 Predicting the electoral college\nUp to now we have focused on the popular vote. But in the United States, elections are not decided by the popular vote but rather by what is known as the electoral college. Each state gets a number of electoral votes that depends, in a somewhat complex way, on the population size of the state. Here are the top 5 states ranked by electoral votes in 2016.\n\nresults_us_election_2016 |&gt; top_n(5, electoral_votes)\n#&gt;          state electoral_votes clinton trump others\n#&gt; 1   California              55    61.7  31.6    6.7\n#&gt; 2        Texas              38    43.2  52.2    4.5\n#&gt; 3      Florida              29    47.8  49.0    3.2\n#&gt; 4     New York              29    59.0  36.5    4.5\n#&gt; 5     Illinois              20    55.8  38.8    5.4\n#&gt; 6 Pennsylvania              20    47.9  48.6    3.6\n\nWith some minor exceptions we don’t discuss, the electoral votes are won all or nothing. For example, if you won California in 2016 by just 1 vote, you still get all 55 of its electoral votes. This means that by winning a few big states by a large margin, but losing many small states by small margins, you can win the popular vote and yet lose the electoral college. This happened in 1876, 1888, 2000, and 2016. The idea behind this is to avoid a few large states having the power to dominate the presidential election.\n\nMany people in the US consider the electoral college unfair and would like to see it abolished in favor of the popular vote.\n\nWe are now ready to predict the electoral college result for 2016. We start by aggregating results from a poll taken during the last week before the election. We use the grepl, which finds strings in character vectors, to remove polls that are not for entire states.\n\nresults &lt;- polls_us_election_2016 |&gt;\n  filter(state!=\"U.S.\" & \n           !grepl(\"CD\", state) & \n           enddate &gt;=\"2016-10-31\" & \n           (grade %in% c(\"A+\",\"A\",\"A-\",\"B+\") | is.na(grade))) |&gt;\n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) |&gt;\n  group_by(state) |&gt;\n  summarize(avg = mean(spread), sd = sd(spread), n = n()) |&gt;\n  mutate(state = as.character(state))\n\nHere are the five closest races according to the polls:\n\nresults |&gt; arrange(abs(avg))\n#&gt; # A tibble: 47 × 4\n#&gt;   state               avg     sd     n\n#&gt;   &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 Florida         0.00356 0.0163     7\n#&gt; 2 North Carolina -0.0073  0.0306     9\n#&gt; 3 Ohio           -0.0104  0.0252     6\n#&gt; 4 Nevada          0.0169  0.0441     7\n#&gt; 5 Iowa           -0.0197  0.0437     3\n#&gt; # ℹ 42 more rows\n\nWe now introduce the command left_join that will let us easily add the number of electoral votes for each state from the dataset us_electoral_votes_2016. Here, we simply say that the function combines the two datasets so that the information from the second argument is added to the information in the first:\n\nresults &lt;- left_join(results, results_us_election_2016, by = \"state\")\n\nNotice that some states have no polls because the winner is pretty much known:\n\nresults_us_election_2016 |&gt; filter(!state %in% results$state) |&gt; \n  pull(state)\n#&gt; [1] \"Rhode Island\"         \"Alaska\"               \"Wyoming\"             \n#&gt; [4] \"District of Columbia\"\n\nNo polls were conducted in DC, Rhode Island, Alaska, and Wyoming because Democrats are sure to win in the first two and Republicans in the last two.\nBecause we can’t estimate the standard deviation for states with just one poll, we will estimate it as the median of the standard deviations estimated for states with more than one poll:\n\nresults &lt;- results |&gt;\n  mutate(sd = ifelse(is.na(sd), median(results$sd, na.rm = TRUE), sd))\n\nTo make probabilistic arguments, we will use a Monte Carlo simulation. For each state, we apply the Bayesian approach to generate an election day \\(\\mu\\). We could construct the priors for each state based on recent history. However, to keep it simple, we assign a prior to each state that assumes we know nothing about what will happen. Since from election year to election year the results from a specific state don’t change that much, we will assign a standard deviation of 2% or \\(\\tau=0.02\\). For now, we will assume, incorrectly, that the poll results from each state are independent. The code for the Bayesian calculation under these assumptions looks like this:\n\nmu &lt;- 0\ntau &lt;- 0.02\nresults |&gt; mutate(sigma = sd/sqrt(n), \n                   B = sigma^2 / (sigma^2 + tau^2),\n                   posterior_mean = B * mu + (1 - B) * avg,\n                   posterior_se = sqrt(1/ (1/sigma^2 + 1/tau^2)))\n#&gt; # A tibble: 47 × 12\n#&gt;   state          avg       sd     n electoral_votes clinton trump others\n#&gt;   &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;           &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 Alabama    -0.149  0.0253       3               9    34.4  62.1    3.6\n#&gt; 2 Arizona    -0.0326 0.0270       9              11    45.1  48.7    6.2\n#&gt; 3 Arkansas   -0.151  0.000990     2               6    33.7  60.6    5.8\n#&gt; 4 California  0.260  0.0387       5              55    61.7  31.6    6.7\n#&gt; 5 Colorado    0.0452 0.0295       7               9    48.2  43.3    8.6\n#&gt; # ℹ 42 more rows\n#&gt; # ℹ 4 more variables: sigma &lt;dbl&gt;, B &lt;dbl&gt;, posterior_mean &lt;dbl&gt;,\n#&gt; #   posterior_se &lt;dbl&gt;\n\nThe estimates based on posterior do move the estimates towards 0, although the states with many polls are influenced less. This is expected as the more poll data we collect, the more we trust those results:\n\n\n\n\n\n\n\n\nNow we repeat this 10,000 times and generate an outcome from the posterior. In each iteration, we keep track of the total number of electoral votes for Clinton. Remember that Trump gets 270 minus the votes for Clinton. Also note that the reason we add 7 in the code is to account for Rhode Island and D.C.:\n\nB &lt;- 10000\nmu &lt;- 0\ntau &lt;- 0.02\nclinton_EV &lt;- replicate(B, {\n  results |&gt; mutate(sigma = sd/sqrt(n), \n                   B = sigma^2 / (sigma^2 + tau^2),\n                   posterior_mean = B * mu + (1 - B) * avg,\n                   posterior_se = sqrt(1 / (1/sigma^2 + 1/tau^2)),\n                   result = rnorm(length(posterior_mean), \n                                  posterior_mean, posterior_se),\n                   clinton = ifelse(result &gt; 0, electoral_votes, 0)) |&gt; \n    summarize(clinton = sum(clinton)) |&gt; \n    pull(clinton) + 7\n})\n\nmean(clinton_EV &gt; 269)\n#&gt; [1] 0.998\n\nThis model gives Clinton over 99% chance of winning. A similar prediction was made by the Princeton Election Consortium. We now know it was quite off. What happened?\nThe model above ignores the general bias and assumes the results from different states are independent. After the election, we realized that the general bias in 2016 was not that big: it was between 1 and 2%. But because the election was close in several big states and these states had a large number of polls, pollsters that ignored the general bias greatly underestimated the standard error. Using the notation we introduce, they assumed the standard error was \\(\\sqrt{\\sigma^2/N}\\) which with large N is quite smaller than the more accurate estimate \\(\\sqrt{\\sigma^2/N + \\sigma_b^2}\\). FiveThirtyEight, which models the general bias in a rather sophisticated way, reported a closer result. We can simulate the results now with a bias term. For the state level, the general bias can be larger so we set it at \\(\\sigma_b = 0.03\\):\n\ntau &lt;- 0.02\nbias_sd &lt;- 0.03\nclinton_EV_2 &lt;- replicate(1000, {\n  results |&gt; mutate(sigma = sqrt(sd^2/n  + bias_sd^2),  \n                   B = sigma^2 / (sigma^2 + tau^2),\n                   posterior_mean = B*mu + (1-B)*avg,\n                   posterior_se = sqrt( 1/ (1/sigma^2 + 1/tau^2)),\n                   result = rnorm(length(posterior_mean), \n                                  posterior_mean, posterior_se),\n                   clinton = ifelse(result&gt;0, electoral_votes, 0)) |&gt; \n    summarize(clinton = sum(clinton) + 7) |&gt; \n    pull(clinton)\n})\nmean(clinton_EV_2 &gt; 269)\n#&gt; [1] 0.848\n\nThis gives us a much more sensible estimate. Looking at the outcomes of the simulation, we see how the bias term adds variability to the final results.\n\n\n\n\n\n\n\n\nFiveThirtyEight includes many other features we do not include here. One is that they model variability with distributions that have high probabilities for extreme events compared to the normal. One way we could do this is by changing the distribution used in the simulation from a normal distribution to a t-distribution. FiveThirtyEight predicted a probability of 71%."
  },
  {
    "objectID": "inference/hierarchical-models.html#forecasting",
    "href": "inference/hierarchical-models.html#forecasting",
    "title": "12  Hierarchichal Models",
    "section": "\n12.6 Forecasting",
    "text": "12.6 Forecasting\nForecasters like to make predictions well before the election. The predictions are adapted as new polls come out. However, an important question forecasters must ask is: how informative are polls taken several weeks before the election about the actual election? Here we study the variability of poll results across time.\nTo make sure the variability we observe is not due to pollster effects, let’s study data from one pollster:\n\none_pollster &lt;- polls_us_election_2016 |&gt; \n  filter(pollster == \"Ipsos\" & state == \"U.S.\") |&gt; \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)\n\nSince there is no pollster effect, then perhaps the theoretical standard error matches the data-derived standard deviation. We compute both here:\n\nse &lt;- one_pollster |&gt; \n  summarize(empirical = sd(spread), \n            theoretical = 2 * sqrt(mean(spread) * (1 - mean(spread)) /\n                                     min(samplesize)))\nse\n#&gt;   empirical theoretical\n#&gt; 1    0.0403      0.0326\n\nBut the empirical standard deviation is higher than the highest possible theoretical estimate. Furthermore, the spread data does not look normal as the theory would predict:\n\n\n\n\n\n\n\n\nThe models we have described include pollster-to-pollster variability and sampling error. But this plot is for one pollster and the variability we see is certainly not explained by sampling error. Where is the extra variability coming from? The following plots make a strong case that it comes from time fluctuations not accounted for by the theory that assumes \\(p\\) is fixed:\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nSome of the peaks and valleys we see coincide with events such as the party conventions, which tend to give the candidate a boost. We can see the peaks and valleys are consistent across several pollsters:\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nThis implies that, if we are going to forecast, our model must include a term to accounts for the time effect. We need to write a model including a bias term for time, denote it \\(b_t\\). The standard deviation of \\(b_t\\) would depend on \\(t\\) since the closer we get to election day, the closer to 0 this bias term should be.\nPollsters also try to estimate trends from these data and incorporate these into their predictions. We can model the time trend \\(b_t\\) with a smooth function. We usually see the trend estimte not for the difference, but for the actual percentages for each candidate like this:\n\n\n\n\n\n\n\n\nOnce a model like the one above is selected, we can use historical and present data to estimate all the necessary parameters to make predictions. There is a variety of methods for estimating trends which we discuss in the Machine Learning part."
  },
  {
    "objectID": "inference/hierarchical-models.html#exercises",
    "href": "inference/hierarchical-models.html#exercises",
    "title": "12  Hierarchichal Models",
    "section": "\n12.7 Exercises",
    "text": "12.7 Exercises\n1. Create this table:\n\nlibrary(tidyverse)\nlibrary(dslabs)\npolls &lt;- polls_us_election_2016 |&gt; \n  filter(state != \"U.S.\" & enddate &gt;= \"2016-10-31\") |&gt; \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)\n\nNow for each poll use the CLT to create a 95% confidence interval for the spread reported by each poll. Call the resulting object cis with columns lower and upper for the limits of the confidence intervals. Use the select function to keep the columns state, startdate, end date, pollster, grade, spread, lower, upper.\n2. You can add the final result to the cis table you just created using the right_join function like this:\n\nadd &lt;- results_us_election_2016 |&gt; \n  mutate(actual_spread = clinton/100 - trump/100) |&gt; \n  select(state, actual_spread)\ncis &lt;- cis |&gt; \n  mutate(state = as.character(state)) |&gt; \n  left_join(add, by = \"state\")\n\nNow determine how often the 95% confidence interval includes the actual result.\n3. Repeat this, but show the proportion of hits for each pollster. Show only pollsters with more than 5 polls and order them from best to worst. Show the number of polls conducted by each pollster and the FiveThirtyEight grade of each pollster. Hint: use n=n(), grade = grade[1] in the call to summarize.\n4. Repeat exercise 3, but instead of pollster, stratify by state. Note that here we can’t show grades.\n5. Make a barplot based on the result of exercise 4. Use coord_flip.\n6. Add two columns to the cis table by computing, for each poll, the difference between the predicted spread and the actual spread, and define a column hit that is true if the signs are the same. Hint: use the function sign. Call the object resids.\n7. Create a plot like in exercise 5, but for the proportion of times the sign of the spread agreed.\n8. In exercise 7, we see that for most states the polls had it right 100% of the time. For only 9 states did the polls miss more than 25% of the time. In particular, notice that in Wisconsin every single poll got it wrong. In Pennsylvania and Michigan more than 90% of the polls had the signs wrong. Make a histogram of the errors. What is the median of these errors?\n9. We see that at the state level, the median error was 3% in favor of Clinton. The distribution is not centered at 0, but at 0.03. This is the general bias we described in the section above. Create a boxplot to see if the bias was general to all states or it affected some states differently. Use filter(grade %in% c(\"A+\",\"A\",\"A-\",\"B+\") | is.na(grade))) to only include pollsters with high grades.\n10. Some of these states only have a few polls. Repeat exercise 9, but only include states with 5 good polls or more. Hint: use group_by, filter then ungroup. You will see that the West (Washington, New Mexico, California) underestimated Hillary’s performance, while the Midwest (Michigan, Pennsylvania, Wisconsin, Ohio, Missouri) overestimated it. In our simulation, we did not model this behavior since we added general bias, rather than a regional bias. Note that some pollsters may now be modeling correlation between similar states and estimating this correlation from historical data. To learn more about this, you can learn about random effects and mixed models.\n11. In April 2013, José Iglesias, a professional baseball player was starting his career. He was performing exceptionally well. Specifically, he had a batting average (AVG) of .450. The batting average statistic is one way of measuring success. Roughly speaking, it tells us the success rate when batting. José had 9 successes out of 20 tries. An AVG of .450 means José has been successful 45% of the times he has batted which is rather high, historically speaking: no one has finished a season with an AVG of .400 or more since Ted Williams did it in 1941! We want to predict José’s batting average at the end of the season after players have about 500 tries or at bats. With the frequentist techniques we have no choice but to predict that his AVG will be .450 at the end of the season. Compute a confidence interval for the success rate.\n12. Despite the frequentist prediction of \\(.450\\) not a single baseball enthusiast would make this prediction. Why is this? One reason is that they now the estimate has much uncertainty. However, the main reason is that they are implicitly using a hierarchical model that factors in information from years of following baseball. Use the following code to explore the distribution of batting averages in the three seasons prior to 2013 and describe what this tells us.\n13. So is José lucky or is he the best batter seen in the last 50 years? Perhaps it’s a combination of both luck and talent. But how much of each? If we become convinced that he is lucky, we should trade him to a team that trusts the .450 observation and is maybe overestimating his potential. The hierarchical model provides a mathematical description of how we came to see the observation of .450. First, we pick a player at random with an intrinsic ability summarized by, for example, \\(\\mu\\). Then we see 20 random outcomes with success probability \\(\\mu\\). What model would you use for the first level of your hierarchical model?\n\nDescribe the second level of the hierarchical model.\n\n15. Apply the hierarchical model to José’s data. Suppose we want to predict his innate ability in the form of his true batting average \\(\\mu\\). Write down the distributions of the hierarchical model.\n16. We now are ready to compute a the distribution of \\(\\mu\\) conditioned on the observed data \\(\\bar{X}\\). Compute the expected value of \\(\\mu\\) given the current average \\(\\bar{X}\\) and provide an intuitive explanation for the mathematical formula.\n17. We started with a frequentist 95% confidence interval that ignored data from other players and summarized just José’s data: .450 \\(\\pm\\) 0.220. Construct a credible interval for \\(\\mu\\) based on the hierarchical model.\n18. The credible interval suggests that if another team is impressed by the .450 observation, we should consider trading José as we are predicting he will be just slightly above average. Interestingly, the Red Sox traded José to the Detroit Tigers in July. Here are the José Iglesias batting averages for the next five months:\n\n\nMonth\nAt Bat\nHits\nAVG\n\n\n\nApril\n20\n9\n.450\n\n\nMay\n26\n11\n.423\n\n\nJune\n86\n34\n.395\n\n\nJuly\n83\n17\n.205\n\n\nAugust\n85\n25\n.294\n\n\nSeptember\n50\n10\n.200\n\n\nTotal w/o April\n330\n97\n.293\n\n\n\nWhich of the two approaches provided a better prediciton?"
  },
  {
    "objectID": "inference/hierarchical-models.html#footnotes",
    "href": "inference/hierarchical-models.html#footnotes",
    "title": "12  Hierarchichal Models",
    "section": "",
    "text": "https://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html↩︎\nhttps://fivethirtyeight.com/features/trump-is-just-a-normal-polling-error-behind-clinton/↩︎\nhttps://projects.fivethirtyeight.com/2016-election-forecast/↩︎"
  },
  {
    "objectID": "linear-models/intro-to-linear-models.html",
    "href": "linear-models/intro-to-linear-models.html",
    "title": "Linear Models",
    "section": "",
    "text": "Up to this point, this book has focused mainly on datasets consisting of a single variable. However, in data analyses challenges, it is very common to be interested in the relationship between two or more variables. In this part of the book we introduce linear models, a general framework that unifies approaches used for analyzing association between variables, such as simple and multivariate regression, treatment effect models, and association test. We will illustrate these using case studies related to understudying if height is hereditary, described in detail in Chapter Chapter 13, using data to build a baseball team on a budget, described in detail in Chapter Chapter 14, determining if a high-fat diet makes mice heavier, described in detail in Chapter Chapter 16, and examining if their is gender bias in research funding in the Netherlands, described in detail in Chapter Chapter 17."
  },
  {
    "objectID": "linear-models/regression.html#case-study-is-height-hereditary",
    "href": "linear-models/regression.html#case-study-is-height-hereditary",
    "title": "13  Regression",
    "section": "\n13.1 Case study: is height hereditary?",
    "text": "13.1 Case study: is height hereditary?\nTo understand the concepts of correlation and simple regression we actually use the dataset from which regression was born. The example is from genetics. Francis Galton1 studied the variation and heredity of human traits. Among many other traits, Galton collected and studied height data from families to try to understand heredity. While doing this, he developed the concepts of correlation and regression, as well as a connection to pairs of data that follow a normal distribution. Of course, at the time this data was collected our knowledge of genetics was quite limited compared to what we know today. A very specific question Galton tried to answer was: how well can we predict a child’s height based on the parents’ height? The technique he developed to answer this question, regression, can also be applied to our baseball question. Regression can be applied in many other circumstances as well.\n\n\n\n\n\n\nGalton made important contributions to statistics and genetics, but he was also one of the first proponents of eugenics, a scientifically flawed philosophical movement favored by many biologists of Galton’s time but with horrific historical consequences. You can read more about it here: https://pged.org/history-eugenics-and-genetics/.\n\n\n\nWe have access to Galton’s family height data through the HistData package. This data contains heights on several dozen families: mothers, fathers, daughters, and sons. To imitate Galton’s analysis, we will create a dataset with the heights of fathers and a randomly selected son of each family:\n\nlibrary(tidyverse)\nlibrary(HistData)\n\nset.seed(1983)\ngalton_heights &lt;- GaltonFamilies |&gt;\n  filter(gender == \"male\") |&gt;\n  group_by(family) |&gt;\n  sample_n(1) |&gt;\n  ungroup() |&gt;\n  select(father, childHeight) |&gt;\n  rename(son = childHeight)\n\nSuppose we were asked to summarize the father and son data. Since both distributions are well approximated by the normal distribution, we could use the two averages and two standard deviations as summaries:\n\ngalton_heights |&gt; \n  summarize(mean(father), sd(father), mean(son), sd(son))\n#&gt; # A tibble: 1 × 4\n#&gt;   `mean(father)` `sd(father)` `mean(son)` `sd(son)`\n#&gt;            &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1           69.1         2.55        69.2      2.71\n\nHowever, this summary fails to describe an important characteristic of the data: the trend that the taller the father, the taller the son.\n\ngalton_heights |&gt; ggplot(aes(father, son)) + \n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n\nWe will learn that the correlation coefficient is an informative summary of how two variables move together and then motivate simple regression by noting how this can be used to predict one variable using the other."
  },
  {
    "objectID": "linear-models/regression.html#sec-corr-coef",
    "href": "linear-models/regression.html#sec-corr-coef",
    "title": "13  Regression",
    "section": "\n13.2 The correlation coefficient",
    "text": "13.2 The correlation coefficient\nThe correlation coefficient is defined for a list of pairs \\((x_1, y_1), \\dots, (x_n,y_n)\\) as the average of the product of the standardized values:\n\\[\n\\rho = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)\\left( \\frac{y_i-\\mu_y}{\\sigma_y} \\right)\n\\]\nwith \\(\\mu_x, \\mu_y\\) the averages of \\(x_1,\\dots, x_n\\) and \\(y_1, \\dots, y_n\\), respectively, and \\(\\sigma_x, \\sigma_y\\) the standard deviations. The Greek letter \\(\\rho\\) is commonly used in statistics books to denote the correlation. The Greek letter for \\(r\\), \\(\\rho\\), because it is the first letter of regression. Soon we learn about the connection between correlation and regression. We can represent the formula above with R code using:\n\nrho &lt;- mean(scale(x) * scale(y))\n\nTo understand why this equation does in fact summarize how two variables move together, consider the \\(i\\)-th entry of \\(x\\) is \\(\\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)\\) SDs away from the average. Similarly, the \\(y_i\\) that is paired with \\(x_i\\), is \\(\\left( \\frac{y_1-\\mu_y}{\\sigma_y} \\right)\\) SDs away from the average \\(y\\). If \\(x\\) and \\(y\\) are unrelated, the product \\(\\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)\\left( \\frac{y_i-\\mu_y}{\\sigma_y} \\right)\\) will be positive ( \\(+ \\times +\\) and \\(- \\times -\\) ) as often as negative (\\(+ \\times -\\) and \\(- \\times +\\)) and will average out to about 0. This correlation is the average and therefore unrelated variables will have 0 correlation. If instead the quantities vary together, then we are averaging mostly positive products ( \\(+ \\times +\\) and \\(- \\times -\\)) and we get a positive correlation. If they vary in opposite directions, we get a negative correlation.\nThe correlation coefficient is always between -1 and 1. We can show this mathematically: consider that we can’t have higher correlation than when we compare a list to itself (perfect correlation) and in this case the correlation is:\n\\[\n\\rho = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)^2 =\n\\frac{1}{\\sigma_x^2} \\frac{1}{n} \\sum_{i=1}^n \\left( x_i-\\mu_x \\right)^2 =\n\\frac{1}{\\sigma_x^2} \\sigma^2_x =\n1\n\\]\nA similar derivation, but with \\(x\\) and its exact opposite, proves the correlation has to be bigger or equal to -1.\nFor other pairs, the correlation is in between -1 and 1. The correlation, computed with the function cor, between father and son’s heights is about 0.5:\n\ngalton_heights |&gt; summarize(r = cor(father, son)) |&gt; pull(r)\n#&gt; [1] 0.433\n\n\nFor reasons similar to those explained in Section Section 10.2.1 for the standard deviation, cor(x,y) divides by length(x)-1 rather than length(x).\n\nTo see what data looks like for different values of \\(\\rho\\), here are six examples of pairs with correlations ranging from -0.9 to 0.99:\n\n\n\n\n\n\n\n\n\n13.2.1 Sample correlation is a random variable\nBefore we continue connecting correlation to regression, let’s remind ourselves about random variability.\nIn most data science applications, we observe data that includes random variation. For example, in many cases, we do not observe data for the entire population of interest but rather for a random sample. As with the average and standard deviation, the sample correlation is the most commonly used estimate of the population correlation. This implies that the correlation we compute and use as a summary is a random variable.\nBy way of illustration, let’s assume that the 179 pairs of fathers and sons is our entire population. A less fortunate geneticist can only afford measurements from a random sample of 25 pairs. The sample correlation can be computed with:\n\nR &lt;- sample_n(galton_heights, 25, replace = TRUE) |&gt; \n  summarize(r = cor(father, son)) |&gt; pull(r)\n\nR is a random variable. We can run a Monte Carlo simulation to see its distribution:\n\nB &lt;- 1000\nN &lt;- 25\nR &lt;- replicate(B, {\n  sample_n(galton_heights, N, replace = TRUE) |&gt; \n    summarize(r = cor(father, son)) |&gt; \n    pull(r)\n})\nhist(R, breaks = 20)\n\n\n\n\n\n\n\nWe see that the expected value of R is the population correlation:\n\nmean(R)\n#&gt; [1] 0.431\n\nand that it has a relatively high standard error relative to the range of values R can take:\n\nsd(R)\n#&gt; [1] 0.161\n\nSo, when interpreting correlations, remember that correlations derived from samples are estimates containing uncertainty.\nAlso, note that because the sample correlation is an average of independent draws, the central limit actually applies. Therefore, for large enough \\(N\\), the distribution of R is approximately normal with expected value \\(\\rho\\). The standard deviation, which is somewhat complex to derive, is \\(\\sqrt{\\frac{1-r^2}{N-2}}\\).\nIn our example, \\(N=25\\) does not seem to be large enough to make the approximation a good one:\n\nggplot(aes(sample = R), data = data.frame(R)) + \n  stat_qq() + \n  geom_abline(intercept = mean(R), slope = sqrt((1 - mean(R)^2)/(N - 2)))\n\n\n\n\n\n\n\nIf you increase \\(N\\), you will see the distribution converging to normal.\n\n13.2.2 Correlation is not always a useful summary\nCorrelation is not always a good summary of the relationship between two variables. The following four artificial datasets, referred to as Anscombe’s quartet, famously illustrate this point. All these pairs have a correlation of 0.82:\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nCorrelation is only meaningful in a particular context. To help us understand when it is that correlation is meaningful as a summary statistic, we will return to the example of predicting a son’s height using his father’s height. This will help motivate and define linear regression. We start by demonstrating how correlation can be useful for prediction."
  },
  {
    "objectID": "linear-models/regression.html#sec-conditional-expectation",
    "href": "linear-models/regression.html#sec-conditional-expectation",
    "title": "13  Regression",
    "section": "\n13.3 Conditional expectations",
    "text": "13.3 Conditional expectations\nSuppose we are asked to guess the height of a randomly selected son and we don’t know his father’s height. Because the distribution of sons’ heights is approximately normal, we know the average height, 69.2, is the value with the highest proportion and would be the prediction with the highest chance of minimizing the error. But what if we are told that the father is taller than average, say 72 inches tall, do we still guess 69.2 for the son?\nIt turns out that if we were able to collect data from a very large number of fathers that are 72 inches, the distribution of their sons’ heights would be normally distributed. This implies that the average of the distribution computed on this subset would be our best prediction.\nIn general, we call this approach conditioning. The general idea is that we stratify a population into groups and compute summaries in each group. To provide a mathematical description of conditioning, consider we have a population of pairs of values \\((x_1,y_1),\\dots,(x_n,y_n)\\), for example all father and son heights in England. In the previous chapter we learned that if you take a random pair \\((X,Y)\\), the expected value and best predictor of \\(Y\\) is \\(\\mbox{E}(Y) = \\mu_y\\), the population average \\(1/n\\sum_{i=1}^n y_i\\). However, we are no longer interested in the general population, instead we are interested in only the subset of a population with a specific \\(x_i\\) value, 72 inches in our example. This subset of the population, is also a population and thus the same principles and properties we have learned apply. The \\(y_i\\) in the subpopulation have a distribution, referred to as the conditional distribution, and this distribution has an expected value referred to as the conditional expectation. In our example, the conditional expectation is the average height of all sons in England with fathers that are 72 inches. The statistical notation for the conditional expectation is\n\\[\n\\mbox{E}(Y \\mid X = x)\n\\]\nwith \\(x\\) representing the fixed value that defines that subset, for example 72 inches. Similarly, we denote the standard deviation of the strata with\n\\[\n\\mbox{SD}(Y \\mid X = x) = \\sqrt{\\mbox{Var}(Y \\mid X = x)}\n\\]\nBecause the conditional expectation \\(E(Y\\mid X=x)\\) is the best predictor for the random variable \\(Y\\) for an individual in the strata defined by \\(X=x\\), many data science challenges reduce to estimating this quantity. The conditional standard deviation quantifies the precision of the prediction.\nIn the example we have been considering, we are interested in computing the average son height conditioned on the father being 72 inches tall. We want to estimate \\(E(Y|X=72)\\) using the sample collected by Galton. We previously learned that the sample average is the preferred approach to estimating the population average. However, a challenge when using this approach to estimating conditional expectations is that for continuous data we don’t have many data points matching exactly one value in our sample. For example, we have only:\n\nsum(galton_heights$father == 72)\n#&gt; [1] 8\n\nfathers that are exactly 72-inches. If we change the number to 72.5, we get even fewer data points:\n\nsum(galton_heights$father == 72.5)\n#&gt; [1] 1\n\nA practical way to improve these estimates of the conditional expectations, is to define strata of with similar values of \\(x\\). In our example, we can round father heights to the nearest inch and assume that they are all 72 inches. If we do this, we end up with the following prediction for the son of a father that is 72 inches tall:\n\nconditional_avg &lt;- galton_heights |&gt; \n  filter(round(father) == 72) |&gt;\n  summarize(avg = mean(son)) |&gt; \n  pull(avg)\nconditional_avg\n#&gt; [1] 70.5\n\nNote that a 72-inch father is taller than average – specifically, (72.0 - 69.1)/2.5 = 1.1 standard deviations taller than the average father. Our prediction 70.5 is also taller than average, but only 0.49 standard deviations larger than the average son. The sons of 72-inch fathers have regressed some to the average height. We notice that the reduction in how many SDs taller is about 0.5, which happens to be the correlation. As we will see in a later section, this is not a coincidence.\nIf we want to make a prediction of any height, not just 72, we could apply the same approach to each strata. Stratification followed by boxplots lets us see the distribution of each group:\n\ngalton_heights |&gt; mutate(father_strata = factor(round(father))) |&gt; \n  ggplot(aes(father_strata, son)) + \n  geom_boxplot() + \n  geom_point()\n\n\n\n\n\n\n\nNot surprisingly, the centers of the groups are increasing with height. Furthermore, these centers appear to follow a linear relationship. Below we plot the averages of each group. If we take into account that these averages are random variables with standard errors, the data is consistent with these points following a straight line:\n\n\n\n\n\n\n\n\nThe fact that these conditional averages follow a line is not a coincidence. In the next section, we explain that the line these averages follow is what we call the regression line, which improves the precision of our estimates. However, it is not always appropriate to estimate conditional expectations with the regression line so we also describe Galton’s theoretical justification for using the regression line."
  },
  {
    "objectID": "linear-models/regression.html#the-regression-line",
    "href": "linear-models/regression.html#the-regression-line",
    "title": "13  Regression",
    "section": "\n13.4 The regression line",
    "text": "13.4 The regression line\nIf we are predicting a random variable \\(Y\\) knowing the value of another \\(X=x\\) using a regression line, then we predict that for every standard deviation, \\(\\sigma_X\\), that \\(x\\) increases above the average \\(\\mu_X\\), our prediction \\(\\hat{Y}\\) increase \\(\\rho\\) standard deviations \\(\\sigma_Y\\) above the average \\(\\mu_Y\\) with \\(\\rho\\) the correlation between \\(X\\) and \\(Y\\). The formula for the regression is therefore:\n\\[\n\\left( \\frac{\\hat{Y}-\\mu_Y}{\\sigma_Y} \\right) = \\rho \\left( \\frac{x-\\mu_X}{\\sigma_X} \\right)\n\\]\nWe can rewrite it like this:\n\\[\n\\hat{Y} = \\mu_Y + \\rho \\left( \\frac{x-\\mu_X}{\\sigma_X} \\right) \\sigma_Y\n\\]\nIf there is perfect correlation, the regression line predicts an increase that is the same number of SDs. If there is 0 correlation, then we don’t use \\(x\\) at all for the prediction and simply predict the average \\(\\mu_Y\\). For values between 0 and 1, the prediction is somewhere in between. If the correlation is negative, we predict a reduction instead of an increase.\nNote that if the correlation is positive and lower than 1, our prediction is closer, in standard units, to the average height than the value used to predict, \\(x\\), is to the average of the \\(x\\)s. This is why we call it regression: the son regresses to the average height. In fact, the title of Galton’s paper was: Regression toward mediocrity in hereditary stature. To add regression lines to plots, we will need the above formula in the form:\n\\[\n\\hat{Y} = b + mx \\mbox{ with slope } m = \\rho \\frac{\\sigma_y}{\\sigma_x} \\mbox{ and intercept } b=\\mu_y - m \\mu_x\n\\]\nHere we add the regression line to the original data:\n\nmu_x &lt;- mean(galton_heights$father)\nmu_y &lt;- mean(galton_heights$son)\ns_x &lt;- sd(galton_heights$father)\ns_y &lt;- sd(galton_heights$son)\nr &lt;- cor(galton_heights$father, galton_heights$son)\n\ngalton_heights |&gt; \n  ggplot(aes(father, son)) + \n  geom_point(alpha = 0.5) +\n  geom_abline(slope = r * s_y/s_x, intercept = mu_y - r * s_y/s_x * mu_x) \n\n\n\n\n\n\n\nThe regression formula implies that if we first standardize the variables, that is subtract the average and divide by the standard deviation, then the regression line has intercept 0 and slope equal to the correlation \\(\\rho\\). You can make same plot, but using standard units like this:\n\ngalton_heights |&gt; \n  ggplot(aes(scale(father), scale(son))) + \n  geom_point(alpha = 0.5) +\n  geom_abline(intercept = 0, slope = r)"
  },
  {
    "objectID": "linear-models/regression.html#regression-improves-precision",
    "href": "linear-models/regression.html#regression-improves-precision",
    "title": "13  Regression",
    "section": "\n13.5 Regression improves precision",
    "text": "13.5 Regression improves precision\nLet’s compare the two approaches to prediction that we have presented:\n\nRound fathers’ heights to closest inch, stratify, and then take the average.\nCompute the regression line and use it to predict.\n\nWe use a Monte Carlo simulation sampling \\(N=50\\) families:\n\nB &lt;- 1000\nN &lt;- 50\n\nset.seed(1983)\nconditional_avg &lt;- replicate(B, {\n  dat &lt;- sample_n(galton_heights, N)\n  dat |&gt; filter(round(father) == 72) |&gt; \n    summarize(avg = mean(son)) |&gt; \n    pull(avg)\n  })\n\nregression_prediction &lt;- replicate(B, {\n  dat &lt;- sample_n(galton_heights, N)\n  mu_x &lt;- mean(dat$father)\n  mu_y &lt;- mean(dat$son)\n  s_x &lt;- sd(dat$father)\n  s_y &lt;- sd(dat$son)\n  r &lt;- cor(dat$father, dat$son)\n  mu_y + r*(72 - mu_x)/s_x*s_y\n})\n\nAlthough the expected value of these two random variables is about the same:\n\nmean(conditional_avg, na.rm = TRUE)\n#&gt; [1] 70.5\nmean(regression_prediction)\n#&gt; [1] 70.5\n\nThe standard error for the regression prediction is substantially smaller:\n\nsd(conditional_avg, na.rm = TRUE)\n#&gt; [1] 0.964\nsd(regression_prediction)\n#&gt; [1] 0.452\n\nThe regression line is therefore much more stable than the conditional mean. There is an intuitive reason for this. The conditional average is computed on a relatively small subset: the fathers that are about 72 inches tall. In fact, in some of the permutations we have no data, which is why we use na.rm=TRUE. The regression always uses all the data.\nSo why not always use the regression for prediction? Because it is not always appropriate. For example, Anscombe provided cases for which the data does not have a linear relationship. So are we justified in using the regression line to predict? Galton answered this in the positive for height data. The justification, which we include in the next section, is somewhat more advanced than the rest of the chapter."
  },
  {
    "objectID": "linear-models/regression.html#bivariate-normal-distribution",
    "href": "linear-models/regression.html#bivariate-normal-distribution",
    "title": "13  Regression",
    "section": "\n13.6 Bivariate normal distribution",
    "text": "13.6 Bivariate normal distribution\nCorrelation and the regression slope are a widely used summary statistic, but they are often misused or misinterpreted. Anscombe’s examples provide over-simplified cases of dataset in which summarizing with correlation would be a mistake. But there are many more real-life examples.\nThe main way we motivate the use of correlation involves what is called the bivariate normal distribution.\nWhen a pair of random variables is approximated by the bivariate normal distribution, scatterplots look like ovals. As we saw in Section Section 13.2), they can be thin (high correlation) or circle-shaped (no correlation.\nA more technical way to define the bivariate normal distribution is the following: if \\(X\\) is a normally distributed random variable, \\(Y\\) is also a normally distributed random variable, and the conditional distribution of \\(Y\\) for any \\(X=x\\) is approximately normal, then the pair is approximately bivariate normal. When three or more variables have the property that each pair is bivariate normal, we say the variables follow a multivariate normal distribution or that they are jointly normal.\n\nor simply that the variables are jointly normal\n\n\nIf we think the height data is well approximated by the bivariate normal distribution, then we should see the normal approximation hold for each strata. Here we stratify the son heights by the standardized father heights and see that the assumption appears to hold:\n\ngalton_heights |&gt;\n  mutate(z_father = round((father - mean(father)) / sd(father))) |&gt;\n  filter(z_father %in% -2:2) |&gt;\n  ggplot() +  \n  stat_qq(aes(sample = son)) +\n  facet_wrap( ~ z_father) \n\n\n\n\n\n\n\nNow we come back to defining correlation. Galton used mathematical statistics to demonstrate that, when two variables follow a bivariate normal distribution, computing the regression line is equivalent to computing conditional expectations. We don’t show the derivation here, but we can show that under this assumption, for any given value of \\(x\\), the expected value of the \\(Y\\) in pairs for which \\(X=x\\) is:\n\\[\n\\mbox{E}(Y | X=x) = \\mu_Y +  \\rho \\frac{x-\\mu_X}{\\sigma_X}\\sigma_Y\n\\]\nThis is the regression line, with slope \\[\\rho \\frac{\\sigma_Y}{\\sigma_X}\\] and intercept \\(\\mu_y - m\\mu_X\\). It is equivalent to the regression equation we showed earlier which can be written like this:\n\\[\n\\frac{\\mbox{E}(Y \\mid X=x)  - \\mu_Y}{\\sigma_Y} = \\rho \\frac{x-\\mu_X}{\\sigma_X}\n\\]\nThis implies that, if our data is approximately bivariate, the regression line gives the conditional probability. Therefore, we can obtain a much more stable estimate of the conditional expectation by finding the regression line and using it to predict.\nIn summary, if our data is approximately bivariate, then the conditional expectation, the best prediction of \\(Y\\) given we know the value of \\(X\\), is given by the regression line."
  },
  {
    "objectID": "linear-models/regression.html#variance-explained",
    "href": "linear-models/regression.html#variance-explained",
    "title": "13  Regression",
    "section": "\n13.7 Variance explained",
    "text": "13.7 Variance explained\nThe bivariate normal theory also tells us that the standard deviation of the conditional distribution described above is:\n\\[\n\\mbox{SD}(Y \\mid X=x ) = \\sigma_Y \\sqrt{1-\\rho^2}\n\\]\nTo see why this is intuitive, notice that without conditioning, \\(\\mbox{SD}(Y) = \\sigma_Y\\), we are looking at the variability of all the sons. But once we condition, we are only looking at the variability of the sons with a tall, 72-inch, father. This group will all tend to be somewhat tall so the standard deviation is reduced.\nSpecifically, it is reduced to \\(\\sqrt{1-\\rho^2} = \\sqrt{1 - 0.25}\\) = 0.87 of what it was originally. We could say that father heights “explain” 13% of the variability observed in son heights.\nThe statement “\\(X\\) explains such and such percent of the variability” is commonly used in academic papers. In this case, this percent actually refers to the variance (the SD squared). So if the data is bivariate normal, the variance is reduced by \\(1-\\rho^2\\), so we say that \\(X\\) explains \\(1- (1-\\rho^2)=\\rho^2\\) (the correlation squared) of the variance.\nBut it is important to remember that the “variance explained” statement only makes sense when the data is approximated by a bivariate normal distribution."
  },
  {
    "objectID": "linear-models/regression.html#there-are-two-regression-lines",
    "href": "linear-models/regression.html#there-are-two-regression-lines",
    "title": "13  Regression",
    "section": "\n13.8 There are two regression lines",
    "text": "13.8 There are two regression lines\nWe computed a regression line to predict the son’s height from father’s height. We used these calculations:\n\nmu_x &lt;- mean(galton_heights$father)\nmu_y &lt;- mean(galton_heights$son)\ns_x &lt;- sd(galton_heights$father)\ns_y &lt;- sd(galton_heights$son)\nr &lt;- cor(galton_heights$father, galton_heights$son)\nm_1 &lt;-  r * s_y / s_x\nb_1 &lt;- mu_y - m_1*mu_x\n\nwhich gives us the function \\(\\mbox{E}(Y\\mid X=x) =\\) 37.3 + 0.46 \\(x\\).\nWhat if we want to predict the father’s height based on the son’s? It is important to know that this is not determined by computing the inverse function: \\(x = \\{ \\mbox{E}(Y\\mid X=x) -\\) 37.3 \\(\\} /\\) 0.5.\nWe need to compute \\(\\mbox{E}(X \\mid Y=y)\\). Since the data is approximately bivariate normal, the theory described above tells us that this conditional expectation will follow a line with slope and intercept:\n\nm_2 &lt;-  r * s_x / s_y\nb_2 &lt;- mu_x - m_2 * mu_y\n\nSo we get \\(\\mbox{E}(X \\mid Y=y) =\\) 40.9 + 0.41y. Again we see regression to the average: the prediction for the father is closer to the father average than the son heights \\(y\\) is to the son average.\nHere is a plot showing the two regression lines, with blue for the predicting son heights with father heights and red for predicting father heights with son heights:\n\ngalton_heights |&gt; \n  ggplot(aes(father, son)) + \n  geom_point(alpha = 0.5) + \n  geom_abline(intercept = b_1, slope = m_1, col = \"blue\") +\n  geom_abline(intercept = -b_2/m_2, slope = 1/m_2, col = \"red\")"
  },
  {
    "objectID": "linear-models/regression.html#linear-models",
    "href": "linear-models/regression.html#linear-models",
    "title": "13  Regression",
    "section": "\n13.9 Linear models",
    "text": "13.9 Linear models\nWe are now ready to understand the title of this part of the book. Specifically, the connection between regression and linear models. We have described how if data is bivariate normal then the conditional expectations follow the regression line. The fact that the conditional expectation is a line is not an extra assumption but rather a derived result. However, in practice it is common to explicitly write down a model that describes the relationship between two or more variables using a linear model.\nWe note that linear here does not refer to lines exclusively, but rather to the fact that the conditional expectation is a linear combination of known quantities. In mathematics, when we multiply each variable by a constant and then add them together, we say we formed a linear combination of the variables. For example, \\(3x - 4y + 5z\\) is a linear combination of \\(x\\), \\(y\\), and \\(z\\). We can also add a constant so \\(2 + 3x - 4y + 5z\\) is also linear combination of \\(x\\), \\(y\\), and \\(z\\).\nWe previously described how if \\(X\\) and \\(Y\\) are bivariate normal, then if we look at only the pairs with \\(X=x\\), then \\(Y \\mid X=x\\) follows a normal distribution with expected value \\(\\mu_Y + \\rho \\frac{x-\\mu_X}{\\sigma_X}\\sigma_Y\\), which is a linear function of \\(x\\), and standard deviation \\(\\sigma_Y \\sqrt{1-\\rho^2}\\) that does not depend on \\(x\\). Note that if we write\n\\[\nY = \\beta_0 + \\beta_1 x + \\varepsilon\n\\]\nthen if we assume \\(\\varepsilon\\) follows a normal distribution with expected value 0 and fixed standard deviation, then \\(Y\\) has the same properties as the regression setup gave us: it follows a normal distribution, the expected value is a linear function \\(x\\), and the standard deviation does not depend on \\(x\\).\n\n\n\n\n\n\nIn statistical textbooks, the \\(\\varepsilon\\)s are referred to as “errors,” which originally represented measurement errors in the initial applications of these models. These errors were associated with inaccuracies in measuring height, weight, or distance. However, the term “error” is now used more broadly, even when the \\(\\varepsilon\\)s do not necessarily signify an actual error. For instance, in the case of height, if someone is 2 inches taller than expected based on their parents’ height, those 2 inches should not be considered an error. Despite its lack of descriptive accuracy, the term “error” is employed to elucidate the unexplained variability in the model, unrelated to other included terms.\n\n\n\nIf we were to specify a linear model for Galton’s data, we would denote the \\(N\\) observed father heights with \\(x_1, \\dots, x_n\\), then we model the \\(N\\) son heights we are trying to predict with:\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\, i=1,\\dots,N.\n\\]\nHere \\(x_i\\) is the father’s height, which is fixed (not random) due to the conditioning, and \\(Y_i\\) is the random son’s height that we want to predict. We can further assume that \\(\\varepsilon_i\\) are independent from each other and all have the same standard deviation.\nIn the above model, we know the \\(x_i\\), but to have a useful model for prediction, we need \\(\\beta_0\\) and \\(\\beta_1\\). We estimate these from the data. Once we do this, we can predict son’s heights for any father’s height \\(x\\). We show how to do this in the next section.\nAlthough this model is exactly the same one we derived earlier by assuming bivariate normal data, a somewhat nuanced difference is that in the first approach we assumed the data was bivariate normal and the linear model was derived, not assumed. In practice, linear models are just assumed without necessarily assuming normality: the distribution of the \\(\\varepsilon\\)s is not necessarily specified. Nevertheless, if your data is bivariate normal, the above linear model holds. If your data is not bivariate normal, then you will need to have other ways of justifying the model.\nOne reason linear models are popular is that they are interpretable. In the case of Galton’s data, we can interpret the data like this: due to inherited genes, the son’s height prediction grows by \\(\\beta_1\\) for each inch we increase the father’s height \\(x\\). Because not all sons with fathers of height \\(x\\) are of equal height, we need the term \\(\\varepsilon\\), which explains the remaining variability. This remaining variability includes the mother’s genetic effect, environmental factors, and other biological randomness.\nGiven how we wrote the model above, the intercept \\(\\beta_0\\) is not very interpretable as it is the predicted height of a son with a father with no height. Due to regression to the mean, the prediction will usually be a bit larger than 0. To make the slope parameter more interpretable, we can rewrite the model slightly as:\n\\[\nY_i = \\beta_0 + \\beta_1 (x_i - \\bar{x}) + \\varepsilon_i, \\, i=1,\\dots,N\n\\]\nwith \\(\\bar{x} = 1/N \\sum_{i=1}^N x_i\\) the average of the \\(x\\). In this case \\(\\beta_0\\) represents the height when \\(x_i = \\bar{x}\\), which is the height of the son of an average father.\nLater, specifically in Chapters Chapter 14 and @treatment-effect-models, we will see how the linear model representation permits us to use the same mathematical frameworks in other contexts and to achieve more complicated goals than predict one variable from another."
  },
  {
    "objectID": "linear-models/regression.html#sec-lse",
    "href": "linear-models/regression.html#sec-lse",
    "title": "13  Regression",
    "section": "\n13.10 Least Squares Estimates",
    "text": "13.10 Least Squares Estimates\nFor linear models to be useful, we have to estimate the unknown \\(\\beta\\)s. The standard approach in science is to find the values that minimize the distance of the fitted model to the data. The following is called the least squares (LS) equation and we will see it often in this chapter. For Galton’s data, we would write:\n\\[\nRSS = \\sum_{i=1}^n \\left\\{  y_i - \\left(\\beta_0 + \\beta_1 x_i \\right)\\right\\}^2\n\\]\nThis quantity is called the residual sum of squares (RSS). Once we find the values that minimize the RSS, we will call the values the least squares estimates (LSE) and denote them with \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). Let’s demonstrate this with the previously defined dataset:\n\nlibrary(HistData)\nset.seed(1983)\ngalton_heights &lt;- GaltonFamilies |&gt;\n  filter(gender == \"male\") |&gt;\n  group_by(family) |&gt;\n  sample_n(1) |&gt;\n  ungroup() |&gt;\n  select(father, childHeight) |&gt;\n  rename(son = childHeight)\n\nLet’s write a function that computes the RSS for any pair of values \\(\\beta_0\\) and \\(\\beta_1\\).\n\nrss &lt;- function(beta0, beta1, data){\n  resid &lt;- galton_heights$son - (beta0 + beta1*galton_heights$father)\n  return(sum(resid^2))\n}\n\nSo for any pair of values, we get an RSS. Here is a plot of the RSS as a function of \\(\\beta_1\\) when we keep the \\(\\beta_0\\) fixed at 25.\n\nbeta1 = seq(0, 1, length = nrow(galton_heights))\nresults &lt;- data.frame(beta1 = beta1,\n                      rss = sapply(beta1, rss, beta0 = 25))\nresults |&gt; ggplot(aes(beta1, rss)) + geom_line() + \n  geom_line(aes(beta1, rss))\n\n\n\n\n\n\n\nWe can see a clear minimum for \\(\\beta_1\\) at around 0.65. However, this minimum for \\(\\beta_1\\) is for when \\(\\beta_0 = 25\\), a value we arbitrarily picked. We don’t know if (25, 0.65) is the pair that minimizes the equation across all possible pairs.\nTrial and error is not going to work in this case. We could search for a minimum within a fine grid of \\(\\beta_0\\) and \\(\\beta_1\\) values, but this is unnecessarily time-consuming since we can use calculus: take the partial derivatives, set them to 0 and solve for \\(\\beta_1\\) and \\(\\beta_2\\). Of course, if we have many parameters, these equations can get rather complex. But there are functions in R that do these calculations for us. We will learn these next. To learn the mathematics behind this, you can consult a book on linear models."
  },
  {
    "objectID": "linear-models/regression.html#the-lm-function",
    "href": "linear-models/regression.html#the-lm-function",
    "title": "13  Regression",
    "section": "\n13.11 The lm function",
    "text": "13.11 The lm function\nIn R, we can obtain the least squares estimates using the lm function. To fit the model:\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\n\\]\nwith \\(Y_i\\) the son’s height and \\(x_i\\) the father’s height, we can use this code to obtain the least squares estimates.\n\nfit &lt;- lm(son ~ father, data = galton_heights)\nfit$coef\n#&gt; (Intercept)      father \n#&gt;      37.288       0.461\n\nThe most common way we use lm is by using the character ~ to let lm know which is the variable we are predicting (left of ~) and which we are using to predict (right of ~). The intercept is added automatically to the model that will be fit.\nThe object fit includes more information about the fit. We can use the function summary to extract more of this information (not shown):\n\nsummary(fit)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = son ~ father, data = galton_heights)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -9.354 -1.566 -0.008  1.726  9.415 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  37.2876     4.9862    7.48  3.4e-12 ***\n#&gt; father        0.4614     0.0721    6.40  1.4e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.45 on 177 degrees of freedom\n#&gt; Multiple R-squared:  0.188,  Adjusted R-squared:  0.183 \n#&gt; F-statistic: 40.9 on 1 and 177 DF,  p-value: 1.36e-09\n\nTo understand some of the information included in this summary we need to remember that the LSE are random variables. Mathematical statistics gives us some ideas of the distribution of these random variables.\nIn Chapter @, after describing a more complex case study, we learn more about applying regression in R."
  },
  {
    "objectID": "linear-models/regression.html#lse-are-random-variables",
    "href": "linear-models/regression.html#lse-are-random-variables",
    "title": "13  Regression",
    "section": "\n13.12 LSE are random variables",
    "text": "13.12 LSE are random variables\nThe LSE is derived from the data \\(y_1,\\dots,y_N\\), which are a realization of random variables \\(Y_1, \\dots, Y_N\\). This implies that our estimates are random variables. To see this, we can run a Monte Carlo simulation in which we assume the son and father height data defines a population, take a random sample of size \\(N=50\\), and compute the regression slope coefficient for each one:\n\nB &lt;- 1000\nN &lt;- 50\nlse &lt;- replicate(B, {\n  sample_n(galton_heights, N, replace = TRUE) |&gt; \n    lm(son ~ father, data = _) |&gt; \n    coef()\n})\nlse &lt;- data.frame(beta_0 = lse[1,], beta_1 = lse[2,]) \n\nWe can see the variability of the estimates by plotting their distributions:\n\n#&gt; \n#&gt; Attaching package: 'gridExtra'\n#&gt; The following object is masked from 'package:dplyr':\n#&gt; \n#&gt;     combine\n\n\n\n\n\n\n\nThe reason these look normal is because the central limit theorem applies here as well: for large enough \\(N\\), the least squares estimates will be approximately normal with expected value \\(\\beta_0\\) and \\(\\beta_1\\), respectively. The standard errors are a bit complicated to compute, but mathematical theory does allow us to compute them and they are included in the summary provided by the lm function. Here it is for one of our simulated data sets:\n\nsample_n(galton_heights, N, replace = TRUE) |&gt; \n  lm(son ~ father, data = _) |&gt; \n  summary() |&gt; \n  coef()\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)\n#&gt; (Intercept)    19.28     11.656    1.65 1.05e-01\n#&gt; father          0.72      0.169    4.25 9.79e-05\n\nYou can see that the standard errors estimates reported by the summary are close to the standard errors from the simulation:\n\nlse |&gt; summarize(se_0 = sd(beta_0), se_1 = sd(beta_1))\n#&gt;   se_0  se_1\n#&gt; 1 8.84 0.128\n\nThe summary function also reports t-statistics (t value) and p-values (Pr(&gt;|t|)). The t-statistic is not actually based on the central limit theorem but rather on the assumption that the \\(\\varepsilon\\)s follow a normal distribution. Under this assumption, mathematical theory tells us that the LSE divided by their standard error, \\(\\hat{\\beta}_0 / \\hat{\\mbox{SE}}(\\hat{\\beta}_0 )\\) and \\(\\hat{\\beta}_1 / \\hat{\\mbox{SE}}(\\hat{\\beta}_1 )\\), follow a t-distribution with \\(N-p\\) degrees of freedom, with \\(p\\) the number of parameters in our model. In the case of height \\(p=2\\), the two p-values are testing the null hypothesis that \\(\\beta_0 = 0\\) and \\(\\beta_1=0\\), respectively.\nRemember that, as we described in Section Section 10.2.3 for large enough \\(N\\), the CLT works and the t-distribution becomes almost the same as the normal distribution. Also, notice that we can construct confidence intervals, but we will soon learn about broom, an add-on package that makes this easy.\nAlthough we do not show examples in this book, hypothesis testing with regression models is commonly used in epidemiology and economics to make statements such as “the effect of A on B was statistically significant after adjusting for X, Y, and Z”. However, several assumptions have to hold for these statements to be true."
  },
  {
    "objectID": "linear-models/regression.html#predicted-values-are-random-variables",
    "href": "linear-models/regression.html#predicted-values-are-random-variables",
    "title": "13  Regression",
    "section": "\n13.13 Predicted values are random variables",
    "text": "13.13 Predicted values are random variables\nOnce we fit our model, we can obtain prediction of \\(Y\\) by plugging in the estimates into the regression model. For example, if the father’s height is \\(x\\), then our prediction \\(\\hat{Y}\\) for the son’s height will be:\n\\[\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\]\nWhen we plot \\(\\hat{Y}\\) versus \\(x\\), we see the regression line.\nKeep in mind that the prediction \\(\\hat{Y}\\) is also a random variable and mathematical theory tells us what the standard errors are. If we assume the errors are normal, or have a large enough sample size, we can use theory to construct confidence intervals as well. In fact, the ggplot2 layer geom_smooth(method = \"lm\") that we previously used plots \\(\\hat{Y}\\) and surrounds it by confidence intervals:\n\ngalton_heights |&gt; ggplot(aes(son, father)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nThe R function predict takes an lm object as input and returns the prediction. If requested, the standard errors and other information from which we can construct confidence intervals is provided:\n\nfit &lt;- galton_heights |&gt; lm(son ~ father, data = _) \n\ny_hat &lt;- predict(fit, se.fit = TRUE)\n\nnames(y_hat)\n#&gt; [1] \"fit\"            \"se.fit\"         \"df\"             \"residual.scale\""
  },
  {
    "objectID": "linear-models/regression.html#diagnostic-plots",
    "href": "linear-models/regression.html#diagnostic-plots",
    "title": "13  Regression",
    "section": "\n13.14 Diagnostic plots",
    "text": "13.14 Diagnostic plots\nWhen the linear model is assumed rather than derived, all interpretations depend on the usefulness of the model. The lm function will fit the model and return summaries even when the model is wrong and unuseful.\nVisually inspecting residuals, defined as the difference between observed values and predicted values\n\\[\nr = Y - \\hat{Y} = Y - \\left(\\hat{\\beta}_0 - \\hat{\\beta}_1 x_i\\right),\n\\] and summaries of the residuals, is a powerful way to diagnose if the model is useful. Note that the residuals can be thought of estimates of the errors since\n\\[\n\\varepsilon = Y - \\left(\\beta_0 + \\beta_1 x_i \\right).\n\\] In fact residuals are often denoted as \\(\\hat{\\varepsilon}\\). This motivates several diagnostic plots. Becasue we obervere, \\(r\\) but don’t observe \\(\\varepsilon\\), we based the plots on the residuals.\n\nBecause the errors are assumed not to depend on the expected value of \\(Y\\), a plot of \\(r\\) versus the fitted values \\(\\hat{Y}\\) should show no relationship.\nIn cases in which we assume the errors follow a normal distribtuion a qqplot of standardized \\(r\\) should fall on a line when plotted against theoretical quantiles.\nBecause we assume the standard deviation of the errors is constant, if we plot the absolute value of the residuals, it should appear constant.\n\nWe prefer plots rather than summaries based on, for example, correlation because, as noted in Section @ascombe, correlation is not always the best summary of association. The function plot applied to an lm object automatically plots these.\n\n\n\n\n\n\n\n\n\nplot(fit, which = 1:3)\n\nThis function can produce six different plots, and the argument which let’s you specify which you want to see. You can learn more by reading the plot.lm help file. However, some of the plots are based on more advanced concepts beyond the scope of this book. To learn more we recommend an advanced book on regression analysis.\nIn Chapters Chapter 14 and Chapter 16 we introduce data analysis challenges in which more than one variables some not included in the model. In these cases an important diagnostic test to add checks if the residuals are related to variables not included in the model."
  },
  {
    "objectID": "linear-models/regression.html#the-regression-fallacy",
    "href": "linear-models/regression.html#the-regression-fallacy",
    "title": "13  Regression",
    "section": "\n13.15 The regression fallacy",
    "text": "13.15 The regression fallacy\nWikipedia defines the sophomore slump as:\n\nA sophomore slump or sophomore jinx or sophomore jitters refers to an instance in which a second, or sophomore, effort fails to live up to the standards of the first effort. It is commonly used to refer to the apathy of students (second year of high school, college or university), the performance of athletes (second season of play), singers/bands (second album), television shows (second seasons) and movies (sequels/prequels).\n\nIn Major League Baseball, the rookie of the year (ROY) award is given to the first-year player who is judged to have performed the best. The sophmore slump phrase is used to describe the observation that ROY award winners don’t do as well during their second year. For example, this Fox Sports article2 asks “Will MLB’s tremendous rookie class of 2015 suffer a sophomore slump?”.\nDoes the data confirm the existence of a sophomore slump? Let’s take a look. Examining the data for widely used measure of success, the batting average, we see that this observation holds true for the top performing ROYs:\n\n\n\n\nnameFirst\nnameLast\nrookie_year\nrookie\nsophomore\n\n\n\nWillie\nMcCovey\n1959\n0.354\n0.238\n\n\nIchiro\nSuzuki\n2001\n0.350\n0.321\n\n\nAl\nBumbry\n1973\n0.337\n0.233\n\n\nFred\nLynn\n1975\n0.331\n0.314\n\n\nAlbert\nPujols\n2001\n0.329\n0.314\n\n\n\n\n\nIn fact, the proportion of players that have a lower batting average their sophomore year is 0.6981132.\nSo is it “jitters” or “jinx”? To answer this question, let’s turn our attention to all players that played the 2013 and 2014 seasons and batted more than 130 times (minimum to win Rookie of the Year).\nThe same pattern arises when we look at the top performers: batting averages go down for most of the top performers.\n\n\n\n\nnameFirst\nnameLast\n2013\n2014\n\n\n\nMiguel\nCabrera\n0.348\n0.313\n\n\nHanley\nRamirez\n0.345\n0.283\n\n\nMichael\nCuddyer\n0.331\n0.332\n\n\nScooter\nGennett\n0.324\n0.289\n\n\nJoe\nMauer\n0.324\n0.277\n\n\n\n\n\nBut these are not rookies! Also, look at what happens to the worst performers of 2013:\n\n\n\n\nnameFirst\nnameLast\n2013\n2014\n\n\n\nDanny\nEspinosa\n0.158\n0.219\n\n\nDan\nUggla\n0.179\n0.149\n\n\nJeff\nMathis\n0.181\n0.200\n\n\nB. J.\nUpton\n0.184\n0.208\n\n\nAdam\nRosales\n0.190\n0.262\n\n\n\n\n\nTheir batting averages mostly go up! Is this some sort of reverse sophomore slump? It is not. There is no such thing as the sophomore slump. This is all explained with a simple statistical fact: the correlation for performance in two separate years is high, but not perfect:\n\n\n\n\n\n\n\n\nThe correlation is 0.460254 and the data look very much like a bivariate normal distribution, which means we predict a 2014 batting average \\(Y\\) for any given player that had a 2013 batting average \\(X\\) with:\n\\[ \\frac{Y - .255}{.032} = 0.46 \\left( \\frac{X - .261}{.023}\\right) \\]\nBecause the correlation is not perfect, regression tells us that, on average, expect high performers from 2013 to do a bit worse in 2014. It’s not a jinx; it’s just due to chance. The ROY are selected from the top values of \\(X\\) so it is expected that \\(Y\\) will regress to the mean."
  },
  {
    "objectID": "linear-models/regression.html#exercises",
    "href": "linear-models/regression.html#exercises",
    "title": "13  Regression",
    "section": "\n13.16 Exercises",
    "text": "13.16 Exercises\n1. Load the GaltonFamilies data from the HistData. The children in each family are listed by gender and then by height. Create a dataset called galton_heights by picking a male and female at random.\n2. Make a scatterplot for heights between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons.\n3. Compute the correlation in heights between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons."
  },
  {
    "objectID": "linear-models/regression.html#footnotes",
    "href": "linear-models/regression.html#footnotes",
    "title": "13  Regression",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Francis_Galton↩︎\nhttp://www.foxsports.com/mlb/story/kris-bryant-carlos-correa-rookies-of-year-award-matt-duffy-francisco-lindor-kang-sano-120715↩︎"
  },
  {
    "objectID": "linear-models/multivariate-regression.html#case-study-moneyball",
    "href": "linear-models/multivariate-regression.html#case-study-moneyball",
    "title": "14  Multivariate Regression",
    "section": "\n14.1 Case study: Moneyball",
    "text": "14.1 Case study: Moneyball\nMoneyball: The Art of Winning an Unfair Game is a book by Michael Lewis about the Oakland Athletics (A’s) baseball team and its general manager, the person tasked with building the team, Billy Beane.\nTraditionally, baseball teams use scouts to help them decide what players to hire. These scouts evaluate players by observing them perform. Scouts tend to favor athletic players with observable physical abilities. For this reason, scouts tend to agree on who the best players are and, as a result, these players tend to be in high demand. This in turn drives up their salaries.\nFrom 1989 to 1991, the A’s had one of the highest payrolls in baseball. They were able to buy the best players and, during that time, they were one of the best teams. However, in 1995 the A’s team owner changed and the new management cut the budget drastically, leaving then general manager, Sandy Alderson, with one of the lowest payrolls in baseball. He could no longer afford the most sought-after players. Alderson began using a statistical approach to find inefficiencies in the market. Alderson was a mentor to Billy Beane, who succeeded him in 1998 and fully embraced data science, as opposed to scouts, as a method for finding low-cost players that data predicted would help the team win. Today, this strategy has been adapted by most baseball teams. As we will see, regression plays a large role in this approach.\nAs motivation for this part of the book, we will pretend it is 2002 and try to build a baseball team with a limited budget, just like the A’s had to do. To appreciate what you are up against, note that in 2002 the Yankees’ payroll of $125,928,583 more than tripled the Oakland A’s $39,679,746:\n\n\n\n\n\n\n\n\nStatistics have been used in baseball since its beginnings. The dataset we will be using, included in the Lahman library, goes back to the 19th century. For example, a summary statistics we will describe soon, the batting average, has been used for decades to summarize a batter’s success. Other statistics1 such as home runs (HR), runs batted in (RBI), and stolen bases (SB) are reported for each player in the game summaries included in the sports section of newspapers, with players rewarded for high numbers. Although summary statistics such as these were widely used in baseball, data analysis per se was not. These statistics were arbitrarily decided on without much thought as to whether they actually predicted anything or were related to helping a team win.\nThis changed with Bill James2. In the late 1970s, this aspiring writer and baseball fan started publishing articles describing more in-depth analysis of baseball data. He named the approach of using data to predict what outcomes best predicted if a team would win sabermetrics3. Until Billy Beane made sabermetrics the center of his baseball operation, Bill James’ work was mostly ignored by the baseball world. Currently, sabermetrics popularity is no longer limited to just baseball; other sports have started to use this approach as well.\nTo simplify the exercise, we will focus on scoring runs and ignore the two other important aspects of the game: pitching and fielding. We will see how regression analysis can help develop strategies to build a competitive baseball team with a constrained budget. The approach can be divided into two separate data analyses. In the first, we determine which recorded player-specific statistics predict runs. In the second, we examine if players were undervalued based on what our first analysis predicts.\n\n14.1.1 Baseball basics\nTo see how regression will help us find undervalued players, we actually don’t need to understand all the details about the game of baseball, which has over 100 rules. Here, we distill the sport to the basic knowledge one needs to know how to effectively attack the data science problem.\nThe goal of a baseball game is to score more runs (points) than the other team. Each team has 9 batters that have an opportunity to hit a ball with a bat in a predetermined order. After the 9th batter has had their turn, the first batter bats again, then the second, and so on. Each time a batter has an opportunity to bat, we call it a plate appearance (PA). At each PA, the other team’s pitcher throws the ball and the batter tries to hit it. The PA ends with an binary outcome: the batter either makes an out (failure) and returns to the bench or the batter doesn’t (success) and can run around the bases, and potentially score a run (reach all 4 bases). Each team gets nine tries, referred to as innings, to score runs and each inning ends after three outs (three failures).\nHere is a video showing a success: https://www.youtube.com/watch?v=HL-XjMCPfio. And here is one showing a failure: https://www.youtube.com/watch?v=NeloljCx-1g. In these videos, we see how luck is involved in the process. When at bat, the batter wants to hit the ball hard. If the batter hits it hard enough, it is a HR, the best possible outcome as the batter gets at least one automatic run. But sometimes, due to chance, the batter hits the ball very hard and a defender catches it, resulting in an out. In contrast, sometimes the batter hits the ball softly, but it lands just in the right place. The fact that there is chance involved hints at why probability models will be involved.\nNow, there are several ways to succeed. Understanding this distinction will be important for our analysis. When the batter hits the ball, the batter wants to pass as many bases as possible. There are four bases with the fourth one called home plate. Home plate is where batters start by trying to hit, so the bases form a cycle.\n\n\n\n\n\n\n\n\n(Courtesy of Cburnett4. CC BY-SA 3.0 license5.) \nA batter who goes around the bases and arrives home, scores a run.\nWe are simplifying a bit, but there are five ways a batter can succeed, that is, not make an out:\n\nBases on balls (BB) - the pitcher fails to throw the ball through a predefined area considered to be hittable (the strikezone), so the batter is permitted to go to first base.\nSingle - Batter hits the ball and gets to first base.\nDouble (2B) - Batter hits the ball and gets to second base.\nTriple (3B) - Batter hits the ball and gets to third base.\nHome Run (HR) - Batter hits the ball and goes all the way home and scores a run.\n\nHere is an example of a HR: https://www.youtube.com/watch?v=xYxSZJ9GZ-w. If a batter gets to a base, the batter still has a chance of getting home and scoring a run if the next batter hits successfully. While the batter is on base, the batter can also try to steal a base (SB). If a batter runs fast enough, the batter can try to go from one base to the next without the other team tagging the runner. Here is an example of a stolen base: https://www.youtube.com/watch?v=JSE5kfxkzfk.\nAll these events are kept track of during the season and are available to us through the Lahman package. Now we will start discussing how data analysis can help us decide how to use these statistics to evaluate players.\n\n14.1.2 No awards for BB\nHistorically, the batting average has been considered the most important offensive statistic. To define this average, we define a hit (H) and an at bat (AB). Singles, doubles, triples, and home runs are hits. The fifth way to be successful, BB, is not a hit. An AB is the number of times you either get a hit or make an out; BBs are excluded. The batting average is simply H/AB and is considered the main measure of a success rate. Today this success rate ranges from 20% to 38%. We refer to the batting average in thousands so, for example, if your success rate is 28%, we call it batting 280.\n\n\n\n\n\n\n\n\n(Picture courtesy of Keith Allison6. CC BY-SA 2.0 license7.)\nOne of Bill James’ first important insights is that the batting average ignores BB, but a BB is a success. He proposed we use the on base percentage (OBP) instead of batting average. He defined OBP as (H+BB)/(AB+BB) which is simply the proportion of plate appearances that don’t result in an out, a very intuitive measure. He noted that a player that gets many more BB than the average player might not be recognized if the batter does not excel in batting average. But is this player not helping produce runs? No award is given to the player with the most BB. However, bad habits are hard to break and baseball did not immediately adopt OBP as an important statistic. In contrast, total stolen bases were considered important and an award8 given to the player with the most. But players with high totals of SB also made more outs as they did not always succeed. Does a player with high SB total help produce runs? Can we use data science to determine if it’s better to pay for players with high BB or SB?\n\n14.1.3 Base on balls or stolen bases?\nOne of the challenges in this analysis is that it is not obvious how to determine if a player produces runs because so much depends on his teammates. We do keep track of the number of runs scored by a player. However, remember that if a player X bats right before someone who hits many HRs, batter X will score many runs. But these runs don’t necessarily happen if we hire player X but not his HR hitting teammate. However, we can examine team-level statistics. How do teams with many SB compare to teams with few? How about BB? We have data! Let’s examine some. We start by creating with statistics from 1962, the first year all teams played 162 games (like today) instead of 154, to 2001, the year before the year for which we will construct a team. We convert the data to a per game rate because a small proportion of seasons had less games than usual due to strikes, and some teams played extra games due to tie breakers.\n\nlibrary(tidyverse)\n#&gt; ── Attaching core tidyverse packages ──────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.1     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#&gt; ✔ lubridate 1.9.2     ✔ tibble    3.2.1\n#&gt; ✔ purrr     1.0.1     ✔ tidyr     1.3.0\n#&gt; ── Conflicts ────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nlibrary(Lahman)\ndat &lt;- Teams |&gt; filter(yearID %in% 1962:2002) |&gt;\n  mutate(team = teamID, year = yearID, r = R/G, \n         singles = (H - X2B - X3B - HR)/G, doubles = X2B/G, triples = X3B/G, hr = HR/G,\n         sb = SB/G, bb = BB/G) |&gt;\n  select(team, year, r, singles, doubles, triples, hr, sb, bb)\n\nNow let’s start with a obvious question: does teams that hit more home runs score more runs? The visualization of choice when exploring the relationship between two variables is a scatter plot.\n\np &lt;- dat |&gt; ggplot(aes(hr, r)) + geom_point(alpha = 0.5)\np \n\n\n\n\n\n\n\nWe defined p because we will add to this plot latter. The plot shows a strong association: teams with more HRs tend to score more runs. Now let’s examine the relationship between stolen bases and runs:\n\ndat |&gt; ggplot(aes(sb, r)) + geom_point(alpha = 0.5)\n\n\n\n\n\n\n\nHere the relationship is not as clear. Finally, let’s examine the relationship between BB and runs:\n\ndat |&gt; ggplot(aes(bb, r)) + geom_point(alpha = 0.5)\n\n\n\n\n\n\n\nHere again we see a clear association. But does this mean that increasing a team’s BBs causes an increase in runs? One of the most important lessons you learn in this book is that association is not causation. In fact, it looks like BBs and HRs are also associated:\n\ndat |&gt; ggplot(aes(hr, bb)) + geom_point(alpha = 0.5)\n\n\n\n\n\n\n\nWe know that HRs cause runs because when a player hits a HR they are guaranteed at least one run. Could it be that HRs also cause BB and this makes it appear as if BB cause runs? When this happens we say there is confounding, an important concept we will learn more about throughout this chapter.\nLinear regression will help us parse all this out and quantify the associations. This will then help us determine what players to recruit. Specifically, we will try to predict things like how many more runs will a team score if we increase the number of BBs, but keep the HRs fixed? Regression will help us answer questions like this one.\n\n14.1.4 Regression applied to baseball statistics\nCan we use regression with these data? First, notice that the HR and Run data, shown above, appear to be bivariate normal. Specifically, the qq-plots confirm that the normal approximation for each HR strata is useful here:\n\ndat |&gt; mutate(z_hr = round(scale(hr))) |&gt;\n  filter(z_hr %in% -2:3) |&gt;\n  ggplot() +  \n  stat_qq(aes(sample = r)) +\n  facet_wrap(~z_hr) \n\n\n\n\n\n\n\nNow we are ready to use linear regression to predict the number of runs a team will score if we know how many home runs the team hits using regression:\n\nhr_fit  &lt;- lm(r ~ hr, data = dat)$coef\np + geom_abline(intercept = hr_fit[[1]], slope = hr_fit[[2]])\n\n\n\n\n\n\n\nNote that we can obtain the same plot quicker by using the ggplot2 function geom_smooth which computes and adds a regression line to plot along with confidence intervals. We use the argument method = \"lm\" which stands for linear model, the title of an upcoming section. So we can simplify the code above like this:\n\np + geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\nIn the example above, the slope is 1.8517449. So this tells us that teams that hit 1 more HR per game than the average team, score 1.8517449 more runs per game than the average team. Given that the most common final score is a difference of a run, this can certainly lead to a large increase in wins. Not surprisingly, HR hitters are very expensive. Because we are working on a budget, we will need to find some other way to increase wins. In the next chapter, we introduce linear models, which provide an framework for performing this analysis. In chapter @ref{@moneyball} we apply what have learned to build a baseball team."
  },
  {
    "objectID": "linear-models/multivariate-regression.html#the-broom-package",
    "href": "linear-models/multivariate-regression.html#the-broom-package",
    "title": "14  Multivariate Regression",
    "section": "\n14.2 The broom package",
    "text": "14.2 The broom package\nThe broom package facilitates the use of R function such as lm within the tidyverse. Recall the that lm does not take a data frame as a first argument and does not return a data frame, which makes using lm in conjunction with the tidyverse difficult. It has three main functions, all of which extract information from the object returned by lm and returns it in a tidyverse friendly data frame. These functions are tidy, glance, and augment. The tidy function returns estimates and related information as a data frame:\n\nlibrary(broom)\nfit &lt;- lm(r ~ bb, data = dat)\ntidy(fit)\n#&gt; # A tibble: 2 × 5\n#&gt;   term        estimate std.error statistic  p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 (Intercept)    1.93     0.116       16.7 1.91e-55\n#&gt; 2 bb             0.739    0.0348      21.2 1.90e-83\n\nWe can add other important summaries, such as confidence intervals:\n\ntidy(fit, conf.int = TRUE)\n#&gt; # A tibble: 2 × 7\n#&gt;   term        estimate std.error statistic  p.value conf.low conf.high\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)    1.93     0.116       16.7 1.91e-55    1.70      2.15 \n#&gt; 2 bb             0.739    0.0348      21.2 1.90e-83    0.671     0.807\n\nBecause the outcome is a data frame, we can immediately use it with summarize to string together the commands that produce the table we are after. Because a data frame is returned, we can filter and select the rows and columns we want, as we will see in the next section.\nNow we return to discussing our original task of determining if slopes changed. The plot we just made, using summarize and tidy, shows that the confidence intervals overlap, which provides a nice visual confirmation that our assumption that the slope does not change is safe.\nThe other functions provided by broom, glance, and augment, relate to model-specific and observation-specific outcomes, respectively. Here, we can see the model fit summaries glance returns:\n\nglance(fit)\n#&gt; # A tibble: 1 × 12\n#&gt;   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC\n#&gt;       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     0.304         0.303 0.493      451. 1.90e-83     1  -737. 1480.\n#&gt; # ℹ 4 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;,\n#&gt; #   nobs &lt;int&gt;\n\nYou can learn more about these summaries in any regression text book."
  },
  {
    "objectID": "linear-models/multivariate-regression.html#confounding",
    "href": "linear-models/multivariate-regression.html#confounding",
    "title": "14  Multivariate Regression",
    "section": "\n14.3 Confounding",
    "text": "14.3 Confounding\nPreviously, we noted a strong relationship between Runs and BB. If we find the regression line for predicting runs from bases on balls, we a get slope of:\n\nbb_slope &lt;- lm(r ~ bb, data = dat)$coef[2]\nbb_slope \n#&gt;    bb \n#&gt; 0.739\n\nSo does this mean that if we go and hire low salary players with many BB, and who therefore increase the number of walks per game by 2, our team will score 1.5 more runs per game?\nWe are again reminded that association is not causation. The data does provide strong evidence that a team with two more BB per game than the average team, scores 1.5 runs per game. But this does not mean that BB are the cause.\nNote that if we compute the regression line slope for singles we get:\n\nlm(r ~ singles, data = dat)$coef[2]\n#&gt; singles \n#&gt;   0.432\n\nwhich is a lower value than what we obtain for BB. Notice that a single gets you to first base just like a BB. Those that know about baseball will tell you that with a single, runners on base have a better chance of scoring than with a BB. So how can BB be more predictive of runs? The reason this happen is because of confounding. Here we show the correlation between HR, BB, and singles:\n\ndat |&gt; summarize(cor(bb, hr), cor(singles, hr), cor(bb, singles))\n#&gt;   cor(bb, hr) cor(singles, hr) cor(bb, singles)\n#&gt; 1       0.406           -0.186          -0.0513\n\nIt turns out that pitchers, afraid of HRs, will sometimes avoid throwing strikes to HR hitters. As a result, HR hitters tend to have more BBs and a team with many HRs will also have more BBs. Although it may appear that BBs cause runs, it is actually the HRs that cause most of these runs. We say that BBs are confounded with HRs. Nonetheless, could it be that BBs still help? To find out, we somehow have to adjust for the HR effect. Regression can help with this as well.\n\n14.3.1 Understanding confounding through stratification\nA first approach is to keep HRs fixed at a certain value and then examine the relationship between BB and runs. As we did when we stratified fathers by rounding to the closest inch, here we can stratify HR per game to the closest ten. We filter out the strata with few points to avoid highly variable estimates and then make a scatterplot for each strata:\n\ndat |&gt; mutate(hr_strata = round(hr, 1)) |&gt; \n  filter(hr_strata &gt;= 0.4 & hr_strata &lt;= 1.2) |&gt;\n  ggplot(aes(bb, r)) +  \n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\") +\n  facet_wrap(~hr_strata) \n\n\n\n\n\n\n\nRemember that the regression slope for predicting runs with BB was 0.7. Once we stratify by HR, these slopes are substantially reduced:\n\ndat |&gt; mutate(hr_strata = round(hr, 1)) |&gt; \n  filter(hr_strata &gt;= 0.5 & hr_strata &lt;= 1.2) |&gt;  \n  group_by(hr_strata) |&gt;\n  reframe(tidy(lm(r ~ bb))) |&gt;\n  filter(term == \"bb\")\n#&gt; # A tibble: 8 × 6\n#&gt;   hr_strata term  estimate std.error statistic      p.value\n#&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n#&gt; 1       0.5 bb       0.566    0.110       5.14 0.00000302  \n#&gt; 2       0.6 bb       0.405    0.0984      4.12 0.0000746   \n#&gt; 3       0.7 bb       0.284    0.0717      3.96 0.000113    \n#&gt; 4       0.8 bb       0.378    0.0638      5.92 0.0000000175\n#&gt; 5       0.9 bb       0.254    0.0762      3.33 0.00108     \n#&gt; # ℹ 3 more rows\n\n\n\n\n\n\n\nNote we use reframe instead of summarize because tidy returns a data frame with two rows.\n\n\n\nThe slopes are reduced, but they are not 0, which indicates that BBs are helpful for producing runs, just not as much as previously thought. In fact, the values above are closer to the slope we obtained from singles, 0.4, which is more consistent with our intuition. Since both singles and BB get us to first base, they should have about the same predictive power.\nAlthough our understanding of the application tells us that HR cause BB but not the other way around, we can still check if stratifying by BB makes the effect of BB go down. To do this, we use the same code except that we swap HR and BBs. In this case, the slopes do not change much from the original:\n\ndat |&gt; mutate(bb_strata = round(bb, 1)) |&gt; \n  filter(bb_strata &gt;= 3 & bb_strata &lt;= 4) |&gt;  \n  group_by(bb_strata) |&gt;\n  reframe(tidy(lm(r ~ hr))) |&gt;\n  filter(term == \"hr\")\n#&gt; # A tibble: 11 × 6\n#&gt;   bb_strata term  estimate std.error statistic  p.value\n#&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1       3   hr        1.51     0.182      8.31 1.47e-12\n#&gt; 2       3.1 hr        1.49     0.168      8.87 3.10e-14\n#&gt; 3       3.2 hr        1.61     0.150     10.8  6.96e-18\n#&gt; 4       3.3 hr        1.57     0.167      9.39 5.73e-15\n#&gt; 5       3.4 hr        1.55     0.153     10.1  3.77e-16\n#&gt; # ℹ 6 more rows\n\nThey are reduced a bit from 1.8517449, which is consistent with the fact that BB do in fact cause some runs.\nRegardless, it seems that if we stratify by HR, we have bivariate distributions for runs versus BB. Similarly, if we stratify by BB, we have approximate bivariate normal distributions for HR versus runs."
  },
  {
    "objectID": "linear-models/multivariate-regression.html#sec-regression-in-r",
    "href": "linear-models/multivariate-regression.html#sec-regression-in-r",
    "title": "14  Multivariate Regression",
    "section": "\n14.4 Multivariable regression",
    "text": "14.4 Multivariable regression\nIt is somewhat complex to be computing regression lines for each strata. We are essentially fitting models like this:\n\\[\n\\mbox{E}[R \\mid BB = x_1, \\, HR = x_2] = \\beta_0 + \\beta_1(x_2) x_1 + \\beta_2(x_1) x_2\n\\]\nwith the slopes for \\(x_1\\) changing for different values of \\(x_2\\) and vice versa. But is there an easier approach?\nIf we take random variability into account, the slopes in the strata don’t appear to change much. If these slopes are in fact the same, this implies that \\(\\beta_1(x_2)\\) and \\(\\beta_2(x_1)\\) are constants. This in turn implies that the expectation of runs conditioned on HR and BB can be written like this:\n\\[\n\\mbox{E}[R \\mid BB = x_1, \\, HR = x_2] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n\\]\nThis model suggests that if the number of HR is fixed at \\(x_2\\), we observe a linear relationship between runs and BB with an intercept of \\(\\beta_0 + \\beta_2 x_2\\). Our exploratory data analysis suggested that this is the case. The model also suggests that as the number of HR grows, the intercept growth is linear as well and determined by \\(\\beta_1\\). In this analysis, referred to as multivariable regression, you will often hear people say that the BB slope \\(\\beta_1\\) is adjusted for the HR effect.\nBecause the data is approximately normal and conditional distributions were also normal we are justified in using a linear model:\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\varepsilon_i\n\\]\nwith \\(Y_i\\) runs per game for team \\(i\\), \\(x_{i,1}\\) walks per game, and \\(x_{i,2}\\). To use lm here, we need to let the function know we have two predictor variables. So we use the + symbol as follows:\n\ntidy(lm(r ~ bb + hr, data = dat), conf.int = TRUE) \n#&gt; # A tibble: 3 × 7\n#&gt;   term        estimate std.error statistic   p.value conf.low conf.high\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)    1.74     0.0820      21.2 3.38e- 83    1.58      1.90 \n#&gt; 2 bb             0.387    0.0269      14.4 8.41e- 43    0.334     0.440\n#&gt; 3 hr             1.57     0.0488      32.1 1.39e-157    1.47      1.66\n\nWhen we fit the model with only one variable, the estimated slopes were 0.7388725 and 1.8517449 for BB and HR, respectively. Note that when fitting the multivariable model both go down, with the BB effect decreasing much more.\n\n\n\n\n\n\nYou are ready to do exercises 1-12 if you want to practice before continuing.\n\n\n\n\n14.4.1 Building a baseball team\nNow we want to construct a metric to pick players, and we need to consider singles, doubles, and triples as well. Can we build a model that predicts runs based on all these outcomes? We take somewhat of a “leap of faith” and assume that these five variables are jointly normal. This means that if we pick any one of them, and hold the other four fixed, the relationship with the outcome is linear and the slope does not depend on the four values held constant. If this is true, then a linear model for our data is:\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\beta_3 x_{i,3}+ \\beta_4 x_{i,4} + \\beta_5 x_{i,5} + \\varepsilon_i\n\\]\nwith \\(x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}\\) representing BB, singles, doubles, triples, and HR respectively.\nUsing lm, we can quickly find the LSE for the parameters using:\n\nfit &lt;- dat |&gt;  filter(year &lt;= 2001) |&gt; lm(r ~ bb + singles + doubles + triples + hr, data = _)\n\nNote we fit the model to data up until 2001, the year before we will construct our team. We can see the coefficients using tidy:\n\ntidy(fit, conf.int = TRUE) |&gt; filter(term != \"(Intercept)\")\n#&gt; # A tibble: 5 × 7\n#&gt;   term    estimate std.error statistic   p.value conf.low conf.high\n#&gt;   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 bb         0.370    0.0119      31.2 1.00e-149    0.347     0.393\n#&gt; 2 singles    0.517    0.0128      40.5 5.29e-213    0.492     0.543\n#&gt; 3 doubles    0.775    0.0229      33.8 7.09e-168    0.730     0.820\n#&gt; 4 triples    1.24     0.0778      15.9 4.62e- 51    1.09      1.39 \n#&gt; 5 hr         1.44     0.0248      58.1 1.98e-323    1.39      1.49\n\nTo see how well our metric actually predicts runs, we can predict the number of runs for each team in 2002 using the function predict, then make a plot:\n\ndat |&gt; mutate(r_hat = predict(fit, newdata = dat)) |&gt;\n  filter(year == 2002) %&gt;%\n  ggplot(aes(r_hat, r, label = team)) + \n  geom_point() +\n  geom_text(nudge_x = 0.1, cex = 2) + \n  geom_abline()\n\n\n\n\n\n\n\nOur model does quite a good job as demonstrated by the fact that points from the observed versus predicted plot fall close to the identity line.\nSo instead of using batting average, or just number of HR, as a measure of picking players, we can use our fitted model to form a metric that relates more directly to run production. Specifically, to define a metric for player A, we imagine a team made up of players just like player A and use our fitted regression model to predict how many runs this team would produce. The formula would look like this: -2.7580763 + 0.3699921 \\(\\times\\) BB + 0.5174284 \\(\\times\\) singles + 0.7750757 \\(\\times\\) doubles + 1.2387738 \\(\\times\\) triples + 1.4419724 \\(\\times\\) HR.\nTo define a player-specific metric, we have a bit more work to do. A challenge here is that we derived the metric for teams, based on team-level summary statistics. For example, the HR value that is entered into the equation is HR per game for the entire team. If we compute the HR per game for a player, it will be much lower since the total is accumulated by 9 batters. Furthermore, if a player only plays part of the game and gets fewer opportunities than average, it is still considered a game played. For players, a rate that takes into account opportunities is the per-plate-appearance rate.\nTo make the per-game team rate comparable to the per-plate-appearance player rate, we compute the average number of team plate appearances per game:\n\npa_per_game &lt;- Batting |&gt; filter(yearID == 2002) |&gt; \n  group_by(teamID) |&gt;\n  summarize(pa_per_game = sum(AB + BB)/162) |&gt; \n  pull(pa_per_game) |&gt; \n  mean()\n\nWe compute the per-plate-appearance rates for players available in 2002 on data from 1997-2001. To avoid small sample artifacts, we filter players with less than 1,000 plate appearances per year. Here is the entire calculation in one line:\n\nplayers &lt;- Batting |&gt; \n  filter(yearID %in% 1997:2001) |&gt; \n  group_by(playerID) |&gt;\n  mutate(pa = BB + AB) |&gt;\n  summarize(g = sum(pa)/pa_per_game,\n    bb = sum(BB)/g,\n    singles = sum(H - X2B - X3B - HR)/g,\n    doubles = sum(X2B)/g, \n    triples = sum(X3B)/g, \n    hr = sum(HR)/g,\n    avg = sum(H)/sum(AB),\n    pa = sum(pa)) |&gt;\n  filter(pa &gt;= 1000) |&gt;\n  select(-g)\n\nplayers$r_hat = predict(fit, newdata = players)\n\nThe player-specific predicted runs computed here can be interpreted as the number of runs we predict a team will score if all batters are exactly like that player. The distribution shows that there is wide variability across players:\n\nhist(players$r_hat, main = \"Predicted runs per game\")\n\n\n\n\n\n\n\nTo actually build the team, we will need to know their salaries as well as their defensive position. For this, we use the righ_join function to combine the players data frame we just created with the player information data frame included in some of the other Lahman data tables.\nStart by adding the 2002 salary of each player:\n\nplayers &lt;- Salaries |&gt; \n  filter(yearID == 2002) |&gt;\n  select(playerID, salary) |&gt;\n  right_join(players, by = \"playerID\")\n\nNext, we add their defensive position. This is a somewhat complicated task because players play more than one position each year. The Lahman package table Appearances tells how many games each player played in each position, so we can pick the position that was most played using which.max on each row. We use apply to do this. However, because some players are traded, they appear more than once on the table, so we first sum their appearances across teams. Here, we pick the one position the player most played using the top_n function. To make sure we only pick one position, in the case of ties, we pick the first row of the resulting data frame. We also remove the OF position which stands for outfielder, a generalization of three positions: left field (LF), center field (CF), and right field (RF). We also remove pitchers since they don’t bat in the league in which the A’s play.\n\nposition_names &lt;- \n  paste0(\"G_\", c(\"p\",\"c\",\"1b\",\"2b\",\"3b\",\"ss\",\"lf\",\"cf\",\"rf\", \"dh\"))\n\ntmp &lt;- Appearances |&gt; \n  filter(yearID == 2002) |&gt; \n  group_by(playerID) |&gt;\n  summarize_at(position_names, sum) |&gt;\n  ungroup()\n  \npos &lt;- tmp |&gt;\n  select(all_of(position_names)) |&gt;\n  apply(X = _, 1, which.max) \n\nplayers &lt;- tibble(playerID = tmp$playerID, POS = position_names[pos]) |&gt;\n  mutate(POS = str_to_upper(str_remove(POS, \"G_\"))) |&gt;\n  filter(POS != \"P\") |&gt;\n  right_join(players, by = \"playerID\") |&gt;\n  filter(!is.na(POS)  & !is.na(salary))\n\nFinally, we add their first and last name:\n\nplayers &lt;- People |&gt;\n  select(playerID, nameFirst, nameLast, debut) |&gt;\n  mutate(debut = as.Date(debut)) |&gt;\n  right_join(players, by = \"playerID\")\n\nIf you are a baseball fan, you will recognize the top 10 players:\n\nplayers |&gt; select(nameFirst, nameLast, POS, salary, r_hat) |&gt; arrange(desc(r_hat)) |&gt; head(10) \n#&gt;    nameFirst nameLast POS   salary r_hat\n#&gt; 1      Barry    Bonds  LF 15000000  8.05\n#&gt; 2      Larry   Walker  RF 12666667  7.96\n#&gt; 3       Todd   Helton  1B  5000000  7.40\n#&gt; 4      Manny  Ramirez  LF 15462727  7.35\n#&gt; 5      Sammy     Sosa  RF 15000000  7.20\n#&gt; 6       Jeff  Bagwell  1B 11000000  7.05\n#&gt; 7       Mike   Piazza   C 10571429  6.99\n#&gt; 8      Jason   Giambi  1B 10428571  6.92\n#&gt; 9      Edgar Martinez  DH  7086668  6.91\n#&gt; 10       Jim    Thome  1B  8000000  6.89\n\nOn average, players with a higher metric have higher salaries:\n\nplayers |&gt; ggplot(aes(salary, r_hat, color = POS)) + \n  geom_point() +\n  scale_x_log10()\n\n\n\n\n\n\n\nWe can search for good deals by looking at players who produce many more runs than others with similar salaries. We can use this table to decide what players to pick and keep our total salary below the 40 million dollars Billy Beane had to work with. This can be done using what computer scientists call linear programming. This is not something we teach, but here are the position players selected with this approach:\n\n\n\n\nnameFirst\nnameLast\nPOS\nsalary\nr_hat\n\n\n\nTodd\nHelton\n1B\n5000000\n7.40\n\n\nMike\nPiazza\nC\n10571429\n6.99\n\n\nEdgar\nMartinez\nDH\n7086668\n6.91\n\n\nJim\nEdmonds\nCF\n7333333\n6.23\n\n\nJeff\nKent\n2B\n6000000\n6.08\n\n\nPhil\nNevin\n3B\n2600000\n5.86\n\n\nMatt\nStairs\nRF\n500000\n5.76\n\n\nHenry\nRodriguez\nLF\n300000\n5.64\n\n\nJohn\nValentin\nSS\n550000\n5.00\n\n\n\n\n\nWe see that all these players have above average BB and most have above average HR rates, while the same is not true for singles and batting average. Here is a table with statistics standardized across players so that, for example, above average HR hitters have values above 0.\n\n\n\n\nnameLast\nbb\nsingles\ndoubles\ntriples\nhr\navg\nr_hat\n\n\n\nHelton\n0.909\n-0.215\n2.649\n-0.311\n1.522\n2.670\n2.542\n\n\nPiazza\n0.328\n0.423\n0.204\n-1.418\n1.825\n2.199\n2.093\n\n\nMartinez\n2.135\n-0.005\n1.265\n-1.224\n0.808\n2.203\n2.004\n\n\nEdmonds\n1.071\n-0.558\n0.791\n-1.152\n0.973\n0.854\n1.259\n\n\nKent\n0.232\n-0.732\n2.011\n0.448\n0.766\n0.787\n1.093\n\n\nNevin\n0.307\n-0.905\n0.479\n-1.191\n1.193\n0.105\n0.850\n\n\nStairs\n1.100\n-1.513\n-0.046\n-1.129\n1.121\n-0.561\n0.742\n\n\nRodriguez\n0.201\n-1.596\n0.332\n-0.782\n1.320\n-0.672\n0.613\n\n\nValentin\n0.180\n-0.929\n1.794\n-0.435\n-0.045\n-0.472\n-0.088"
  },
  {
    "objectID": "linear-models/multivariate-regression.html#exercises",
    "href": "linear-models/multivariate-regression.html#exercises",
    "title": "14  Multivariate Regression",
    "section": "\n14.5 Exercises",
    "text": "14.5 Exercises\nWe have shown how BB and singles have similar predictive power for scoring runs. Another way to compare the usefulness of these baseball metrics is by assessing how stable they are across the years. Since we have to pick players based on their previous performances, we will prefer metrics that are more stable. In these exercises, we will compare the stability of singles and BBs.\n1. Before we get started, we want to generate two tables. One for 2002 and another for the average of 1999-2001 seasons. We want to define per plate appearance statistics. Here is how we create the 2017 table. Keeping only players with more than 100 plate appearances.\n\nlibrary(Lahman)\ndat &lt;- Batting |&gt; filter(yearID == 2002) |&gt;\n  mutate(pa = AB + BB, \n         singles = (H - X2B - X3B - HR) / pa, bb = BB / pa) |&gt;\n  filter(pa &gt;= 100) |&gt;\n  select(playerID, singles, bb)\n\nNow compute a similar table but with rates computed over 1999-2001.\n2. You can use the inner_join function to combine the 2001 data and averages in the same table:\n\ndat &lt;- inner_join(dat, avg, by = \"playerID\")\n\nCompute the correlation between 2002 and the previous seasons for singles and BB.\n3. Note that the correlation is higher for BB. To quickly get an idea of the uncertainty associated with this correlation estimate, we will fit a linear model and compute confidence intervals for the slope coefficient. However, first make scatterplots to confirm that fitting a linear model is appropriate.\n4. Now fit a linear model for each metric and use the confint function to compare the estimates.\n5. In a previous section, we computed the correlation between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons, and noticed that the highest correlation is between fathers and sons and the lowest is between mothers and sons. We can compute these correlations using:\n\nlibrary(HistData)\nset.seed(1)\ngalton_heights &lt;- GaltonFamilies |&gt;\n  group_by(family, gender) |&gt;\n  sample_n(1) |&gt;\n  ungroup()\n\ncors &lt;- galton_heights |&gt; \n  pivot_longer(father:mother, names_to = \"parent\", values_to = \"parentHeight\") |&gt;\n  mutate(child = ifelse(gender == \"female\", \"daughter\", \"son\")) |&gt;\n  unite(pair, c(\"parent\", \"child\")) |&gt; \n  group_by(pair) |&gt;\n  summarize(cor = cor(parentHeight, childHeight))\n\nAre these differences statistically significant? To answer this, we will compute the slopes of the regression line along with their standard errors. Start by using lm and the broom package to compute the slopes LSE and the standard errors.\n6. Repeat the exercise above, but compute a confidence interval as well.\n7. Plot the confidence intervals and notice that they overlap, which implies that the data is consistent with the inheritance of height being independent of sex.\n8. Because we are selecting children at random, we can actually do something like a permutation test here. Repeat the computation of correlations 100 times taking a different sample each time. Hint: use similar code to what we used with simulations.\n9. Fit a linear regression model to obtain the effects of BB and HR on Runs (at the team level) in 1971. Use the tidy function in the broom package to obtain the results in a data frame.\n10. Now let’s repeat the above for each year since 1962 and make a plot. Use summarize and the broom package to fit this model for every year since 1962.\n11. Use the results of the previous exercise to plot the estimated effects of BB on runs.\n12. Advanced. Write a function that takes R, HR, and BB as arguments and fits two linear models: R ~ BB and R~BB+HR. Then use the summary function to obtain the BB for both models for each year since 1962. Then plot these against each other as a function of time.\n13. Since the 1980s, sabermetricians have used a summary statistic different from batting average to evaluate players. They realized walks were important and that doubles, triples, and HRs, should be weighed more than singles. As a result, they proposed the following metric:\n\\[\n\\frac{\\mbox{BB}}{\\mbox{PA}} + \\frac{\\mbox{Singles} + 2 \\mbox{Doubles} + 3 \\mbox{Triples} + 4\\mbox{HR}}{\\mbox{AB}}\n\\]\nThey called this on-base-percentage plus slugging percentage (OPS). Although the sabermetricians probably did not use regression, here we show how this metric is close to what one gets with regression.\nCompute the OPS for each team in the 2001 season. Then plot Runs per game versus OPS.\n14. For every year since 1962, compute the correlation between runs per game and OPS; then plot these correlations as a function of year.\n15. Note that we can rewrite OPS as a weighted average of BBs, singles, doubles, triples, and HRs. We know that the weights for doubles, triples, and HRs are 2, 3, and 4 times that of singles. But what about BB? What is the weight for BB relative to singles? Hint: the weight for BB relative to singles will be a function of AB and PA.\n16. Note that the weight for BB, \\(\\frac{\\mbox{AB}}{\\mbox{PA}}\\), will change from team to team. To see how variable it is, compute and plot this quantity for each team for each year since 1962. Then plot it again, but instead of computing it for every team, compute and plot the ratio for the entire year. Then, once you are convinced that there is not much of a time or team trend, report the overall average.\n17. So now we know that the formula for OPS is proportional to \\(0.91 \\times \\mbox{BB} + \\mbox{singles} + 2 \\times \\mbox{doubles} + 3 \\times \\mbox{triples} + 4 \\times \\mbox{HR}\\). Let’s see how these coefficients compare to those obtained with regression. Fit a regression model to the data after 1962, as done earlier: using per game statistics for each year for each team. After fitting this model, report the coefficients as weights relative to the coefficient for singles.\n18. We see that our linear regression model coefficients follow the same general trend as those used by OPS, but with slightly less weight for metrics other than singles. For each team in years after 1962, compute the OPS, the predicted runs with the regression model and compute the correlation between the two as well as the correlation with runs per game.\n19. We see that using the regression approach predicts runs slightly better than OPS, but not that much. However, note that we have been computing OPS and predicting runs for teams when these measures are used to evaluate players. Let’s show that OPS is quite similar to what one obtains with regression at the player level. For the 1962 season and after, compute the OPS and the predicted runs from our model for each player and plot them. Use the PA per game correction we used in the previous chapter:\n20. What players have show the largest difference between their rank by predicted runs and OPS?"
  },
  {
    "objectID": "linear-models/multivariate-regression.html#footnotes",
    "href": "linear-models/multivariate-regression.html#footnotes",
    "title": "14  Multivariate Regression",
    "section": "",
    "text": "http://mlb.mlb.com/stats/league_leaders.jsp↩︎\nhttps://en.wikipedia.org/wiki/Bill_James↩︎\nhttps://en.wikipedia.org/wiki/Sabermetrics↩︎\nhttps://en.wikipedia.org/wiki/User:Cburnett↩︎\nhttps://creativecommons.org/licenses/by-sa/3.0/deed.en↩︎\nhttps://www.flickr.com/people/27003603@N00↩︎\nhttps://creativecommons.org/licenses/by-sa/2.0↩︎\nhttp://www.baseball-almanac.com/awards/lou_brock_award.shtml↩︎"
  },
  {
    "objectID": "linear-models/measurement-error-models.html#exercises",
    "href": "linear-models/measurement-error-models.html#exercises",
    "title": "\n15  Measurement error models\n",
    "section": "\n15.1 Exercises",
    "text": "15.1 Exercises\n1. Plot of co2 evels for the first 12 months of the co2 dataset and notice it seems to follow a sin wave with frequency 1 cycle per month. This means that a measurement error model that might work is\n\\[\ny_i = \\mu + A \\sin(2\\pi t_i / 12 + \\phi) + \\varepsilon_i\n\\] with \\(t_i\\) the month number of observation \\(i\\). Is this a linear model for the parameters \\(mu\\), \\(A\\) and \\(\\phi\\)?\n2. Using trigonometry we can show that we can rewrite this model as\n$$ y_i = _0 + _1 (2t_i/12) + _2 (2t_i/12) + _i\n$$ Is this a linear model?\n3. Find least square estimates for the \\(\\beta\\)s using lm. Show a plot of \\(y_i\\) versus \\(t_i\\) with a curve on the same plot showing \\(\\hat{Y}_i\\) versus \\(t_i\\).\n4. Now fit a measurement error model to the entire co2 dataset that includes a trend term that is a parabola as well as the sine wave model.\n5. Run diagnostic plots for the fitted model and describe the results."
  },
  {
    "objectID": "linear-models/treatment-effect-models.html#comparing-group-means",
    "href": "linear-models/treatment-effect-models.html#comparing-group-means",
    "title": "16  Treatment effect models",
    "section": "\n16.1 Comparing group means",
    "text": "16.1 Comparing group means\nThe sample averages for the two groups, high-fat and chow diets, are different:\n\nlibrary(tidyverse)\nmice_weights |&gt; group_by(diet) |&gt; summarize(average = mean(body_weight))\n#&gt; # A tibble: 2 × 2\n#&gt;   diet  average\n#&gt;   &lt;fct&gt;   &lt;dbl&gt;\n#&gt; 1 chow     31.5\n#&gt; 2 hf       36.7\n\nBut this is a random sample of mice and the assignment to the diet group is also random. So is this difference due to chance? We will use hypothesis testing, first described in Chapter Chapter 9, to answer this question.\nDenote with \\(\\mu_1\\) and \\(\\sigma_1\\) the weight average and standard deviation we would observe if the entire population of mice were on the high-fat diet. Define \\(\\mu_0\\) and \\(\\sigma_0\\) similarly for the chow diet. Define \\(N_1\\) and \\(N_0\\) as the sample sizes, let’s call them \\(\\bar{X}_1\\) and \\(\\bar{X}_0\\) as the sample averages, and \\(s_1\\) and \\(s_0\\) the sample standard deviations for the for the high-fat and chow diets, respectively. Because this is a random sample the central limit theorem tells us that the difference in averages \\(bar{X}_1 - \\bar{X}_0\\) follows a normal distribution with expected value \\(\\mu_1-\\mu_0\\) and standard error \\(\\sqrt{\\frac{s_1^2}{N_1} + \\frac{s_0^2}{N_0}}\\). If we define the null hypothesis as the high-fat diet having no effect, or \\(\\mu_1 - \\mu_0 = 0\\), the the following summary statistic\n\\[\nt = \\frac{\\bar{X}_1 - \\bar{X}_0}{\\sqrt{\\frac{s_1^2}{N_1} + \\frac{s_0^2}{N_0}}}\n\\]\nfollows a standard normal distribution when the null hypothesis is true, which implies we can easily compute the probability of observing a value as large as the one we did:\n\nstats &lt;- mice_weights |&gt; group_by(diet) |&gt; summarize(xbar = mean(body_weight), s = sd(body_weight), n = n()) \nt_stat &lt;- with(stats, (xbar[2] - xbar[1])/sqrt(s[2]^2/n[2] + s[1]^2/n[1]))\nt_stat\n#&gt; [1] 9.34\n\nHere \\(t\\) is well over 3, so we don’t really need to compute the p-value 1-pnorm(t_stat) as we know it will be very small.\nNote that when \\(N\\) is not large, then the CLT does not apply. However, if the outcome data, in this case weight, follows a normal distribution, then \\(t\\) follows a t-distribution with \\(N_1+N_2-2\\) degrees of freedom. So the calculation of the p-value is the same except we use 1-pt(t_stat, with(stats, n[2]+n[1]-2) to compute the p-value.\nBecause using differences in mean are so common in scientific studies, this t-statistic is one of the most widely reported summaries. When use it in a hypothesis testing setting, it is referred to a performing a t test.\n\n\n\n\n\n\nIn the computation above we computed the probability of t being as large as what we observed. However, when we are equally interested in both directions, for example, either an increase or decrease in weight, then we need to compute the probability of t being as extreme as what we observe. The formula simply changes to using the absolute value: 1 - pnorm(abs(t-test)) or 1-pt(t_stat, with(stats, n[2]+n[1]-2)."
  },
  {
    "objectID": "linear-models/treatment-effect-models.html#one-factor-design",
    "href": "linear-models/treatment-effect-models.html#one-factor-design",
    "title": "16  Treatment effect models",
    "section": "\n16.2 One factor design",
    "text": "16.2 One factor design\nAlthough the t-test is useful for cases in which we only account for two treatments, it is common to have other variables affect our outcomes. Linear models permit hypothesis testing in more general situations. We start the description of the use linear models for estimating treatment effects by demonstrating how they can be used to perform t-tests.\nIf we assume that the weight distributions for both chow and high-fat diets are normally distributed, we can write the following linear model to represent the data:\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\n\\] with \\(X_i\\) 1 if the \\(i\\)-th mice was fed the high-fat diet and 0 otherwise and the errors \\(\\varepsilon_i\\) independent and normally distributed with expected value 0 and standard deviation \\(\\sigma\\). Note that this mathematical formula looks exactly like the model we wrote out for the father-son heights. However, the fact that \\(x_i\\) is now 0 or 1 rather than a continuous variable, permits us to use it in this different context. In particular notice that now \\(\\beta_0\\) represents the population average height of the mice on the chow diet and \\(\\beta_0 + \\beta_1\\) represents the population average for the weight of the mice on the high-fat diet.\nA nice feature of this model is that \\(\\beta_1\\) represents the treatment effect of receiving the high-fat diet. If the null hypothesis that the high-fat diet has no effect can be quantified as \\(\\beta_1 = 0\\). We can then estimate \\(\\beta_1\\) and answer the question of weather or not the observed difference is real by computing the estimates being as large as it was under the null. So how do we estimate \\(\\beta_1\\) and a standard error for the estimate?\nA powerful characteristics of linear models is that we can can estimate the parameters \\(\\beta\\)s and their standard errors with the same LSE machinery:\n\nfit &lt;- lm(body_weight ~ diet, data = mice_weights)\n\nBecause diet is a factor with two entries, the lm function knows to fit the model above with a \\(x_i\\) a indicator variable. The summary function shows us the resulting estimate, standard error, and p-value:\n\ncoefficients(summary(fit))\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)\n#&gt; (Intercept)    31.54      0.386   81.74 0.00e+00\n#&gt; diethf          5.14      0.548    9.36 8.02e-20\n\nor using broom we can write:\n\nlibrary(broom)\ntidy(fit, conf.int = TRUE) |&gt; filter(term == \"diethf\")\n#&gt; # A tibble: 1 × 7\n#&gt;   term   estimate std.error statistic  p.value conf.low conf.high\n#&gt;   &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 diethf     5.14     0.548      9.36 8.02e-20     4.06      6.21\n\nThe statistic computed here is the estimate divided by its standard error: \\(\\hat{beta}_1 / \\hat{\\mbox{SE}}(\\hat{beta}_1)\\). In the case for the simple one-factor model, we can show that this statistic is almost equivalent to the t-test. Intuitively it makes since both \\(\\hat{\\beta_1}\\) and the numerator of the t-test are estimates of the treatment effect. In fact, we can see that we obtain a number similar to the \\(t\\) computed in the previous section.\n\nc(coefficients(summary(fit))[2,3], t_stat)\n#&gt; [1] 9.36 9.34\n\nOne minor difference is that the linear model does not assume a different standard deviation for each population. Instead, both populations share \\(\\mbox{SD}(\\varepislon)\\) as a standard deviation. Note that, although we do not show how to do it with R here, we can redefine the linear model to have different standard errors for each group.\n\n\n\n\n\n\nIn the linear model description provided here we assumed \\(\\varepsilon\\) follows a normal distribution. This assumption permits us to show that the statistics formed by dividing estimates by their estimated standard errors follow t-distribution, which in turn permits us to estimate p-values or confidence intervals. However, note that we do not need this assumption to compute the expected value and standard error of the least squared estimates. Furthermore, if the number of observations in large enough, then the central limit theorem applies and we can obtain p-values and confidence intervals even without the normal distribution assumption."
  },
  {
    "objectID": "linear-models/treatment-effect-models.html#two-factor-designs",
    "href": "linear-models/treatment-effect-models.html#two-factor-designs",
    "title": "16  Treatment effect models",
    "section": "\n16.3 Two factor designs",
    "text": "16.3 Two factor designs\nNote that this experiment included male and female mice, and male mice are known to be heavier. This explains why the residuals depend on the sex variable:\n\nboxplot(fit$residuals ~ mice_weights$sex)\n\n\n\n\n\n\n\nThis misspecification can have real implications since if more male mice received the high-fat diet, then this could explain the increase. Or if less received it, then we might underestimate the diet effect. Sex might be a confounder. Our model can certainly be improved.\nFrom examining the data:\n\nmice_weights |&gt; ggplot(aes(diet, log2(body_weight), fill = sex)) + geom_boxplot()\n\n\n\n\n\n\n\nwe see that there diet effect is observed for both sexes and that males are heavier than females. Although not nearly as obvious, it also appears the diet effect is stronger in males. A linear models that permits a different expected value four groups, 1) female on chow diet, 2) females on high-fat diet, 3) male on chow diet, and 4)males on high-fat diet,\n\\[\nY_i = \\beta_1 x_{i,1} + \\beta_2 x_{i,2}  + \\beta_3 x_{i,3}  + \\beta_4 x_{i,4}  + \\varepsilon_i\n\\] with the \\(x_i\\)s indicator variables for each of the four groups. However, with this representation, none of the \\(\\beta\\)s represent the effect of interest: the diet effect. Furthermore, we now are accounting for the possibility that the diet effect is different for males and females have a different, and can test that hypothesis as well.\nA powerful feature of linear models is that we can rewrite the model so that we still have a different expected value for each group, but the parameters represent effects we are interested. So, for example, in the representation\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i,1}  + \\beta_2 x_{i,2}  + \\beta_3 x_{i,1} x_{i,2}  + \\varepsilon_i\n\\] with \\(x_{i,1}\\) and indicator that is one if you have the treatment and \\(x_{i,2}\\) an indicator that is one if you are male, the \\(\\beta_1\\) can be interpreted as the treatment effect for females, \\(\\beta_2\\) as the difference between males and females, and \\(\\beta_3\\) the added treatment effect for males. In Statistics, this last effect is referred to as an interaction effect. The \\(\\beta_0\\) is consider the baseline value which is the average weight of females on the chow diet.\nStatistical textbooks describes several other ways in which the model can be rewritten to obtain other types of interpretations. For example, we might want \\(\\beta_2\\) to represent the average treatment effect between females and males, rather that the female treatment effects. This is achieved by defining what contrasts we are interested.\nIn R we can specific this model using the following\n\nfit &lt;- lm(body_weight ~ diet*sex, data = mice_weights)\n\nThe * implies that the term that multiplies \\(x_{i,1}\\) and \\(x_{i,2}\\) should be included, along with the \\(x_{i,1}\\) and \\(x_{i,2}\\) terms.\n\ntidy(fit, conf.int = TRUE) |&gt; filter(!str_detect(term, \"Intercept\"))\n#&gt; # A tibble: 3 × 7\n#&gt;   term        estimate std.error statistic  p.value conf.low conf.high\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 diethf          3.88     0.624      6.22 8.02e-10    2.66       5.10\n#&gt; 2 sexM            7.53     0.627     12.0  1.27e-30    6.30       8.76\n#&gt; 3 diethf:sexM     2.66     0.891      2.99 2.91e- 3    0.912      4.41\n\nNote that the male effect is larger that the diet effect, and the diet effect is statistically significant for both sexes, with the males having a higher effect by between 1 and 4.5 grams.\nA common approach applied when more than one factor is thought to affect the measurement is to simply include an additive effect for each factor like this:\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i,1}  + \\beta_2 x_{i,2}   + \\varepsilon_i\n\\]\nIn this model, the \\(\\beta_1\\) is a general diet effect that applies regardless of sex. In R we use the following code using a + instead of *:\n\nfit &lt;- lm(body_weight ~ diet + sex, data = mice_weights)\n\nBecause their a strong interaction effect, a diagnostic plots shows that the residuals are biased: the average negative for females on the diet and positive for the males on the diet, rather than 0.\n\nplot(fit, which = 1)\n\n\n\n\n\n\n\nScientific studies, particularly within epidemiology and social sciences, frequently omit interaction terms from models due to the high number of variables. Adding interactions necessitates numerous parameters, which, in extreme cases, may prevent the model from fitting. However, this approach assumes that the interaction terms are zero, which, if incorrect, can skew the results’ interpretation. Conversely, when this assumption is valid, models excluding interactions are simpler to interpret as parameters are typically viewed as the extent to which the outcome increases with the assigned treatment.\n\n\n\n\n\n\nLinear models are very flexible and can be applied in many contexts. For example, we can include many more factors than 2. We have just scratched the surface of how linear models can be used to estimate treatment effects. We highly recommend learning more about this through linear model textbooks and R manuals on using the lm, contrasts, and model.matrix functions."
  },
  {
    "objectID": "linear-models/treatment-effect-models.html#analysis-of-variance",
    "href": "linear-models/treatment-effect-models.html#analysis-of-variance",
    "title": "16  Treatment effect models",
    "section": "\n16.4 Analysis of variance",
    "text": "16.4 Analysis of variance\nIn the example we have examined, each treatment had only two levels: diet and chow and high-fat and sex had female and male. However, often we have variables of interest that have more than one level. For example, we might have tested a third diet on the mice. In statistics textbooks these variables are referred to as factor. In these cases it is common to want to know rather than the effect of each levels of the factor, a more general quantification regarding the variability across the levels. Analysis of variances or ANOVA does just this. The summary used to quantify the variability of a factor is the mean squared error of the estimated effects of each level.\nAs an example, consider that the mice in our dataset are actually from several generations:\n\ntable(mice_weights$gen)\n#&gt; \n#&gt;   4   7   8   9  11 \n#&gt;  97 195 193  97 198\n\nWe can fit a linear model that fits an effect for each of these generations along with diet and sex model previously fit:\n\nfit &lt;- lm(body_weight ~ diet * sex + gen,  data = mice_weights)\n\nWe can then perform an analysis of variance with the R aov function:\n\nsummary(aov(fit))\n#&gt;              Df Sum Sq Mean Sq F value Pr(&gt;F)    \n#&gt; diet          1   5143    5143  133.58 &lt;2e-16 ***\n#&gt; sex           1  15260   15260  396.33 &lt;2e-16 ***\n#&gt; gen           4    295      74    1.91 0.1061    \n#&gt; diet:sex      1    349     349    9.06 0.0027 ** \n#&gt; Residuals   772  29725      39                   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThis analysis shows that the largest variation is explained by sex and then diet. The generation factor explains very little variation in comparison and is not found to be statistically significant.\n\n\n\n\n\n\nIn this book, we do not provide the details for how we compute this p-value. There are several books on analysis of variance and textbooks on linear models often include chapters on this topic. Those interested in learning more about these topics can consult these textbooks."
  },
  {
    "objectID": "linear-models/treatment-effect-models.html#exercises",
    "href": "linear-models/treatment-effect-models.html#exercises",
    "title": "16  Treatment effect models",
    "section": "\n16.5 Exercises",
    "text": "16.5 Exercises\n1. Once you fit a model, the estimate of the standard error \\(\\sigma\\) can be obtained like this:\n\nfit &lt;- lm(body_weight ~ diet, data = mice_weights)\nsummary(fit)$sigma\n\nCompute the estimate of \\(\\sigma\\) using the model that includes just diet and a model that accounts for sex. Are the estimates the same? If not, why not?\n2. One of the assumption of the linear model fit by lm is that the standard deviation of the errors \\(\\varepsilon_i\\) is equal for all \\(i\\). This implies it does not depend on the expected value. Group the mice by their weight like this:\n\nbreaks &lt;- with(mice_weights, seq(min(body_weight), max(body_weight), 1))\ndat &lt;- mutate(mice_weights, group = cut(body_weight, breaks, include_lowest = TRUE))\n\nCompute the average and standard deviation for groups having more than 10 observations and use data exploration to see if this assumption holds?\n3. The dataset also includes a variable indicating which litter the mice came from. Create a boxplot showing weights by litter. Use faceting to make separate plots for each diet and sex combination.\n\nmice_weights |&gt; ggplot(aes(litter, body_weight)) + geom_boxplot() + facet_grid(sex~diet)\n\n\n\n\n\n\n\n4. Use a linear model to test for a litter effect. Account for sex and diet. Use ANOVA to compare the variability explained by litter to other factors.\n5. The mouse_weights data includes two other outcomes: bone density and percent fat. Make a boxplot showing bone density by sex and diet. Compare what the visualizations shows for the diet effect by sex.\n\nFit a linear model and test for the diet effect on bone density separately for each sex. Note that the diet effect is statistically significant for females but not for males. Then fit the model to the entire dataset that includes diet, sex and their interaction. Note that the diet effect is significant, yet the interaction effect is not. Explain how this can happen? Hint: To fit a model to the entire dataset that fit a separate effect for males and females you can use the formula ~ sex + diet:sex\n\n\n7. In Chapter Chapter 10, we talked about pollster bias. We used visualization to motivate the presence of such bias. Here we will give it a more rigorous treatment. We will consider two pollsters that conducted daily polls. We will look at national polls for the month before the election.\n\npolls &lt;- polls_us_election_2016 |&gt; \n  filter(pollster %in% c(\"Rasmussen Reports/Pulse Opinion Research\",\n                         \"The Times-Picayune/Lucid\") &\n           enddate &gt;= \"2016-10-15\" &\n           state == \"U.S.\") |&gt; \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) \n\nWe want to answer the question: is there a poll bias? Make a plot showing the spreads for each poll.\n8. The data does seem to suggest there is a difference. However, these data are subject to variability. Perhaps the differences we observe are due to chance.\nThe urn model theory says nothing about pollster effect. Under the urn model, both pollsters have the same expected value: the election day difference, that we call \\(\\mu\\).\nTo answer the question “is there an urn model?”, we will model the observed data \\(Y_{i,j}\\) in the following way:\n\\[\nY_{i,j} = \\mu + b_i + \\varepsilon_{i,j}\n\\]\nwith \\(i=1,2\\) indexing the two pollsters, \\(b_i\\) the bias for pollster \\(i\\) and \\(\\varepsilon_ij\\) poll to poll chance variability. We assume the \\(\\varepsilon\\) are independent from each other, have expected value \\(0\\) and standard deviation \\(\\sigma_i\\) regardless of \\(j\\).\nWhich of the following best represents our question?\n\nIs \\(\\varepsilon_{i,j}\\) = 0?\nHow close are the \\(Y_{i,j}\\) to \\(\\mu\\)?\nIs \\(b_1 \\neq b_2\\)?\nAre \\(b_1 = 0\\) and \\(b_2 = 0\\) ?\n\n9. In the right side of this model only \\(\\varepsilon_{i,j}\\) is a random variable. The other two are constants. What is the expected value of \\(Y_{1,j}\\)?\n10. Suppose we define \\(\\bar{Y}_1\\) as the average of poll results from the first poll, \\(Y_{1,1},\\dots,Y_{1,N_1}\\) with \\(N_1\\) the number of polls conducted by the first pollster:\n\npolls |&gt; \n  filter(pollster==\"Rasmussen Reports/Pulse Opinion Research\") |&gt; \n  summarize(N_1 = n())\n\nWhat is the expected values \\(\\bar{Y}_1\\)?\n11. What is the standard error of \\(\\bar{Y}_1\\) ?\n12. Suppose we define \\(\\bar{Y}_2\\) as the average of poll results from the first poll, \\(Y_{2,1},\\dots,Y_{2,N_2}\\) with \\(N_2\\) the number of polls conducted by the first pollster. What is the expected value \\(\\bar{Y}_2\\)?\n13. What is the standard error of \\(\\bar{Y}_2\\) ?\n14. Using what we learned by answering the questions above, what is the expected value of \\(\\bar{Y}_{2} - \\bar{Y}_1\\)?\n15. Using what we learned by answering the questions above, what is the standard error of \\(\\bar{Y}_{2} - \\bar{Y}_1\\)?\n16. The answer to the question above depends on \\(\\sigma_1\\) and \\(\\sigma_2\\), which we don’t know. We learned that we can estimate these with the sample standard deviation. Write code that computes these two estimates.\n17. What does the CLT tell us about the distribution of \\(\\bar{Y}_2 - \\bar{Y}_1\\)?\n\nNothing because this is not the average of a sample.\nBecause the \\(Y_{ij}\\) are approximately normal, so are the averages.\nNote that \\(\\bar{Y}_2\\) and \\(\\bar{Y}_1\\) are sample averages, so if we assume \\(N_2\\) and \\(N_1\\) are large enough, each is approximately normal. The difference of normally distributed variables is also normally distributed.\nThe data are not 0 or 1, so CLT does not apply.\n\n18. We have constructed a random variable that has expected value \\(b_2 - b_1\\), the pollster bias difference. If our model holds, then this random variable has an approximately normal distribution and we know its standard error. The standard error depends on \\(\\sigma_1\\) and \\(\\sigma_2\\), but we can plug the sample standard deviations we computed above. We started off by asking: is \\(b_2 - b_1\\) different from 0? Use all the information we have learned above to construct a 95% confidence interval for the difference \\(b_2\\) and \\(b_1\\).\n19. The confidence interval tells us there is relatively strong pollster effect resulting in a difference of about 5%. Random variability does not seem to explain it. We can compute a p-value to relay the fact that chance does not explain it. What is the p-value?\n20. The statistic formed by dividing our estimate of \\(b_2-b_1\\) by its estimated standard error:\n\\[\n\\frac{\\bar{Y}_2 - \\bar{Y}_1}{\\sqrt{s_2^2/N_2 + s_1^2/N_1}}\n\\]\nis the t-statistic. Now notice that we have more than two pollsters. We can also test for pollster effect using all pollsters, not just two. The idea is to compare the variability across polls to variability within polls.\nFor this exercise, create a new table:\n\npolls &lt;- polls_us_election_2016 |&gt; \n  filter(enddate &gt;= \"2016-10-15\" &\n           state == \"U.S.\") |&gt;\n  group_by(pollster) |&gt;\n  filter(n() &gt;= 5) |&gt; \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) |&gt;\n  ungroup()\n\nCompute the average and standard deviation for each pollster and examine the variability across the averages and how it compares to the variability within the pollsters, summarized by the standard deviation."
  },
  {
    "objectID": "linear-models/association-tests.html#case-study-funding-success-rates",
    "href": "linear-models/association-tests.html#case-study-funding-success-rates",
    "title": "17  Association tests",
    "section": "\n17.1 Case study: funding success rates",
    "text": "17.1 Case study: funding success rates\nA 2014 PNAS paper1 analyzed success rates from funding agencies in the Netherlands and concluded that their:\n\nresults reveal gender bias favoring male applicants over female applicants in the prioritization of their “quality of researcher” (but not “quality of proposal”) evaluations and success rates, as well as in the language use in instructional and evaluation materials.\n\nThe main evidence for this conclusion comes down to a comparison of the percentages. Table S1 in the paper includes the information we need. Here are the three columns showing the overall outcomes:\n\nlibrary(tidyverse)\nlibrary(dslabs)\nresearch_funding_rates |&gt; select(discipline, applications_total, \n                                  success_rates_total) |&gt; head()\n#&gt;           discipline applications_total success_rates_total\n#&gt; 1  Chemical sciences                122                26.2\n#&gt; 2  Physical sciences                174                20.1\n#&gt; 3            Physics                 76                26.3\n#&gt; 4         Humanities                396                16.4\n#&gt; 5 Technical sciences                251                17.1\n#&gt; 6  Interdisciplinary                183                15.8\n\nWe have these values for each gender:\n\nnames(research_funding_rates)\n#&gt;  [1] \"discipline\"          \"applications_total\"  \"applications_men\"   \n#&gt;  [4] \"applications_women\"  \"awards_total\"        \"awards_men\"         \n#&gt;  [7] \"awards_women\"        \"success_rates_total\" \"success_rates_men\"  \n#&gt; [10] \"success_rates_women\"\n\nWe can compute the totals that were successful and the totals that were not as follows:\n\ntotals &lt;- research_funding_rates |&gt; \n  select(-discipline) |&gt; \n  summarize_all(sum) |&gt;\n  summarize(yes_men = awards_men, \n            no_men = applications_men - awards_men, \n            yes_women = awards_women, \n            no_women = applications_women - awards_women) \n\nSo we see that a larger percent of men than women received awards:\n\ntotals |&gt; summarize(percent_men = yes_men/(yes_men+no_men),\n                    percent_women = yes_women/(yes_women+no_women))\n#&gt;   percent_men percent_women\n#&gt; 1       0.177         0.149\n\nBut could this be due just to random variability? Here we learn how to perform inference for this type of data."
  },
  {
    "objectID": "linear-models/association-tests.html#lady-tasting-tea",
    "href": "linear-models/association-tests.html#lady-tasting-tea",
    "title": "17  Association tests",
    "section": "\n17.2 Lady Tasting Tea",
    "text": "17.2 Lady Tasting Tea\nR.A. Fisher2 was one of the first to formalize hypothesis testing. The “Lady Tasting Tea” is one of the most famous examples.\nThe story is as follows: an acquaintance of Fisher’s claimed that she could tell if milk was added before or after tea was poured. Fisher was skeptical. He designed an experiment to test this claim. He gave her four pairs of cups of tea: one with milk poured first, the other after. The order was randomized. The null hypothesis here is that she is guessing. Fisher derived the distribution for the number of correct picks on the assumption that the choices were random and independent.\nAs an example, suppose she picked 3 out of 4 correctly. Do we believe she has a special ability? The basic question we ask is: if the tester is actually guessing, what are the chances that she gets 3 or more correct? Just as we have done before, we can compute a probability under the null hypothesis that she is guessing 4 of each. Under this null hypothesis, we can think of this particular example as picking 4 balls out of an urn with 4 blue (correct answer) and 4 red (incorrect answer) balls. Remember, she knows that there are four before tea and four after.\nUnder the null hypothesis that she is simply guessing, each ball has the same chance of being picked. We can then use combinations to figure out each probability. The probability of picking 3 is \\(\\binom{4}{3} \\binom{4}{1} / \\binom{8}{4} = 16/70\\). The probability of picking all 4 correct is \\(\\binom{4}{4} \\binom{4}{0} / \\binom{8}{4}= 1/70\\). Thus, the chance of observing a 3 or something more extreme, under the null hypothesis, is \\(\\approx 0.24\\). This is the p-value. The procedure that produced this p-value is called Fisher’s exact test and it uses the hypergeometric distribution."
  },
  {
    "objectID": "linear-models/association-tests.html#two-by-two-tables",
    "href": "linear-models/association-tests.html#two-by-two-tables",
    "title": "17  Association tests",
    "section": "\n17.3 Two-by-two tables",
    "text": "17.3 Two-by-two tables\nThe data from the experiment is usually summarized by a table like this:\n\ntab &lt;- matrix(c(3,1,1,3),2,2)\nrownames(tab)&lt;-c(\"Poured Before\",\"Poured After\")\ncolnames(tab)&lt;-c(\"Guessed before\",\"Guessed after\")\ntab\n#&gt;               Guessed before Guessed after\n#&gt; Poured Before              3             1\n#&gt; Poured After               1             3\n\nThese are referred to as a two-by-two table. For each of the four combinations one can get with a pair of binary variables, they show the observed counts for each occurrence.\nThe function fisher.test performs the inference calculations above:\n\nfisher.test(tab, alternative=\"greater\")$p.value\n#&gt; [1] 0.243"
  },
  {
    "objectID": "linear-models/association-tests.html#chi-square-test",
    "href": "linear-models/association-tests.html#chi-square-test",
    "title": "17  Association tests",
    "section": "\n17.4 Chi-square Test",
    "text": "17.4 Chi-square Test\nNotice that, in a way, our funding rates example is similar to the Lady Tasting Tea. However, in the Lady Tasting Tea example, the number of blue and red beads is experimentally fixed and the number of answers given for each category is also fixed. This is because Fisher made sure there were four cups with milk poured before tea and four cups with milk poured after and the lady knew this, so the answers would also have to include four before and four afters. If this is the case, the sum of the rows and the sum of the columns are fixed. This defines constraints on the possible ways we can fill the two by two table and also permits us to use the hypergeometric distribution. In general, this is not the case. Nonetheless, there is another approach, the Chi-squared test, which is described below.\nImagine we have 290, 1,345, 177, 1,011 applicants, some are men and some are women and some get funded, whereas others don’t. We saw that the success rates for men and woman were:\n\ntotals |&gt; summarize(percent_men = yes_men/(yes_men+no_men),\n                    percent_women = yes_women/(yes_women+no_women))\n#&gt;   percent_men percent_women\n#&gt; 1       0.177         0.149\n\nrespectively. Would we see this again if we randomly assign funding at the overall rate:\n\nrate &lt;- with(totals, (yes_men + yes_women))/sum(totals)\nrate\n#&gt; [1] 0.165\n\nThe Chi-square test answers this question. The first step is to create the two-by-two data table:\n\ntwo_by_two &lt;- with(totals, data.frame(awarded = c(\"no\", \"yes\"), \n                                      men = c(no_men, yes_men),\n                                      women = c(no_women, yes_women)))\ntwo_by_two\n#&gt;   awarded  men women\n#&gt; 1      no 1345  1011\n#&gt; 2     yes  290   177\n\nThe general idea of the Chi-square test is to compare this two-by-two table to what you expect to see, which would be:\n\nwith(totals, data.frame(awarded = c(\"no\", \"yes\"), \n                        men = (no_men + yes_men) * c(1 - rate, rate),\n                        women = (no_women + yes_women) * c(1 - rate, rate)))\n#&gt;   awarded  men women\n#&gt; 1      no 1365   991\n#&gt; 2     yes  270   197\n\nWe can see that more men than expected and fewer women than expected received funding. However, under the null hypothesis these observations are random variables. The Chi-square test tells us how likely it is to see a deviation this large or larger. This test uses an asymptotic result, similar to the CLT, related to the sums of independent binary outcomes. The R function chisq.test takes a two-by-two table and returns the results from the test:\n\nchisq_test &lt;- chisq.test(two_by_two[, -1])\n\nWe see that the p-value is 0.0509:\n\nchisq_test$p.value\n#&gt; [1] 0.0509"
  },
  {
    "objectID": "linear-models/association-tests.html#sec-glm",
    "href": "linear-models/association-tests.html#sec-glm",
    "title": "17  Association tests",
    "section": "\n17.5 Generalized linear models",
    "text": "17.5 Generalized linear models\nWe presented a way to perform hypothesis testing for determining if there is association between two binary outcome. But we have not yet described how to quantify effects. Can we estimate the effect of being a woman in funding success in the Netherlands? Note that if our outcomes are binary, then the linear models presented in the Chapter Chapter 16 are not appropriate because the \\(\\beta\\)s and \\(\\varepsilon\\) are continuous. However, an adaptation of these methods, that is widely used in, for example, medical studies, gives us a way to estimate effects along with their standard errors.\nThe idea is to model a transformation of the expected value of the outcomes with a linear model. The transformation is selected so that any continuous value is possible. The mathematical equation for a model with one variable looks like this:\n\\[\ng\\{\\mbox{E}(Y_i)\\} = \\beta_0 + \\beta_1 x_i\n\\]\nTo finish describing the model we impose a distribution on \\(Y\\) such as binomial or Poisson. These are referred to as _generalized linear models.\nWe demonstrate with the funding rates example. We define \\(Y_i\\) to be 1 if person \\(i\\) received funding and 0 otherwise and \\(x_i\\) to be 1 for person \\(i\\) is a women and 0 for men. For this data the expected value of \\(Y_i\\) is the probability of funding for person \\(i\\) \\(\\mbox{Pr}(Y_i=1)\\). We assume the outcomes \\(Y_i\\) are binomial with \\(N=1\\) and probability \\(p_i\\). For binomial data, the most widely used transformation is the logit function \\(g(p) = \\log \\{p / (1-p)\\}\\) which takes numbers between 0 and 1 to any continuous number. The model looks like this:\n\\[\n\\log \\frac{\\mbox{Pr}(Y_i=1)}{1-\\mbox{Pr}(Y_i=1)} = \\beta_0 +  \\beta_1 x_i\n\\]\n\n17.5.1 The odds ratio\nTo understand how \\(\\beta_1\\) can be used to quantify the effect of being a woman on success rates, first note that \\(\\mbox{Pr}(Y_i=1)/\\{1-\\mbox{Pr}(Y_i=1)\\} = \\mbox{Pr}(Y_i=1)/\\mbox{Pr}(Y_i=0)\\) is the odds of person \\(i\\) getting funding: the ratio of the probability of success and probability of failure. This implies that \\(e^{\\beta_0}\\) is the odds for men and \\(e^{\\beta_0}e^{\\beta_1}\\) is the odds for women, which implies \\(\\beta_1\\) is the odds for women divided by the odds for men. This quantity is called the odds ratio. To see this not that if use \\(p_1\\) and \\(p_0\\) to denote the probability of success for women and men, respectively, then \\(e^\\{beta_1\\) can be rewritten as\n\\[\ne^{\\beta_1} = \\frac{p_1}{1-p_1} \\, / \\, \\frac{p_0}{1-p_0}\n\\]\n\\(\\beta_1\\) therefore quantifies the log odds ratio.\nNow how do we estimate these parameters? Although the details are not described in this book, least squares is no longer an optimal way of estimating the parameters and instead we use an approach called maximum likelihood estimation (MLE). More advanced mathematical derivations show that a version of the central limit theorem applies and the estimates obtained this way are approximately normal when th number of observations is large. The theory also provides a way to calculate standard errors for the estimates of the \\(\\beta\\)s.\n\n17.5.2 Fitting the model\nTo obtain the maximum likelihood estimates using R we can use the glm function with the family argument set to binomial. This defaults to using the logit transformation. Note that we do not have the individual level data, but because we our model assumes the probability of success is the same for all women and all men, then the number of success can be modeled as binomial with \\(N_1\\) trials and probability \\(p_1\\) for women and binomial with \\(N_0\\) trials and probability \\(p_0\\) for men, with \\(N_1\\) and \\(N_0\\) the total number of women and men. In this case the glm function is used like this:\n\nsuccess &lt;- with(totals, c(yes_men, yes_women))\nfailure &lt;- with(totals, c(no_men, no_women))\ngender &lt;- factor(c(\"men\", \"women\"))\nfit &lt;- glm(cbind(success, failure) ~ gender, family = \"binomial\") \ncoefficients(summary(fit))\n#&gt;             Estimate Std. Error z value  Pr(&gt;|z|)\n#&gt; (Intercept)   -1.534     0.0647   -23.7 3.83e-124\n#&gt; genderwomen   -0.208     0.1041    -2.0  4.54e-02\n\nThe estimate of the odds ratio is 0.811982 which is interpreted as the odds being lowered by 20% for women as compared to men. But is this due to chance? We already noted that the p-value is about 0.05, but the GLM approach also permits us to compute confidence intervals using the confint function. To show the interval for the more interpretable odds ratio we simply exponentiate:\n\nexp(confint(fit, 2))\n#&gt;  2.5 % 97.5 % \n#&gt;  0.661  0.995\n\n\n\n\n\n\n\nWe have used a simple version of GLMs in which the only variable is binary. However, the method can be expanded to use multiple variables including continuous ones. However, in these contexts the log odds ratio interpretation becomes more complex. Also note that we have shown just one version of GLM appropriate for binomial data using a logit transformation. This version is referred to often referred to as logistic regression. However, GLM can be used with other transformation and distributions. You can learn more by consulting a GLM text book.\n\n\n\n\n17.5.3 Simple standard error approximation for two-by-two table odds ratio\nUsing glm we can obtain estimates, standard errors, and confidence intervals for a wide range of models. To do this we use a rather complex algorithms. In the case of two-by-two tables we can obtain a standard error for the log odds ratio using a simple approximation.\nIf our two-by-two tables has the following entries:\n\n\n\n\n\nMen\nWomen\n\n\n\nAwarded\na\nb\n\n\nNot Awarded\nc\nd\n\n\n\n\n\nIn this case, the odds ratio is simply \\(\\frac{a/c}{b/d} = \\frac{ad}{bc}\\). We can confirm we obtain the same estimate as when using glm:\n\nor &lt;- with(two_by_two, women[2]/sum(women) / (women[1]/sum(women)) / ((men[2]/sum(men)) / (men[1]/sum(men))))\nc(log(or), fit$coef[2])\n#&gt;             genderwomen \n#&gt;      -0.208      -0.208\n\nStatistical theory tells us that when all four entries of the two-by-two table are large enough, then the log odds ratio is approximately normal with standard error\n\\[\n\\sqrt{1/a + 1/b + 1/c + 1/d}\n\\]\nThis implies that a 95% confidence interval for the log odds ratio can be formed by:\n\\[\n\\log\\left(\\frac{ad}{bc}\\right) \\pm 1.96 \\sqrt{1/a + 1/b + 1/c + 1/d}\n\\]\nBy exponentiating these two numbers we can construct a confidence interval of the odds ratio.\nUsing R we can compute this confidence interval as follows:\n\nse &lt;- two_by_two |&gt; select(-awarded) |&gt;\n  summarize(se = sqrt(sum(1/men) + sum(1/women))) |&gt;\n  pull(se)\nexp(log(or) + c(-1,1) * qnorm(0.975) * se)\n#&gt; [1] 0.662 0.996\n\nNote that 1 is not included in the confidence interval which must mean that the p-value is smaller than 0.05. We can confirm this using:\n\n2*(1 - pnorm(abs(log(or)), 0, se))\n#&gt; [1] 0.0454\n\n\n\n\n\n\n\nNote that the p-values obtained with chisq.test, glm and this simple approximation are all slightly different. This is because these are both based on different approximation approaches."
  },
  {
    "objectID": "linear-models/association-tests.html#large-samples-small-p-values",
    "href": "linear-models/association-tests.html#large-samples-small-p-values",
    "title": "17  Association tests",
    "section": "\n17.6 Large samples, small p-values",
    "text": "17.6 Large samples, small p-values\nAs mentioned earlier, reporting only p-values is not an appropriate way to report the results of data analysis. In scientific journals, for example, some studies seem to overemphasize p-values. Some of these studies have large sample sizes and report impressively small p-values. Yet when one looks closely at the results, we realize odds ratios are quite modest: barely bigger than 1. In this case the difference may not be practically significant or scientifically significant.\nNote that the relationship between odds ratio and p-value is not one-to-one. It depends on the sample size. So a very small p-value does not necessarily mean a very large odds ratio. Notice what happens to the p-value if we multiply our two-by-two table by 10, which does not change the odds ratio:\n\ntwo_by_two_x_10 &lt;- two_by_two |&gt; \n  select(-awarded) |&gt;\n  mutate(men = men*10, women = women*10) \nchisq.test(two_by_two_x_10)$p.value\n#&gt; [1] 2.63e-10\n\n:::{.callout-note title = “Small count correction”} Note that the log odds ratio is not defined if any of the cells of the two-by-two table is 0. This is because if \\(a\\), \\(b\\), \\(c\\), or \\(d\\) is 0, the \\(\\log(\\frac{ad}{bc})\\) is either the log of 0 or has a 0 in the denominator. For this situation, it is common practice to avoid 0s by adding 0.5 to each cell. This is referred to as the Haldane–Anscombe correction and has been shown, both in practice and theory, to work well. :::"
  },
  {
    "objectID": "linear-models/association-tests.html#exercises",
    "href": "linear-models/association-tests.html#exercises",
    "title": "17  Association tests",
    "section": "\n17.7 Exercises",
    "text": "17.7 Exercises\n1. A famous athlete has an impressive career, winning 70% of her 500 career matches. However, this athlete gets criticized because in important events, such as the Olympics, she has a losing record of 8 wins and 9 losses. Perform a Chi-square test to determine if this losing record can be simply due to chance as opposed to not performing well under pressure.\n2. Why did we use the Chi-square test instead of Fisher’s exact test in the previous exercise?\n\nIt actually does not matter, since they give the exact same p-value.\nFisher’s exact and the Chi-square are different names for the same test.\nBecause the sum of the rows and columns of the two-by-two table are not fixed so the hypergeometric distribution is not an appropriate assumption for the null hypothesis. For this reason, Fisher’s exact test is rarely applicable with observational data.\nBecause the Chi-square test runs faster.\n\n3. Compute the odds ratio of “losing under pressure” along with a confidence interval.\n4. Notice that the p-value is larger than 0.05 but the 95% confidence interval does not include 1. What explains this?\n\nWe made a mistake in our code.\nThese are based on t-statistics so the connection between p-value and confidence intervals does not apply.\nDifferent approximations are used for the p-value and the confidence interval calculation. If we had a larger sample size the match would be better.\nWe should use the Fisher exact test to get confidence intervals.\n\n5. Multiply the two-by-two table by 2 and see if the p-value and confidence retrieval are a better match.\n6. During the 2016 US presidential election, then candidate Donald J. Trump used his twitter account as a way to communicate with potential voters. Todd Vaziri hypothesized that “Every non-hyperbolic tweet is from iPhone (his staff). Every hyperbolic tweet is from Android (from him).” We will test this hypothesis using association tests. The following code coverts the tweets to a table with the counts for several sentiments from each source (Android or iPhone):\n\nlibrary(tidyverse)\nlibrary(dslabs)\nlibrary(tidytext)\n\nlinks &lt;- \"https://t.co/[A-Za-z\\\\d]+|&amp;\"\nnrc &lt;- get_sentiments(\"nrc\") |&gt; select(word, sentiment)\nandroid_iphone &lt;- trump_tweets |&gt; \n  extract(source, \"source\", \"Twitter for (.*)\") |&gt;\n  filter(source %in% c(\"Android\", \"iPhone\") &\n           created_at &gt;= ymd(\"2015-06-17\") & \n           created_at &lt; ymd(\"2016-11-08\")) |&gt;\n  filter(!is_retweet) |&gt;\n  arrange(created_at) |&gt; \n  mutate(text = str_replace_all(text, links, \"\"))  |&gt;\n  unnest_tokens(word, text) |&gt;\n  filter(!word %in% stop_words$word &\n           !str_detect(word, \"^\\\\d+$\")) |&gt;\n  mutate(word = str_replace(word, \"^'\", \"\")) |&gt;\n  filter(!word %in% stop_words$word)\n\nsentiment_counts &lt;- android_iphone |&gt;\n  left_join(nrc, by = \"word\", relationship = \"many-to-many\") |&gt;\n  count(source, sentiment) |&gt;\n  pivot_wider(names_from = \"source\", values_from = \"n\") |&gt;\n  mutate(sentiment = replace_na(sentiment, replace = \"none\"))\n\nCompute an odds ratio comparing Android to iPhone for each sentiment and add it to the table.\n7. Compute a 95% confidence interval for each odds ratio.\n8. Generate a plot showing the estimated odds ratios along with their confidence intervals.\n9. Test the null hypothesis that there is no difference between tweets from Android and iPhone and report the sentiments with p-values less than 0.05 and more likely to come from Android.\n10. For each sentiment, find the words assigned to that sentiment, keep words that appear at least 25 times, compute the odd ratio for each, and show a barplot for those with odds ratio larger than 2 or smaller than 1/2."
  },
  {
    "objectID": "linear-models/association-tests.html#footnotes",
    "href": "linear-models/association-tests.html#footnotes",
    "title": "17  Association tests",
    "section": "",
    "text": "http://www.pnas.org/content/112/40/12349.abstract↩︎\nhttps://en.wikipedia.org/wiki/Ronald_Fisher↩︎"
  },
  {
    "objectID": "linear-models/association-not-causation.html#spurious-correlation",
    "href": "linear-models/association-not-causation.html#spurious-correlation",
    "title": "\n18  Association is not causation\n",
    "section": "\n18.1 Spurious correlation",
    "text": "18.1 Spurious correlation\nThe following comical example underscores that correlation is not causation. It shows a very strong correlation between divorce rates and margarine consumption.\n\n\n\n\n\n\n\n\nDoes this mean that margarine causes divorces? Or do divorces cause people to eat more margarine? Of course the answer to both these questions is no. This is just an example of what we call a spurious correlation.\nYou can see many more absurd examples on the Spurious Correlations website1.\nThe cases presented in the spurious correlation site are all instances of what is generally called data dredging, data fishing, or data snooping. It’s basically a form of what in the US they call cherry picking. An example of data dredging would be if you look through many results produced by a random process and pick the one that shows a relationship that supports a theory you want to defend.\nA Monte Carlo simulation can be used to show how data dredging can result in finding high correlations among uncorrelated variables. We will save the results of our simulation into a tibble:\n\nlibrary(tidyverse)\nN &lt;- 25\ng &lt;- 1000000\nsim_data &lt;- tibble(group = rep(1:g, each = N), \n                   x = rnorm(N*g), \n                   y = rnorm(N*g))\n\nThe first column denotes group. We created groups and for each one we generated a pair of independent vectors, \\(X\\) and \\(Y\\), with 25 observations each, stored in the second and third columns. Because we constructed the simulation, we know that \\(X\\) and \\(Y\\) are not correlated.\nNext, we compute the correlation between X and Y for each group and look at the max:\n\nres &lt;- sim_data |&gt; \n  group_by(group) |&gt; \n  summarize(r = cor(x, y)) |&gt; \n  arrange(desc(r))\nres\n#&gt; # A tibble: 1,000,000 × 2\n#&gt;    group     r\n#&gt;    &lt;int&gt; &lt;dbl&gt;\n#&gt; 1 606777 0.789\n#&gt; 2 949026 0.781\n#&gt; 3 752659 0.774\n#&gt; 4 815223 0.773\n#&gt; 5 890876 0.768\n#&gt; # ℹ 999,995 more rows\n\nWe see a maximum correlation of 0.7892932 and if you just plot the data from the group achieving this correlation, it shows a convincing plot that \\(X\\) and \\(Y\\) are in fact correlated:\n\nsim_data |&gt; filter(group == res$group[which.max(res$r)]) |&gt;\n  ggplot(aes(x, y)) +\n  geom_point() + \n  geom_smooth(method = \"lm\")\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nRemember that the correlation summary is a random variable. Here is the distribution generated by the Monte Carlo simulation:\n\nres |&gt; ggplot(aes(x=r)) + geom_histogram(binwidth = 0.1, color = \"black\")\n\n\n\n\n\n\n\nIt’s just a mathematical fact that if we observe random correlations that are expected to be 0, but have a standard error of 0.2041507, the largest one will be close to 1.\nIf we performed regression on this group and interpreted the p-value, we would incorrectly claim this was a statistically significant relation:\n\nlibrary(broom)\nsim_data |&gt; \n  filter(group == res$group[which.max(res$r)]) |&gt;\n  summarize(tidy(lm(y ~ x))) |&gt; \n  filter(term == \"x\")\n#&gt; # A tibble: 1 × 5\n#&gt;   term  estimate std.error statistic    p.value\n#&gt;   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 x        0.690     0.112      6.16 0.00000274\n\nThis particular form of data dredging is referred to as p-hacking. P-hacking is a topic of much discussion because it is a problem in scientific publications. Because publishers tend to reward statistically significant results over negative results, there is an incentive to report significant results. In epidemiology and the social sciences, for example, researchers may look for associations between an adverse outcome and several exposures and report only the one exposure that resulted in a small p-value. Furthermore, they might try fitting several different models to account for confounding and pick the one that yields the smallest p-value. In experimental disciplines, an experiment might be repeated more than once, yet only the results of the one experiment with a small p-value reported. This does not necessarily happen due to unethical behavior, but rather as a result of statistical ignorance or wishful thinking. In advanced statistics courses, you can learn methods to adjust for these multiple comparisons."
  },
  {
    "objectID": "linear-models/association-not-causation.html#outliers",
    "href": "linear-models/association-not-causation.html#outliers",
    "title": "\n18  Association is not causation\n",
    "section": "\n18.2 Outliers",
    "text": "18.2 Outliers\nSuppose we take measurements from two independent outcomes, \\(X\\) and \\(Y\\), and we standardize the measurements. However, imagine we make a mistake and forget to standardize entry 23. We can simulate such data using:\n\nset.seed(1985)\nx &lt;- rnorm(100,100,1)\ny &lt;- rnorm(100,84,1)\nx[-23] &lt;- scale(x[-23])\ny[-23] &lt;- scale(y[-23])\n\nThe data look like this:\n\nqplot(x, y)\n#&gt; Warning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\nNot surprisingly, the correlation is very high:\n\ncor(x,y)\n#&gt; [1] 0.988\n\nBut this is driven by the one outlier. If we remove this outlier, the correlation is greatly reduced to almost 0, which is what it should be:\n\ncor(x[-23], y[-23])\n#&gt; [1] -0.0442\n\nThere is an alternative to the sample correlation for estimating the population correlation that is robust to outliers. It is called Spearman correlation. The idea is simple: compute the correlation on the ranks of the values. Here is a plot of the ranks plotted against each other:\n\nqplot(rank(x), rank(y))\n\n\n\n\n\n\n\nThe outlier is no longer associated with a very large value and the correlation comes way down:\n\ncor(rank(x), rank(y))\n#&gt; [1] 0.00251\n\nSpearman correlation can also be calculated like this:\n\ncor(x, y, method = \"spearman\")\n#&gt; [1] 0.00251\n\nThere are also methods for robust fitting of linear models which you can learn about in, for instance, this book: Robust Statistics: Edition 2 by Peter J. Huber & Elvezio M. Ronchetti."
  },
  {
    "objectID": "linear-models/association-not-causation.html#reversing-cause-and-effect",
    "href": "linear-models/association-not-causation.html#reversing-cause-and-effect",
    "title": "\n18  Association is not causation\n",
    "section": "\n18.3 Reversing cause and effect",
    "text": "18.3 Reversing cause and effect\nAnother way association is confused with causation is when the cause and effect are reversed. An example of this is claiming that tutoring makes students perform worse because they test lower than peers that are not tutored. In this case, the tutoring is not causing the low test scores, but the other way around.\nA form of this claim actually made it into an op-ed in the New York Times titled Parental Involvement Is Overrated2. Consider this quote from the article:\n\n\nWhen we examined whether regular help with homework had a positive impact on children’s academic performance, we were quite startled by what we found. Regardless of a family’s social class, racial or ethnic background, or a child’s grade level, consistent homework help almost never improved test scores or grades… Even more surprising to us was that when parents regularly helped with homework, kids usually performed worse.\n\n\nA very likely possibility is that the children needing regular parental help, receive this help because they don’t perform well in school.\nWe can easily construct an example of cause and effect reversal using the father and son height data. If we fit the model:\n\\[X_i = \\beta_0 + \\beta_1 y_i + \\varepsilon_i, i=1, \\dots, N\\]\nto the father and son height data, with \\(X_i\\) the father height and \\(y_i\\) the son height, we do get a statistically significant result. We use the galton_heights dataset defined in Chapter Chapter 13:\n\ngalton_heights |&gt; summarize(tidy(lm(father ~ son)))\n#&gt; Warning: Returning more (or less) than 1 row per `summarise()` group was\n#&gt; deprecated in dplyr 1.1.0.\n#&gt; ℹ Please use `reframe()` instead.\n#&gt; ℹ When switching from `summarise()` to `reframe()`, remember that\n#&gt;   `reframe()` always returns an ungrouped data frame and adjust\n#&gt;   accordingly.\n#&gt; # A tibble: 2 × 5\n#&gt;   term        estimate std.error statistic  p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 (Intercept)   40.9      4.40        9.29 5.47e-17\n#&gt; 2 son            0.407    0.0636      6.40 1.36e- 9\n\nThe model fits the data very well. If we look at the mathematical formulation of the model above, it could easily be incorrectly interpreted so as to suggest that the son being tall caused the father to be tall. But given what we know about genetics and biology, we know it’s the other way around. The model is technically correct. The estimates and p-values were obtained correctly as well. What is wrong here is the interpretation."
  },
  {
    "objectID": "linear-models/association-not-causation.html#confounders",
    "href": "linear-models/association-not-causation.html#confounders",
    "title": "\n18  Association is not causation\n",
    "section": "\n18.4 Confounders",
    "text": "18.4 Confounders\nConfounders are perhaps the most common reason that leads to associations begin misinterpreted.\nIf \\(X\\) and \\(Y\\) are correlated, we call \\(Z\\) a confounder if changes in \\(Z\\) causes changes in both \\(X\\) and \\(Y\\). Earlier, when studying baseball data, we saw how Home Runs was a confounder that resulted in a higher correlation than expected when studying the relationship between Bases on Balls and Runs. In some cases, we can use linear models to account for confounders. However, this is not always the case.\nIncorrect interpretation due to confounders is ubiquitous in the lay press and they are often hard to detect. Here, we present a widely used example related to college admissions.\n\n18.4.1 Example: UC Berkeley admissions\nAdmission data from six U.C. Berkeley majors, from 1973, showed that more men were being admitted than women: 44% men were admitted compared to 30% women. PJ Bickel, EA Hammel, and JW O’Connell. Science (1975). We can load the data and compute a statistical test, which clearly rejects the hypothesis that gender and admission are independent:\n\ntwo_by_two &lt;- admissions |&gt; group_by(gender) |&gt; \n  summarize(total_admitted = round(sum(admitted / 100 * applicants)), \n            not_admitted = sum(applicants) - sum(total_admitted)) |&gt;\n  select(-gender) \n  \nchisq.test(two_by_two)$p.value\n#&gt; [1] 1.06e-21\n\nBut closer inspection shows a paradoxical result. Here are the percent admissions by major:\n\nadmissions |&gt; select(major, gender, admitted) |&gt;\n  pivot_wider(names_from = \"gender\", values_from = \"admitted\") |&gt;\n  mutate(women_minus_men = women - men)\n#&gt; # A tibble: 6 × 4\n#&gt;   major   men women women_minus_men\n#&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;\n#&gt; 1 A        62    82              20\n#&gt; 2 B        63    68               5\n#&gt; 3 C        37    34              -3\n#&gt; 4 D        33    35               2\n#&gt; 5 E        28    24              -4\n#&gt; # ℹ 1 more row\n\nFour out of the six majors favor women. More importantly, all the differences are much smaller than the 14.2 difference that we see when examining the totals.\nThe paradox is that analyzing the totals suggests a dependence between admission and gender, but when the data is grouped by major, this dependence seems to disappear. What’s going on? This actually can happen if an uncounted confounder is driving most of the variability.\nSo let’s define three variables: \\(X\\) is 1 for men and 0 for women, \\(Y\\) is 1 for those admitted and 0 otherwise, and \\(Z\\) quantifies the selectivity of the major. A gender bias claim would be based on the fact that \\(\\mbox{Pr}(Y=1 | X = x)\\) is higher for \\(x=1\\) than \\(x=0\\). However, \\(Z\\) is an important confounder to consider. Clearly \\(Z\\) is associated with \\(Y\\), as the more selective a major, the lower \\(\\mbox{Pr}(Y=1 | Z = z)\\). But is major selectivity \\(Z\\) associated with gender \\(X\\)?\nOne way to see this is to plot the total percent admitted to a major versus the percent of women that made up the applicants:\n\nadmissions |&gt; \n  group_by(major) |&gt; \n  summarize(major_selectivity = sum(admitted * applicants)/sum(applicants),\n            percent_women_applicants = sum(applicants * (gender==\"women\")) /\n                                             sum(applicants) * 100) |&gt;\n  ggplot(aes(major_selectivity, percent_women_applicants, label = major)) +\n  geom_text()\n\n\n\n\n\n\n\nThere seems to be association. The plot suggests that women were much more likely to apply to the two “hard” majors: gender and major’s selectivity are confounded. Compare, for example, major B and major E. Major E is much harder to enter than major B and over 60% of applicants to major E were women, while less than 30% of the applicants of major B were women.\n\n18.4.2 Confounding explained graphically\nThe following plot shows the number of applicants that were admitted and those that were not by:\n\n\n\n\n\n\n\n\nIt also breaks down the acceptances by major. This breakdown allows us to see that the majority of accepted men came from two majors: A and B. It also lets us see that few women applied to these majors.\n\n18.4.3 Average after stratifying\nIn this plot, we can see that if we condition or stratify by major, and then look at differences, we control for the confounder and this effect goes away:\n\nadmissions |&gt; \n  ggplot(aes(major, admitted, col = gender, size = applicants)) +\n  geom_point()\n\n\n\n\n\n\n\nNow we see that major by major, there is not much difference. The size of the dot represents the number of applicants, and explains the paradox: we see large red dots and small blue dots for the easiest majors, A and B.\nIf we average the difference by major, we find that the percent is actually 3.5% higher for women.\n\nadmissions |&gt;  group_by(gender) |&gt; summarize(average = mean(admitted))\n#&gt; # A tibble: 2 × 2\n#&gt;   gender average\n#&gt;   &lt;chr&gt;    &lt;dbl&gt;\n#&gt; 1 men       38.2\n#&gt; 2 women     41.7"
  },
  {
    "objectID": "linear-models/association-not-causation.html#simpsons-paradox",
    "href": "linear-models/association-not-causation.html#simpsons-paradox",
    "title": "\n18  Association is not causation\n",
    "section": "\n18.5 Simpson’s paradox",
    "text": "18.5 Simpson’s paradox\nThe case we have just covered is an example of Simpson’s paradox. It is called a paradox because we see the sign of the correlation flip when comparing the entire publication and specific strata. As an illustrative example, suppose you have three random variables \\(X\\), \\(Y\\), and \\(Z\\) and that we observe realizations of these. Here is a plot of simulated observations for \\(X\\) and \\(Y\\) along with the sample correlation:\n\n\n\n\n\n\n\n\nYou can see that \\(X\\) and \\(Y\\) are negatively correlated. However, once we stratify by \\(Z\\) (shown in different colors below) another pattern emerges:\n\n\n\n\n\n\n\n\nIt is really \\(Z\\) that is negatively correlated with \\(X\\). If we stratify by \\(Z\\), the \\(X\\) and \\(Y\\) are actually positively correlated as seen in the plot above."
  },
  {
    "objectID": "linear-models/association-not-causation.html#exercises",
    "href": "linear-models/association-not-causation.html#exercises",
    "title": "\n18  Association is not causation\n",
    "section": "\n18.6 Exercises",
    "text": "18.6 Exercises\nFor the next set of exercises, we examine the data from a 2014 PNAS paper3 that analyzed success rates from funding agencies in the Netherlands and concluded:\n\nOur results reveal gender bias favoring male applicants over female applicants in the prioritization of their “quality of researcher” (but not “quality of proposal”) evaluations and success rates, as well as in the language used in instructional and evaluation materials.\n\nA response4 was published a few months later titled No evidence that gender contributes to personal research funding success in The Netherlands: A reaction to Van der Lee and Ellemers which concluded:\n\nHowever, the overall gender effect borders on statistical significance, despite the large sample. Moreover, their conclusion could be a prime example of Simpson’s paradox; if a higher percentage of women apply for grants in more competitive scientific disciplines (i.e., with low application success rates for both men and women), then an analysis across all disciplines could incorrectly show “evidence” of gender inequality.\n\nWho is right here? The original paper or the response? Here, you will examine the data and come to your own conclusion.\n1. The main evidence for the conclusion of the original paper comes down to a comparison of the percentages. Table S1 in the paper includes the information we need:\n\nlibrary(dslabs)\nresearch_funding_rates\n\nConstruct the two-by-two table used for the conclusion about differences in awards by gender.\n2. Compute the difference in percentage from the two-by-two table.\n3. In the previous exercise, we noticed that the success rate is lower for women. But is it significant? Compute a p-value using a Chi-square test.\n4. We see that the p-value is about 0.05. So there appears to be some evidence of an association. But can we infer causation here? Is gender bias causing this observed difference? The response to the original paper claims that what we see here is similar to the UC Berkeley admissions example. Specifically they state that this “could be a prime example of Simpson’s paradox; if a higher percentage of women apply for grants in more competitive scientific disciplines, then an analysis across all disciplines could incorrectly show ‘evidence’ of gender inequality.” To settle this dispute, create a dataset with number of applications, awards, and success rate for each gender. Re-order the disciplines by their overall success rate. Hint: use the reorder function to re-order the disciplines in a first step, then use pivot_longer, separate, and pivot_wider to create the desired table.\n5. To check if this is a case of Simpson’s paradox, plot the success rates versus disciplines, which have been ordered by overall success, with colors to denote the genders and size to denote the number of applications.\n6. We definitely do not see the same level of confounding as in the UC Berkeley example. It is hard to say there is a confounder here. However, we do see that, based on the observed rates, some fields favor men and others favor women and we do see that the two fields with the largest difference favoring men are also the fields with the most applications. But, unlike the UC Berkeley example, women are not more likely to apply for the harder subjects. So perhaps some of the selection committees are biased and others are not.\nBut, before we conclude this, we must check if these differences are any different than what we get by chance. Are any of the differences seen above statistically significant? Keep in mind that even when there is no bias, we will see differences due to random variability in the review process as well as random variability across candidates. Perform a Chi-square test for each discipline. Hint: define a function that receives the total of a two-by-two table and returns a data frame with the p-value. Use the 0.5 correction. Then use the summarize function.\n7. For the medical sciences, there appears to be a statistically significant difference. But is this a spurious correlation? We performed 9 tests. Reporting only the one case with a p-value less than 0.05 might be considered an example of cherry picking. Repeat the exercise above, but instead of a p-value, compute a log odds ratio divided by their standard error. Then use qq-plot to see how much these log odds ratios deviate from the normal distribution we would expect: a standard normal distribution."
  },
  {
    "objectID": "linear-models/association-not-causation.html#footnotes",
    "href": "linear-models/association-not-causation.html#footnotes",
    "title": "\n18  Association is not causation\n",
    "section": "",
    "text": "http://tylervigen.com/spurious-correlations↩︎\nhttps://opinionator.blogs.nytimes.com/2014/04/12/parental-involvement-is-overrated↩︎\nhttp://www.pnas.org/content/112/40/12349.abstract↩︎\nhttp://www.pnas.org/content/112/51/E7036.extract↩︎"
  },
  {
    "objectID": "highdim/intro-highdim.html",
    "href": "highdim/intro-highdim.html",
    "title": "High dimensional data",
    "section": "",
    "text": "There is a variety of computational techniques and statistical concepts that are useful for analysis of datasets for which each observation is associated with a large number of numerical variables. In this chapter we provide a basic introduction to these techniques and concepts by describing matrix operations in R, dimension reduction, regularization, and matrix factorization. Handwritten digits data and movie recommendation systems serve as motivating examples.\nA task that serves as motivation for this part of the book is quantifying the similarity between any two observations. For example, we might want to know how much two handwritten digits look like each other. However, note that each observations is associated with \\(28 \\times 28 = 784\\) pixels so we can’t simply use subtraction as we would do if our data was one dimensional. Instead, we will define observations as points in a high-dimensional space and mathematically define a distance. Many machine learning techniques, discussed in the next part of the book, require this calculation.\nAdditionally, this part of the book discusses dimension reduction. Here we search of data summaries that result in more manageable lower dimension versions of the data, but preserve most or all the information we need. Here too we can use distance between observations as specific challenge: we will reduce the dimensions summarize the data into lower dimensions, but in a way that preserves the distance between any two observations. We use linear algebra as a mathematical foundation for all the techniques presented here."
  },
  {
    "objectID": "highdim/matrices-in-R.html#sec-mnist",
    "href": "highdim/matrices-in-R.html#sec-mnist",
    "title": "\n19  Matrices in R\n",
    "section": "\n19.1 Case study: MNIST",
    "text": "19.1 Case study: MNIST\nAn example comes from handwritten digits. The first step in handling mail received in the post office is sorting letters by zip code:\n\nIn the Machine Learning part of this book we will describe how we can build computer algorithms to read handwritten digits, which robots then use to sort the letters. To build these algorithms, we first need to collect data, which in this case is a high-dimensional dataset.\nThe MNIST dataset was generated by digitizing thousands of handwritten digits, already read and annotated by humans1. Below are three images of written digits.\n\n\n\n\n\n\n\n\nThe images are converted into \\(28 \\times 28 = 784\\) pixels and, for each pixel, we obtain a grey scale intensity between 0 (white) and 255 (black). The following plot shows the individual features for each image:\n\n\n\n\n\n\n\n\nFor each digitized image, indexed by \\(i\\), we are provided 784 variables and a categorical outcome, or label, representing which digit among \\(0, 1, 2, 3, 4, 5, 6, 7 , 8,\\) and \\(9\\) the image is representing. Let’s load the data using the dslabs package:\n\nlibrary(tidyverse)\nlibrary(dslabs)\nmnist &lt;- read_mnist()\n\nIn these cases, the pixel intensities are saved in a matrix:\n\nclass(mnist$train$images)\n#&gt; [1] \"matrix\" \"array\"\n\nThe labels associated with each image are included in a vector:\n\ntable(mnist$train$labels)\n#&gt; \n#&gt;    0    1    2    3    4    5    6    7    8    9 \n#&gt; 5923 6742 5958 6131 5842 5421 5918 6265 5851 5949\n\n\n19.1.1 Motivating tasks\nTo motivate the use of matrices in R, we will pose six tasks related to the handwritten digits data and then show the fast and simple code that solves them.\n1. Visualize the original image. The pixel intensities are provided as rows in a matrix. We will show how to conver these to a matrix that we can visualize.\n2. Do some digits require more ink to write than others? We will study the distribution of the total pixel darkness and how it varies by digits.\n3. Are some pixels uninformative? We will study the variation of each pixel across digits and remove predictors (columns) associated with pixels that don’t change much and thus can’t provide much information for classification.\n4. Can we remove smudges? We will first, look at the distribution of all pixel values. Then we will use this to pick a cutoff to define unwritten space. Then, set anything below that cutoff to 0.\n5. Binarize the data. First, we will look at the distribution of all pixel values. We will then use this to pick a cutoff to distinguish between writing and no writing. Then, we will convert all entries into either 1 or 0.\n6. Standardize the digits. We will scale each of the predictors in each entry to have the same average and standard deviation.\nTo complete these, we will have to perform mathematical operations involving several variables. The tidyverse or data.table are not developed to perform these types of mathematical operations. For this task, it is convenient to use matrices.\nTo simplify the code below, we will rename these x and y respectively:\n\nx &lt;- mnist$train$images\ny &lt;- mnist$train$labels"
  },
  {
    "objectID": "highdim/matrices-in-R.html#sec-matrix-notation",
    "href": "highdim/matrices-in-R.html#sec-matrix-notation",
    "title": "\n19  Matrices in R\n",
    "section": "\n19.5 Mathematical notation",
    "text": "19.5 Mathematical notation\nMatrices are usually represented with bold upper case letters:\n\\[\n\\mathbf{X} =\n\\begin{bmatrix}\nx_{1,1}&x_{1,2}&\\dots & x_{1,p}\\\\\nx_{2,1}&x_{2,2}&\\dots & x_{2,p}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n,1}&x_{n,2}&\\dots&x_{n,p}\\\\\n\\end{bmatrix}\n\\]\nwith \\(x_{i,j}\\) representing the \\(j\\)-the feature for the \\(i\\)-th observation.\nWe denote vectors with lower case bold letters and represent them as one column matrices, often referred to as column vectors. R follows this convention when converting a vector to a matrix:\n\ndim(matrix(x[300,]))\n#&gt; [1] 784   1\n\nHowever, column vectors should not be confused with the columns of the matrix. They have this name simply because they have one column.\nMathematical descriptions of machine learning often make reference to vectors representing the \\(p\\) features:\n\\[\n\\mathbf{x} =\n\\begin{bmatrix}\nx_1\\\\\\\nx_2\\\\\\\n\\vdots\\\\\\\nx_p\n\\end{bmatrix}\n\\]\nTo distinguish between features associated with the observations \\(i=1,\\dots,n\\) we add an index:\n\\[\n\\mathbf{x}_i = \\begin{bmatrix}\nx_{i,1}\\\\\nx_{i,2}\\\\\n\\vdots\\\\\nx_{i,p}\n\\end{bmatrix}\n\\]\n\nBold lower case letter are also commonly used to represent matrix columns rather than rows. This can be confusing because \\(\\mathbf{x}_1\\) can represent either the first row or the first column of \\(\\mathbf{X}\\). One way to distinguish is using notation similar to computer code: using the colon \\(:\\) to represent all. So \\(\\mathbf{X}_{1,:}\\) is a row, the first row and all the columns, and \\(\\mathbf{X}_{:,1}\\) is a column, the first column and all the rows. Another approach is to distinguish by the letter used to index, with \\(i\\) used for rows and \\(j\\) used for columns. So \\(\\mathbf{x}_i\\) is the \\(i\\)th row and \\(\\mathbf{x}_j\\) is the \\(j\\)th column. With this approach it is important to clarify which dimension, row or column, is being represented. Further confusion can arise because, as discussed, it is common to represent all vectors as 1 column matrices, including the rows of a matrix."
  },
  {
    "objectID": "highdim/matrices-in-R.html#converting-vectors-to-a-matrices",
    "href": "highdim/matrices-in-R.html#converting-vectors-to-a-matrices",
    "title": "\n19  Matrices in R\n",
    "section": "\n19.3 Converting vectors to a matrices",
    "text": "19.3 Converting vectors to a matrices\nVectors can be thought of as \\(n\\times 1\\) matrices. However, in R, a vector does not have dimensions:\n\ndim(x_1)\n#&gt; NULL\n\nVectors are not matrices in R. However, we can easily convert then to a matrix:\n\ndim(matrix(x_1))\n#&gt; [1] 5 1\n\nIt is also possible to change the dimensions of the resulting matrix. To see an example of how can this be useful, consider wanting to visualize the the rows pixel intensities in their original \\(28\\times28\\) grid.\nThe filling of matrces with values from a vector happens by column: the first column is filled first, then the second and so on. This example helps illustrate:\n\nmy_vector &lt;- 1:15\nmat &lt;- matrix(my_vector, 5, 3)\nmat\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    1    6   11\n#&gt; [2,]    2    7   12\n#&gt; [3,]    3    8   13\n#&gt; [4,]    4    9   14\n#&gt; [5,]    5   10   15\n\nWe can fill by row by using the byrow argument:\n\nmat_t &lt;- matrix(my_vector, 3, 5, byrow = TRUE)\nmat_t\n#&gt;      [,1] [,2] [,3] [,4] [,5]\n#&gt; [1,]    1    2    3    4    5\n#&gt; [2,]    6    7    8    9   10\n#&gt; [3,]   11   12   13   14   15"
  },
  {
    "objectID": "highdim/matrices-in-R.html#motivating-questions",
    "href": "highdim/matrices-in-R.html#motivating-questions",
    "title": "\n19  Matrices in R\n",
    "section": "\n19.4 Motivating questions",
    "text": "19.4 Motivating questions\nTo motivate the use of matrices in R, we will pose five questions/challenges related to the handwritten digits data:\n1. Do some digits require more ink to write than others? We will study the distribution of the total pixel darkness and how it varies by digits.\n2. Are some pixels uninformative? We will study the variation of each pixel across digits and remove predictors (columns) associated with pixels that don’t change much and thus can’t provide much information for classification.\n3. Can we remove smudges? We will first, look at the distribution of all pixel values. Then we will use this to pick a cutoff to define unwritten space. Then, set anything below that cutoff to 0.\n4. Binarize the data. First, we will look at the distribution of all pixel values. We will then use this to pick a cutoff to distinguish between writing and no writing. Then, we will convert all entries into either 1 or 0.\n5. Standardize the digits. We will scale each of the predictors in each entry to have the same average and standard deviation.\nTo complete these, we will have to perform mathematical operations involving several variables. The tidyverse or data.table are not developed to perform these types of mathematical operations. For this task, it is convenient to use matrices.\n\n\n\n\n\n\nThe matrix function recycles values in the vector without warning if the product of columns and rows does not match the length of the vector:\n\nmatrix(1:3, 2, 5)\n#&gt; Warning in matrix(1:3, 2, 5): data length [3] is not a sub-multiple or\n#&gt; multiple of the number of rows [2]\n#&gt;      [,1] [,2] [,3] [,4] [,5]\n#&gt; [1,]    1    3    2    1    3\n#&gt; [2,]    2    1    3    2    1\n\n\n\n\nTo put the pixel intensities of our, say, 3rd entry, which is a 4 into grid, we can use:\n\ngrid &lt;- matrix(x[3,], 28, 28)\n\nTo confirm that in fact we have done this correctly, we can use the function image, which shows an image of its third argument. The top of this plot is pixel 1, which is shown at the bottom so the image is flipped. To code below includes code showing how to flip it back:\n\nimage(1:28, 1:28, grid)\nimage(1:28, 1:28, grid[, 28:1])"
  },
  {
    "objectID": "highdim/matrices-in-R.html#row-and-column-summaries",
    "href": "highdim/matrices-in-R.html#row-and-column-summaries",
    "title": "\n19  Matrices in R\n",
    "section": "\n19.7 Row and column summaries",
    "text": "19.7 Row and column summaries\nA common operation with matrices is to apply the same function to each row or to each column. For example, we may want to compute row averages and standard deviations. The apply function lets you do this. The first argument is the matrix, the second is the dimension, 1 for rows, 2 for columns, and the third is the function to be applied.\nSo, for example, to compute the averages and standard deviations of each row we write:\n\navgs &lt;- apply(x, 1, mean)\nsds &lt;- apply(x, 1, sd)\n\nTo compute these for the columns we simply change the 1 to a 2:\n\navgs &lt;- apply(x, 1, mean)\nsds &lt;- apply(x, 1, sd)\n\nBecause these operations are so common, special functions are available to perform them. So, for example, the functions rowMeans computes the average of each row\n\navg &lt;- rowMeans(x)\n\nand the matrixStats function rowSds computes the standard deviations for each row:\n\nlibrary(matrixStats)\nsds &lt;- rowSds(x)\n\nThe functions colMeans, and colSds provide the version for columns. For more fast implementations look at the functions available in matrixStats.\nTask 2: Do some digits require more ink to write than others?\nFor the second task, related to total pixel darkness, we want to see the average use of ink plotted against digit. We have already computed this average and can generate a boxplot to answer the question:\n\navg &lt;- rowMeans(x)\nboxplot(avg ~ y)\n\n\n\n\n\n\n\nFrom this plot we see that, not surprisingly, 1s use less ink than the other digits."
  },
  {
    "objectID": "highdim/matrices-in-R.html#apply",
    "href": "highdim/matrices-in-R.html#apply",
    "title": "\n19  Matrices in R\n",
    "section": "\n19.6 apply\n",
    "text": "19.6 apply\n\nThe functions just described are performing an operation similar to what sapply and the purrr function map do: apply the same function to a part of your object. In this case, the function is applied to either each row or each column. The apply function lets you apply any function, not just sum or mean, to a matrix. The first argument is the matrix, the second is the dimension, 1 for rows, 2 for columns, and the third is the function. So, for example, rowMeans can be written as:\n\navgs &lt;- apply(x, 1, mean)\n\nBut notice that just like with sapply and map, we can perform any function. So if we wanted the standard deviation for each column, we could write:\n\nsds &lt;- apply(x, 2, sd)\n\nThe trade off for this flexibility is that these operations are not as fast as dedicated functions such as rowMeans."
  },
  {
    "objectID": "highdim/matrices-in-R.html#filtering-columns-based-on-summaries",
    "href": "highdim/matrices-in-R.html#filtering-columns-based-on-summaries",
    "title": "\n19  Matrices in R\n",
    "section": "\n19.7 Filtering columns based on summaries",
    "text": "19.7 Filtering columns based on summaries\nWe now turn to task 2: studying the variation of each pixel and removing columns associated with pixels that don’t change much and thus do not inform the classification. Although a simplistic approach, we will quantify the variation of each pixel with its standard deviation across all entries. Since each column represents a pixel, we use the colSds function from the matrixStats package:\n\nlibrary(matrixStats)\n#&gt; \n#&gt; Attaching package: 'matrixStats'\n#&gt; The following object is masked from 'package:dplyr':\n#&gt; \n#&gt;     count\nsds &lt;- colSds(x)\n\nA quick look at the distribution of these values shows that some pixels have very low entry to entry variability:\n\n\n\n\n\n\n\n\n\nhist(sds, breaks = 30, main = \"SDs\")\n\nThis makes sense since we don’t write in some parts of the box. Here is the variance plotted by location:\n\nimage(1:28, 1:28, matrix(sds, 28, 28)[, 28:1])\n\n\n\n\n\n\n\n\n\nWe see that there is little variation in the corners.\nWe could remove features that have no variation since these can’t help us predict. We can extract columns from matrices using the following code:\n\nx[ ,c(351,352)]\n\nand rows like this:\n\nx[c(2,3),]\n\nWe can also use logical indexes to determine which columns or rows to keep. So if we wanted to remove uninformative predictors from our matrix, we could write this one line of code:\n\nnew_x &lt;- x[ ,colSds(x) &gt; 60]\ndim(new_x)\n#&gt; [1] 300 316\n\nOnly the columns for which the standard deviation is above 60 are kept, which removes over half the predictors.\nHere we add an important warning related to subsetting matrices: if you select one column or one row, the result is no longer a matrix but a vector.\n\nclass(x[, 1])\n#&gt; [1] \"integer\"\ndim(x[1, ])\n#&gt; NULL\n\nHowever, we can preserve the matrix class by using the argument drop=FALSE:\n\nclass(x[, 1, drop = FALSE])\n#&gt; [1] \"matrix\" \"array\"\ndim(x[, 1, drop = FALSE])\n#&gt; [1] 300   1"
  },
  {
    "objectID": "highdim/matrices-in-R.html#indexing-with-matrices",
    "href": "highdim/matrices-in-R.html#indexing-with-matrices",
    "title": "\n19  Matrices in R\n",
    "section": "\n19.9 Indexing with matrices",
    "text": "19.9 Indexing with matrices\nA operation that facilitates efficient coding is that we can change entries of a matrix based on conditionals applied to that same matrix. Here is a simple example:\nTo see what this does, we look at a smaller matrix:\n\nmat &lt;- matrix(1:15, 3, 5)\nmat[mat &gt; 6 & mat &lt; 12] &lt;- 0\nmat\n#&gt;      [,1] [,2] [,3] [,4] [,5]\n#&gt; [1,]    1    4    0    0   13\n#&gt; [2,]    2    5    0    0   14\n#&gt; [3,]    3    6    0   12   15\n\nA useful application of this approach is that we can change all the NA entries of a matrix to something else:\n\nx[!is.na(x)] &lt;- 0\n\n\n19.9.1 Task 4: Can we remove smudges?\nA histogram of all our predictor data:\n\n\n\n\n\n\n\n\n\nhist(as.vector(x), breaks = 30, main = \"Pixel intensities\")\n\nshows a clear dichotomy which is explained as parts of the image with ink and parts without. If we think that values below, say, 50 are smudges, we can quickly make them zero using:\n\nnew_x &lt;- x\nnew_x[new_x &lt; 50] &lt;- 0\n\nTask 5: Binarizing the data\nThe histogram above seems to suggest that this data is mostly binary. A pixel either has ink or does not. Using what we have learned, we can binarize the data using just matrix operations:\n\nbin_x &lt;- x\nbin_x[bin_x &lt; 255/2] &lt;- 0 \nbin_x[bin_x &gt; 255/2] &lt;- 1\n\nWe can also convert to a matrix of logicals and then coerce to numbers like this:\n\nbin_X &lt;- (x &gt; 255/2)*1"
  },
  {
    "objectID": "highdim/matrices-in-R.html#binarizing-the-data",
    "href": "highdim/matrices-in-R.html#binarizing-the-data",
    "title": "\n19  Matrices in R\n",
    "section": "\n19.9 Binarizing the data",
    "text": "19.9 Binarizing the data\nThe histogram above seems to suggest that this data is mostly binary. A pixel either has ink or does not. Using what we have learned, we can binarize the data using just matrix operations:\n\nbin_x &lt;- x\nbin_x[bin_x &lt; 255/2] &lt;- 0 \nbin_x[bin_x &gt; 255/2] &lt;- 1\n\nWe can also convert to a matrix of logicals and then coerce to numbers like this:\n\nbin_X &lt;- (x &gt; 255/2)*1"
  },
  {
    "objectID": "highdim/matrices-in-R.html#vectorization-for-matrices",
    "href": "highdim/matrices-in-R.html#vectorization-for-matrices",
    "title": "\n19  Matrices in R\n",
    "section": "\n19.10 Vectorization for matrices",
    "text": "19.10 Vectorization for matrices\nIn R, if we subtract a vector from a matrix, the first element of the vector is subtracted from the first row, the second element from the second row, and so on. Using mathematical notation, we would write it as follows:\n\\[\n\\begin{bmatrix}\n  X_{1,1}&\\dots & X_{1,p} \\\\\n  X_{2,1}&\\dots & X_{2,p} \\\\\n   & \\vdots & \\\\\n  X_{n,1}&\\dots & X_{n,p}\n  \\end{bmatrix}\n-\n\\begin{bmatrix}\na_1\\\\\\\na_2\\\\\\\n\\vdots\\\\\\\na_n\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n  X_{1,1}-a_1&\\dots & X_{1,p} -a_1\\\\\n  X_{2,1}-a_2&\\dots & X_{2,p} -a_2\\\\\n   & \\vdots & \\\\\n  X_{n,1}-a_n&\\dots & X_{n,p} -a_n\n  \\end{bmatrix}\n\\]\nThe same holds true for other arithmetic operations.\nThe function sweep facilitates this type of operation. It works similarly to apply. It takes each entry of a vector and applies an arithmetic operation to the corresponding row. Subtraction is the default artihmetic operation. So, for example, to center each row around the avarage we can use:\n\nsweep(x, 1, rowMeans(x))\n\nTask 6: Standardize the digits\nThe way R vectorizes arithmetic opearions implies that we can scale each row of a matrix like this:\n\n(x - rowMeans(x))/rowSds(x)\n\nIf you want to scale each column, be careful since this approach does not work for columns. For columns we can sweep:\n\nx_mean_0 &lt;- sweep(x, 2, colMeans(x))\n\nTo divide by the standard deviation, we change the default arithmetic operation to division as follows:\n\nx_standardized &lt;- sweep(x_mean_0, 2, colSds(x), FUN = \"/\")\n\nIn R, if you add, subtract, multiple or divide two matrices, the operation is done elementwise. For example, if two matrices are stored in x and y, then\n\nx*y\n\ndoes not result in matrix multiplication. Instead, the entry in row \\(i\\) and column \\(j\\) of this product is the product of the entryin row \\(i\\) and column \\(j\\) of x and y, respectively."
  },
  {
    "objectID": "highdim/matrices-in-R.html#exercises",
    "href": "highdim/matrices-in-R.html#exercises",
    "title": "\n19  Matrices in R\n",
    "section": "\n19.11 Exercises",
    "text": "19.11 Exercises\n1. Create a 100 by 10 matrix of randomly generated normal numbers. Put the result in x.\n2. Apply the three R functions that give you the dimension of x, the number of rows of x, and the number of columns of x, respectively.\n3. Add the scalar 1 to row 1, the scalar 2 to row 2, and so on, to the matrix x.\n4. Add the scalar 1 to column 1, the scalar 2 to column 2, and so on, to the matrix x. Hint: use sweep with FUN = \"+\".\n5. Compute the average of each row of x.\n6. Compute the average of each column of x.\n7. For each digit in the MNIST training data, compute the proportion of pixels that are in a grey area, defined as values between 50 and 205. Make boxplot by digit class. Hint: use logical operators and rowMeans."
  },
  {
    "objectID": "highdim/matrices-in-R.html#footnotes",
    "href": "highdim/matrices-in-R.html#footnotes",
    "title": "\n19  Matrices in R\n",
    "section": "",
    "text": "http://yann.lecun.com/exdb/mnist/↩︎"
  },
  {
    "objectID": "highdim/linear-algebra.html#transpose-of-a-matrix",
    "href": "highdim/linear-algebra.html#transpose-of-a-matrix",
    "title": "20  Applied Linear Algebra",
    "section": "\n20.1 Transpose of a matrix",
    "text": "20.1 Transpose of a matrix\nA common operation when working with matrices is the transpose. We will see examples in later sections. This operation simply converts the rows of a matrix into columns. We use the symbols \\(\\top\\) or \\('\\) next to the bold upper case letter to denote the transpose:\n\\[\n\\mathbf{X}^\\top =\n\\begin{pmatrix}\n  x_{1,1}&x_{2,1}&\\dots & x_{n,1} \\\\\n  x_{1,2}&x_{2,2}&\\dots & x_{n,2} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots & \\\\\n  x_{1,p}&x_{2,p}&\\dots & x_{n,p}\n  \\end{pmatrix}\n\\] If we are writing out a column, such as \\(\\mathbf{x}_1\\) defined above, in a sentence we often use the notation: \\(\\mathbf{x}_1 = ( x_{1,1}, \\dots x_{n,1})^\\top\\) to avoid wasting vertical space in the text.\nIn R we compute the transpose using the function t\n\ndim(x)\n#&gt; [1] 300 784\ndim(t(x))\n#&gt; [1] 784 300"
  },
  {
    "objectID": "highdim/linear-algebra.html#matrix-multiplication",
    "href": "highdim/linear-algebra.html#matrix-multiplication",
    "title": "20  Applied Linear Algebra",
    "section": "\n20.1 Matrix multiplication",
    "text": "20.1 Matrix multiplication\nA commonly used operation in data analysis is matrix multiplication. Here we define and motivate the operation.\nLinear algebra was born from mathematicians developing systematic ways to solve systems of linear equations, for example\n\\[\n\\begin{align}\nx +  3 y  - 2 z  &= 5\\\\\n3x + 5y + 6z &= 7\\\\\n2x + 4y + 3z &= 8.\n\\end{align}\n\\]\nMathematicians figured out that by representing these linear systems of equations using matrices and vectors, predefined algorithms could be designed to solve any system of linear equations. A basic linear algebra class will teach some of these algorithms, such as Gaussian elimination, the Gauss-Jordan elimination, and the LU and QR decompositions. These methods are usually covered in detail in university level linear algebra courses.\nTo explain matrix multiplication, define two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\)\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\na_{11}&a_{12}&\\dots&a_{1n}\\\\\na_{21}&a_{22}&\\dots&a_{2n}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\na_{m1}&a_{2}&\\dots&a_{mn}\n\\end{pmatrix}, \\,\n\\mathbf{B} = \\begin{pmatrix}\nb_{11}&b_{12}&\\dots&b_{1p}\\\\\nb_{21}&b_{22}&\\dots&b_{2p}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\nb_{n1}&b_{n2}&\\dots&b_{np}\n\\end{pmatrix}\n\\]\nand define the product of matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) as the matrix \\(\\mathbf{C} = \\mathbf{A}\\mathbf{B}\\) that has entries \\(c_{ij}\\) equal to the sum of the component-wise product of the \\(i\\)th row of \\(\\mathbf{A}\\) with the \\(j\\)th column of \\(\\mathbf{B}\\). Using R code we can define \\(\\mathbf{C}= \\mathbf{A}\\mathbf{B}\\) as follows:\n\nm &lt;- nrow(A)\np &lt;- ncol(B)\nC &lt;- matrix(0, m, p)\nfor(i in 1:m){\n  for(j in 1:p){\n    C[i,j] &lt;- sum(A[i,] * B[,j])\n  }\n}\n\nBecause this operation is so common, R includes a mathematical operator %*% for matrix multiplication:\n\nC &lt;- A %*% B\n\nUsing mathematical notation \\(\\mathbf{C} = \\mathbf{A}\\mathbf{B}\\) looks like this:\n\\[\n\\begin{pmatrix}\na_{11}b_{11} + \\dots + a_{1n}b_{n1}&\na_{11}b_{12} + \\dots + a_{1n}b_{n2}&\n\\dots&\na_{11}b_{1p} + \\dots + a_{1n}b_{np}\\\\\na_{21}b_{11} + \\dots + a_{2n}b_{n1}&\na_{21}b_{n2} + \\dots + a_{2n}b_{n2}&\n\\dots&\na_{21}b_{1p} + \\dots + a_{2n}b_{np}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\na_{m1}b_{11} + \\dots +a_{mn}b_{n1}&\na_{m1}b_{n2} + \\dots + a_{mn}b_{n2}&\n\\dots&\na_{m1}b_{1p} + \\dots + a_{mn}b_{np}\\\\\n\\end{pmatrix}\n\\]\nNote this definition implies that the multiplication \\(\\mathbf{A}\\mathbf{B}\\) is only possible when the number of rows of \\(\\mathbf{A}\\) matches the number of columns of \\(\\mathbf{B}\\).\nSo how does this definition of matrix multiplication help solve systems of equations? First, any system of equations with unknowns \\(x_1, \\dots x_n\\)\n\\[\n\\begin{align}\na_{11} x_1 + a_{12} x_2 \\dots + a_{1n}x_n &= b_1\\\\\na_{21} x_1 + a_{22} x_2 \\dots + a_{2n}x_n &= b_2\\\\\n\\vdots\\\\\na_{n1} x_1 + a_{n2} x_2 \\dots + a_{nn}x_n &= b_n\\\\\n\\end{align}\n\\]\ncan now be represented as matrix multiplication by defining the following matrices:\n\\[\n\\mathbf{A} =\\begin{pmatrix}\na_{11}&a_{12}&\\dots&a_{1n}\\\\\na_{21}&a_{22}&\\dots&a_{2n}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\na_{m1}&a_{22}&\\dots&a_{nn}\n\\end{pmatrix}\n,\\,\n\\mathbf{b} =\n\\begin{pmatrix}\nb_1\\\\\nb_2\\\\\n\\vdots\\\\\nb_n\n\\end{pmatrix}\n,\\, \\mbox{ and }\n\\mathbf{x} =\n\\begin{pmatrix}\nx_1\\\\\nx_2\\\\\n\\vdots\\\\\nx_n\n\\end{pmatrix}\n\\]\nand rewriting the equation simply as\n\\[\n\\mathbf{A}\\mathbf{x} =  \\mathbf{b}\n\\]\nThe linear algebra algorithms listed above, such as Gaussian elimination, provide a way to compute the inverse matrix \\(A^{-1}\\) that solves the equation for \\(\\mathbf{x}\\):\n\\[\n\\mathbf{A}^{-1}\\mathbf{A}\\mathbf{x} =   \\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}\n\\]\nTo solve the first equation we wrote out in R, we can use the function solve:\n\nA &lt;- matrix(c(1, 3, -2, 3, 5, 6, 2, 4, 3), 3, 3, byrow = TRUE)\nb &lt;- matrix(c(5, 7, 8))\nsolve(A, b)"
  },
  {
    "objectID": "highdim/linear-algebra.html#the-identity-matrix",
    "href": "highdim/linear-algebra.html#the-identity-matrix",
    "title": "20  Applied Linear Algebra",
    "section": "\n20.2 The identity matrix",
    "text": "20.2 The identity matrix\nThe identity matrix, represented with a bold \\(\\mathbf{I}\\), is like the number 1, but for matrices: if you multiply a matrix by the identity matrix, you get back the matrix.\n\\[\n\\mathbf{I}\\mathbf{x} = \\mathbf{x}\n\\] If you do some math with the definition of matrix multiplication you will realize that \\(\\mathbf{1}\\) is a matrix with the same number of rows and columns (refereed to as square matrix) with 0s everywhere except the diagonal:\n\\[\n\\mathbf{I}=\\begin{pmatrix}\n1&0&\\dots&0\\\\\n0&1&\\dots&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\n0&0&\\dots&1\n\\end{pmatrix}\n\\] It also implies that due to the definition of an inverse matrix we have\n\\[\n\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{1}\n\\]\nBecause the default for the second argument in solve is an identity matrix, if we simply type solve(A), we obtain the inverse \\(\\mathbf{A}^{-1}\\). This means we can also obtain a solution to our system of equations with:\n\nsolve(A) %*% b"
  },
  {
    "objectID": "highdim/linear-algebra.html#distance",
    "href": "highdim/linear-algebra.html#distance",
    "title": "20  Applied Linear Algebra",
    "section": "\n20.3 Distance",
    "text": "20.3 Distance\nMany of the analyses we perform with high-dimensional data relate directly or indirectly to distance. For example, most machine learning techniques rely on being able to define distances between observations, using features or predictors. Clustering algorithms, for example, search of observations that are similar. But what does this mean mathematically?\nTo define distance, we introduce another linear algebra concept: the norm. Recall that a point in two dimensions can represented in polar coordinates as:\n\n\n\n\n\n\n\n\nwith \\(\\theta = \\arctan{\\frac{x2}{x1}}\\) and \\(r = \\sqrt{x_1^2 + x_2^2}\\). If we think of the point as two dimensional column vector \\(\\mathbf{x} = (x_1, x_2)^\\top\\), \\(r\\) defines the norm of \\(\\mathbf{x}\\). The norm can be thought of as the size of the two-dimensional vector disregarding the direction: if we change the angle, the vector changes but the size does not. The point of defining the norm is that we can extrapolated the concept of size to higher dimensions. Specifically, we write the norm for any vector \\(\\mathbf{x}\\) as:\n\\[\n||\\mathbf{x}|| = \\sqrt{x_1^2 + x_2^2 + \\dots + x_p^2}\n\\]\nWe can use the linear algebra concepts we have learned to define the norm like this:\n\\[\n||\\mathbf{x}||^2 = \\mathbf{x}^\\top\\mathbf{x}\n\\]\nTo define distance, suppose we have two two-dimensional points \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\). We can define how similar they are by simply using euclidean distance.\n\n\n\n\n\n\n\n\nWe know that the distance is equal to the length of the hypotenuse:\n\\[\n\\sqrt{(x_{11} - x_{12})^2 + (x_{21} - x_{22})^2}\n\\]\nThe reason we introduced the norm is because this distance is the size of the vector between the two points and this can be extrapolated to any dimension. The distance between two points, regardless of the dimensions, is defined as the norm of the difference:\n\\[\n|| \\mathbf{x}_1 - \\mathbf{x}_2||.\n\\]\nIf we use the digit data, the distance between the first and second observation will compute distance using all 784 features:\n\\[\n|| \\mathbf{x}_1 - \\mathbf{x}_2 || = \\sqrt{ \\sum_{j=1}^{784} (x_{1,j}-x_{2,j })^2 }\n\\]\nTo demonstrate, let’s pick the features for three digits:\n\nx_1 &lt;- x[6,]\nx_2 &lt;- x[17,]\nx_3 &lt;- x[16,]\n\nWe can compute the distances between each pair using the definitions we just learned:\n\nc(sum((x_1 - x_2)^2), sum((x_1 - x_3)^2), sum((x_2 - x_3)^2)) |&gt; sqrt()\n#&gt; [1] 2320 2331 2519\n\nIn R, the function crossprod(x) is convenient for computing norms it multiplies t(x) by x\n\nc(crossprod(x_1 - x_2), crossprod(x_1 - x_3), crossprod(x_2 - x_3)) |&gt; sqrt()\n#&gt; [1] 2320 2331 2519\n\nNote crossprod takes a matrix as the first argument and therefore the vectors used here are being coerced into single column matrices. Also note that crossprod(x,y) multiples t(x) by y.\nWe can see that the distance is smaller between the first two. This is in agreement with the fact that the first two are 2s and the third is a 7.\n\ny[c(6, 17, 16)]\n#&gt; [1] 2 2 7\n\nWe can also compute all the distances at once relatively quickly using the function dist, which computes the distance between each row and produces an object of class dist:\n\nd &lt;- dist(x[c(6,17,16),])\nclass(d)\n#&gt; [1] \"dist\"\n\nThere are several machine learning related functions in R that take objects of class dist as input. To access the entries using row and column indices, we need to coerce it into a matrix. We can see the distance we calculated above like this:\n\nd\n#&gt;      1    2\n#&gt; 2 2320     \n#&gt; 3 2331 2519\n\nWe can quickly see an image of the distances between observations using this function. As an example, we compute the distance between each of the first 300 observations and then make an image:\n\nd &lt;- dist(x[1:300,])\nimage(as.matrix(d))\n\nIf we order this distance by the labels, we can see yellowish squares near the diagonal. This is because observations from the same digits tend to be closer than to different digits:\n\nimage(as.matrix(d)[order(y[1:300]), order(y[1:300])])"
  },
  {
    "objectID": "highdim/linear-algebra.html#sec-predictor-space",
    "href": "highdim/linear-algebra.html#sec-predictor-space",
    "title": "20  Applied Linear Algebra",
    "section": "\n20.4 Spaces",
    "text": "20.4 Spaces\nPredictor space is a concept that is often used to describe machine learning algorithms. The term space refers to an advanced mathematical definition for which we provide a simplified explanation to help understand the term predictor space when used in the context of machine learning algorithms.\nWe can think of all predictors \\((x_{i,1}, \\dots, x_{i,p})^\\top\\) for all observations \\(i=1,\\dots,n\\) as \\(n\\) \\(p\\)-dimensional points. A space can be thought of as the collection of all possible points that should be considered for the data analysis in question. This includes points we could see, but have not been observed yet. In the case of the handwritten digits, we can think of the predictor space as any point \\((x_{1}, \\dots, x_{p})^\\top\\) as long as each entry \\(x_i, \\, i = 1, \\dots, p\\) is between 0 and 255.\nSome Machine Learning algorithms also define subspaces. A common approach is to define neighborhoods of points that are close to a center. We can do this by selecting a center \\(\\mathbf{x}_0\\), a minimum distance \\(r\\), and defining the subspace as the collection of points \\(\\mathbf{x}\\) that satisfy\n\\[\n|| \\mathbf{x} - \\mathbf{x}_0 || \\leq r.\n\\]\nWe can think of this subspace as a multidimensional sphere since every point is the same distance away from the center.\nOther machine learning algorithms partition the predictor space into non-overlapping regions and then make different predictions for each region using the data in the region. We will learn about these in Section 29.5."
  },
  {
    "objectID": "highdim/linear-algebra.html#exercises",
    "href": "highdim/linear-algebra.html#exercises",
    "title": "20  Applied Linear Algebra",
    "section": "\n20.5 Exercises",
    "text": "20.5 Exercises\n1. Generate two matrix, A and B, containing randomly generated and normally distributed numbers. The dimensions of these two matrices should \\(4 \\times 3\\) and \\(3 \\times 6\\), respectively. Confirm that C &lt;- A %*% B produce the same results as:\n\nm &lt;- nrow(A)\np &lt;- ncol(B)\nC &lt;- matrix(0, m, p)\nfor(i in 1:m){\n  for(j in 1:p){\n    C[i,j] &lt;- sum(A[i,] * B[,j])\n  }\n}\n\n2. Solve the following system of equations using R:\n\\[\n\\begin{align}\nx + y + z + w &= 10\\\\\n2x + 3y - z - w &= 5\\\\\n3x - y + 4z - 2w &= 15\\\\\n2x + 2y - 2z - 2w &= 20\\\\\n\\end{align}\n\\]\n3. Define x\n\nmnist &lt;- read_mnist()\nx &lt;- mnist$train$images[1:300,] \ny &lt;- mnist$train$labels[1:300]\n\nand compute the distance matrix\n\nd &lt;- dist(x)\nclass(d)\n\nGenerate a boxplot showing the distances for the second row of d stratified by digits. Do not include the distance to itself which we know it is 0. Can you predict what digit is represented by the second row of x?\n4. Use the apply function and matrix algebra to compute the distance between the second digit mnist$train$images[4,] and all other digits represented in mnist$train$images. Then generate as boxplot as in exercise 2 and predict what digit is the fourth row.\n5. Compute the distance between each feature and the feature representing the middle pixel (row 14 column 14). Create an image plot of where the distance is shown with color in the pixel position."
  },
  {
    "objectID": "highdim/dimension-reduction.html#motivation-preserving-distance",
    "href": "highdim/dimension-reduction.html#motivation-preserving-distance",
    "title": "\n21  Dimension reduction\n",
    "section": "\n21.1 Motivation: preserving distance",
    "text": "21.1 Motivation: preserving distance\nWe consider an example with twin heights. Some pairs are adults, the others are children. Here we simulate 100 two-dimensional points that represent the number of standard deviations each individual is from the mean height. Each point is a pair of twins. We use the mvrnorm function from the MASS package to simulate bivariate normal data.\n\nset.seed(1983)\nlibrary(MASS)\nn &lt;- 100\nrho &lt;- 0.9\nsigma &lt;- 3\ns &lt;- sigma^2*matrix(c(1, rho, rho, 1), 2, 2)\nx &lt;- rbind(mvrnorm(n/2, c(69, 69), s),\n           mvrnorm(n/2, c(60, 60), s))\n\nA scatterplot quickly reveals that the correlation is high and that there are two groups of twins, the adults (upper right points) and the children (lower left points):\n\n\n\n\n\n\n\n\nOur features are \\(n\\) two-dimensional points, the two heights. For illustrative purposes, we will act as if visualizing two dimensions is too challenging and we want to explore the data through a histogram of a one-dimensional variable. We therefore want to reduce the dimensions from two to one, but still be able to understand important characteristics of the data, for example that the observations cluster into two groups: adults and children. To show the ideas presented here are generally useful, we will standardize the data so that observations are in standard units rather than inches:\n\nlibrary(matrixStats)\n#&gt; \n#&gt; Attaching package: 'matrixStats'\n#&gt; The following object is masked from 'package:dplyr':\n#&gt; \n#&gt;     count\nx &lt;- sweep(x, 2, colMeans(x))\nx &lt;- sweep(x, 2, colSds(x), \"/\")\n\nIn the figure above we show the distance between observation 1 and 2 (blue), and observation 1 and 51 (red). Note that the blue line is shorter, which implies 1 and 2 are closer.\nWe can compute these distances using dist:\n\nd &lt;- dist(x)\nas.matrix(d)[1, 2]\n#&gt; [1] 0.595\nas.matrix(d)[2, 51]\n#&gt; [1] 1.39\n\nThis distance is based on two dimensions and we need a distance approximation based on just one.\nLet’s start with the naive approach of simply removing one of the two dimensions. Let’s compare the actual distances to the distances computed with just the first dimension:\n\nz &lt;- x[,1]\n\nTo make the distances comparable, we divide the sum of squares by the number of dimensions being added. So for the two dimensional case we have\n\\[\n\\sqrt{ \\frac{1}{2} \\sum_{j=1}^2 (x_{1,j}-x_{2,j})^2 },\n\\]\nso to make the distances comparable we divide by \\(\\sqrt{2}\\):\n\nplot(dist(x) / sqrt(2), dist(z))\nabline(0, 1, col = \"red\")\n\n\n\n\n\n\n\n\n\nThis one number summary does ok at preserving distances, but, can we pick a one-dimensional summary that makes the approximation even better?\nIf we look back at the scatterplot and visualize a line between any pair of points, the length of this line is the distance between the two points. These lines tend to go along the direction of the diagonal. We will learn that we can rotate the points in a way that preserve the distance between points, while increasing the variability in one dimension and reducing it on the other. By doing this, we keep more of the information about distances in the first dimension. In the next section we describe a mathematical approach that permits us to find rotations that preserve distance between points. We can then find the rotation that maximizes the variability in the first dimension."
  },
  {
    "objectID": "highdim/dimension-reduction.html#rotations",
    "href": "highdim/dimension-reduction.html#rotations",
    "title": "\n21  Dimension reduction\n",
    "section": "\n21.2 Rotations",
    "text": "21.2 Rotations\nAny two-dimensional point \\((x_1, x_2)^\\top\\) can be written as the base and height of a triangle with a hypotenuse going from \\((0,0)^\\top\\) to \\((x_1, x_2)^\\top\\):\n\\[\nx_1 = r \\cos\\phi, \\,\\, x_2 = r \\sin\\phi\n\\]\nwith \\(r\\) the length of the hypotenuse and \\(\\phi\\) the angel between the hypotenuse and the x-axis.\nWe can rotate the point \\((x_1, x_2)^\\top\\) around a circle with center \\((0,0)^\\top\\) and radius \\(r\\) by an angle \\(\\theta\\) by changing the angle in the previous equation to \\(\\phi + \\theta\\):\n\\[\nz_1 = r \\cos(\\phi+ \\theta), \\,\\,\nz_2 = r \\sin(\\phi + \\theta)\n\\]\n\n\n\n\n\n\n\n\nWe can use trigonometric identities to rewrite \\((z_1, z_2)\\) in the following way:\n\\[\n\\begin{align}\nz_1 = r \\cos(\\phi + \\theta) = r \\cos \\phi \\cos\\theta -  r \\sin\\phi \\sin\\theta =  x_1 \\cos(\\theta) -  x_2 \\sin(\\theta)\\\\\nz_2 = r \\sin(\\phi + \\theta) =  r \\cos\\phi \\sin\\theta + r \\sin\\phi \\cos\\theta =  x_1 \\sin(\\theta) + x_2 \\cos(\\theta)\n\\end{align}\n\\]\nNow we can rotate each point in the dataset by simply applying the formula above to each pair \\((x_{i,1}, x_{i,2})^\\top\\). Here is what the twin standardized heights look like after rotating each point by \\(-45\\) degrees:\n\n\n\n\n\n\n\n\nNote that while the variability of \\(x_1\\) and \\(x_2\\) are similar, the variability of \\(z_1\\) is much larger than the variability of \\(z_2\\). Also note that the distances between points appear to be preserved. In the next sections, we show, mathematically, that this in fact the case."
  },
  {
    "objectID": "highdim/dimension-reduction.html#linear-transformations",
    "href": "highdim/dimension-reduction.html#linear-transformations",
    "title": "\n21  Dimension reduction\n",
    "section": "\n21.3 Linear transformations",
    "text": "21.3 Linear transformations\nAny time a matrix \\(\\mathbf{X}\\) is multiplied by another matrix \\(\\mathbf{A}\\), we refer to the product \\(\\mathbf{Z} = \\mathbf{X}\\mathbf{A}\\) as a linear transformation of \\(\\mathbf{X}\\). Below we show that the rotations described above are a linear transformation. To see this, note that for any row \\(i\\), the first entry was:\n\\[z_{i,1} = a_{1,1} x_{i,1} + a_{2,1} x_{i,2}\\]\nwith \\(a_{1,1} = \\cos\\theta\\) and \\(a_{2,1} = -\\sin\\theta\\).\nThe second entry was also a linear transformation:\n\\[z_{i,2} = a_{1,2} x_{i,1} + a_{2,2} x_{i,2}\\]\nwith \\(a_{1,2} = \\sin\\theta\\) and \\(a_{2,2} = \\cos\\theta\\).\nWe can write these equations using matrix notation:\n\\[\n\\begin{pmatrix}\nz_1\\\\z_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\na_{1,1}&a_{1,2}\\\\\na_{2,1}&a_{2,2}\n\\end{pmatrix}^\\top\n\\begin{pmatrix}\nx_1\\\\x_2\n\\end{pmatrix}\n\\]\nAn advantage of using linear algebra is that we can write the transformation for the entire dataset by saving all observations in a \\(N \\times 2\\) matrix\n\\[\n\\mathbf{X} \\equiv\n\\begin{bmatrix}\n\\mathbf{x_1}^\\top\\\\\n\\vdots\\\\\n\\mathbf{x_n}^\\top\n\\end{bmatrix} =\n\\begin{bmatrix}\nx_{1,1}&x_{1,2}\\\\\n\\vdots&\\vdots\\\\\nx_{n,1}&x_{n,2}\n\\end{bmatrix}\n\\]\nWe can then obtained the rotated values \\(\\mathbf{z}_i\\) for each row \\(i\\) by applying a linear transformation of \\(X\\):\n\\[\n\\mathbf{Z} = \\mathbf{X} \\mathbf{A}\n\\mbox{ with }\n\\mathbf{A} = \\,\n\\begin{pmatrix}\na_{1,1}&a_{1,2}\\\\\na_{2,1}&a_{2,2}\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\cos \\theta&\\sin \\theta\\\\\n-\\sin \\theta&\\cos \\theta\n\\end{pmatrix}\n.\n\\]\nIf we define\n\ntheta &lt;- 2*pi*-45/360 #convert to radians\nA &lt;- matrix(c(cos(theta), -sin(theta), sin(theta), cos(theta)), 2, 2)\n\nWe can write code implementing a rotation by any angle \\(\\theta\\) using linear algebra:\n\nrotate &lt;- function(x, theta){\n  theta &lt;- 2*pi*theta/360\n  A &lt;- matrix(c(cos(theta), -sin(theta), sin(theta), cos(theta)), 2, 2)\n  x %*% A\n}\n\nThe columns of \\(\\mathbf{A}\\) are referred to as directions because if we draw a vector from \\((0,0)\\) to \\((a_{1,j}, a_{2,j})\\) it points in the direction of the line that will become the \\(j-th\\) dimension.\nAnother advantage of linear algebra is that if we can find the inverse matrix of \\(\\mathbf{A}^\\top\\) we can convert \\(\\mathbf{Z}\\) back to \\(\\mathbf{X}\\) again using a linear transformation.\nIn this particular case we can use trigonometry to show that\n\\[\nx_{i,1} = b_{1,1} z_{i,1} + b_{2,1} z_{i,2}\\\\\nx_{i,2} = b_{1,2} z_{i,1} + b_{2,2} z_{i,2}\n\\]\nwith \\(b_{2,1} = \\cos\\theta\\), \\(b_{2,1} = \\sin\\theta\\), \\(b_{1,2} = -\\sin\\theta\\), and \\(b_{2,2} = \\cos\\theta\\).\nThis implies that\n\\[\n\\mathbf{X} = \\mathbf{Z}\n\\begin{pmatrix}\n\\cos \\theta&-\\sin \\theta\\\\\n\\sin \\theta&\\cos \\theta\n\\end{pmatrix}.\n\\] Note that the transformation used above is actually \\(\\mathbf{A}^\\top\\) which implies that\n\\[\n\\mathbf{Z} \\mathbf{A}^\\top = \\mathbf{X} \\mathbf{A}\\mathbf{A}^\\top\\ = \\mathbf{X}\n\\]\nand therefore that \\(\\mathbf{A}^\\top\\) is the inverse of \\(\\mathbf{A}\\). This also implies that all the information in \\(\\mathbf{X}\\) is included in the rotation \\(\\mathbf{Z}\\), and it can be retrieved via a linear transformation. A consequence is that for any rotation the distances are preserved. Here is an example for a 30 degree rotation, but it works for any angle:\n\nall.equal(as.matrix(dist(rotate(x, 30))), as.matrix(dist(x)))\n#&gt; [1] TRUE\n\nThe next section explains why this happens."
  },
  {
    "objectID": "highdim/dimension-reduction.html#orthogonal-transformations",
    "href": "highdim/dimension-reduction.html#orthogonal-transformations",
    "title": "\n21  Dimension reduction\n",
    "section": "\n21.4 Orthogonal transformations",
    "text": "21.4 Orthogonal transformations\nRecall that the distance between two points, say rows \\(h\\) and \\(i\\) of the transformation \\(\\mathbf{Z}\\), can be written like this:\n\\[\n||\\mathbf{z}_h - \\mathbf{z}_i|| = (\\mathbf{z}_h - \\mathbf{z}_i)^\\top(\\mathbf{z}_h - \\mathbf{z}_i)\n\\]\nwith \\(\\mathbf{z}_h\\) and \\(\\mathbf{z}_i\\) the \\(p \\times 1\\) column vectors stored in the \\(h\\)-th and \\(i\\)-th rows of \\(\\mathbf{X}\\), respectively.\n\n\n\n\n\n\nRemember that we represent the rows of a matrix as column vectors. This explains why we use \\(\\mathbf{A}\\) when showing the multiplication for the matrix \\(\\mathbf{Z}=\\mathbf{X}\\mathbf{A}\\) but transpose the operation when showing the transformation for just one observation: \\(\\mathbf{z}_i = \\mathbf{A}^\\top\\mathbf{x}_i\\)\n\n\n\nUsing linear algebra, we can rewrite the quantity above as\n\\[\n||\\mathbf{z}_h - \\mathbf{z}_i|| =\n||\\mathbf{A}^\\top \\mathbf{x}_h - \\mathbf{A}^\\top\\mathbf{x}_i||^2 =\n(\\mathbf{x}_h - \\mathbf{x}_i)^\\top \\mathbf{A} \\mathbf{A}^\\top (\\mathbf{x}_h - \\mathbf{x}_i)\n\\]\nNote that if \\(\\mathbf{A} \\mathbf{A} ^\\top= \\mathbf{I}\\) then the distance between the \\(h\\)th and \\(i\\)th rows is the same for the original and transformed data.\nWe refer to transformation with the property \\(\\mathbf{A} \\mathbf{A}^\\top = \\mathbf{I}\\) as an orthogonal transformations and they are guaranteed to preserves the distance between any two points.\nWe previously demonstrated our rotation has this property. We can confirm using R:\n\nA %*% t(A)\n#&gt;          [,1]     [,2]\n#&gt; [1,] 1.00e+00 1.01e-17\n#&gt; [2,] 1.01e-17 1.00e+00\n\nNotice that \\(\\mathbf{A}\\) being orthogonal also guarantees that the total sum of squares (TSS) of \\(\\mathbf{X}\\), defined as \\(\\sum_{i=1}^n \\sum_{j=1}^p x_{i,j}^2\\) is equal to the total sum of squares of the rotation \\(\\mathbf{Z} = \\mathbf{X}\\mathbf{A}^\\top\\). To show this notice that if we denote the rows of \\(\\mathbf{Z}\\) as \\(\\mathbf{z}_1, \\dots, \\mathbf{z}_n\\), then sum of squares can be written as:\n\\[\n\\sum_{1=1}^n ||\\mathbf{z}_i||^2 = \\sum_{i=1}^n ||\\mathbf{A}^\\top\\mathbf{x}_i||^2 = \\sum_{i=1}^n \\mathbf{x}_i^\\top \\mathbf{A}\\mathbf{A}^\\top  \\mathbf{x}_i = \\sum_{i=1}^n \\mathbf{x}_i^\\top\\mathbf{x}_i = \\sum_{i=1}^n||\\mathbf{x}_i||^2\n\\]\nWe can confirm using R:\n\ntheta &lt;- -45\nz &lt;- rotate(x, theta) # works for any theta\nsum(x^2)\n#&gt; [1] 198\nsum(z^2)\n#&gt; [1] 198\n\nThis can be interpreted as a consequence of the fact that orthogonal transformation guarantee that all the information is preserved.\nHowever, although the total is preserved, the sum of squares for the individual columns changes. Here we compute the proportion of TSS attributed to each column, referred to as the variance explained or variance captured by each column, for \\(\\mathbf{X}\\)\n\ncolSums(x^2)/sum(x^2)\n#&gt; [1] 0.5 0.5\n\nand \\(\\mathbf{Z}\\)\n\ncolSums(z^2)/sum(z^2)\n#&gt; [1] 0.9848 0.0152\n\nIn the next section we describe how this last mathematical result can be useful."
  },
  {
    "objectID": "highdim/dimension-reduction.html#sec-pca",
    "href": "highdim/dimension-reduction.html#sec-pca",
    "title": "\n21  Dimension reduction\n",
    "section": "\n21.5 Principal Component Analysis (PCA)",
    "text": "21.5 Principal Component Analysis (PCA)\nWe have established that orthogonal transformations preserve the the distance between observations and the total sum of squares. We have also established that, while the TSS remains the same, the way this total is distributed across the columns can change.\nThe general idea behind Principal Component Analysis (PCA) is to try to find orthogonal transformations that concentrate the variance explained in the first few columns. We can then focus on these few columns, effectively reducing the dimension of the problem. In our specific example, we are looking for the rotation that maximizes the variance explained in the first column. The following code performs a grid search across rotations from -90 to 0:\n\n\n\n\n\n\n\n\n\nangles &lt;- seq(0, -90)\nv &lt;- sapply(angles, function(angle) colSums(rotate(x, angle)^2))\nvariance_explained &lt;- v[1,]/sum(x^2)\nplot(angles, variance_explained, type = \"l\")\n\nWe find that a -45 degree rotation appears to achieve the maximum, with over 98% of the total variability explained by the first dimension. We can rotate the entire dataset using:\n\nz &lt;- x %*% A\n\nThe following animation further illustrates how different rotations affect the variability explained by the dimensions of the rotated data:\n\n#&gt; Output at: pca.gif\n\n\n\n\n\n\n\nThe first dimension of z is referred to as the first principal component (PC). Because almost all the variation is explained by this first PC, the distance between rows in x can be very well approximated by the distance calculated with just z[,1].\n\n\n\n\n\n\n\n\nWe also notice that the two groups, adults and children, can be clearly observed with the one number summary, better than with any of the two orginal dimesions.\n\n\n\n\n\n\n\n\n\nhist(x[,1], breaks = seq(-4,4,0.5))\nhist(x[,2], breaks = seq(-4,4,0.5))\nhist(z[,1], breaks = seq(-4,4,0.5))\n\nWe can visualize these to see how the first component summarizes the data. In the plot below red represents high values and blue negative values:\n\n\n\n\n\n\n\n\nThis idea generalizes to dimensions higher than 2. As we did in our two dimensional example, we start by finding the \\(p\\times1\\) vector \\(\\mathbf{a}_1\\) with\\(||\\mathbf{a}_1||=1\\) that maximizes \\(||\\mathbf{X} \\mathbf{a}_1||\\). \\(\\mathbf{X} \\mathbf{a}_1\\) is the first PC. To find the second PC, we subtract the variation explained by first PC from \\(\\mathbf{X}\\):\n\\[\n\\mathbf{r} = \\mathbf{X} - \\mathbf{X} \\mathbf{a}_1 \\mathbf{a}_1^\\top\n\\]\nand then find the vector \\(\\mathbf{a}_2\\) with\\(||\\mathbf{a}_2||=1\\) that maximizes \\(||\\mathbf{r} \\mathbf{a}_2||\\). \\(\\mathbf{X} \\mathbf{a}_2\\) is the second PC. We then subtract the variation explained by the first two PCs, and continue this process until we have the entire rotation matrix and matrix of principal components, respectively:\n\\[\n\\mathbf{A} =\n\\begin{bmatrix}\n\\mathbf{a}_1&\\dots&\\mathbf{a}_p\n\\end{bmatrix},\n\\mathbf{Z} = \\mathbf{X}\\mathbf{A}\n\\]\nThe ideas of distance preervation extends to higher dimensions. For a multidimensional matrix with \\(p\\) columns, the \\(\\mathbf{A}\\) transformation preserves distance between rows, but with the variance exaplined by the columns in decreasing order. If the variances of the columns \\(\\mathbf{Z}_j\\), \\(j&gt;k\\) are very small, these dimensions have little to contribute to the distance calculation and we can approximate distance between any two points with just \\(k\\) dimensions. If \\(k\\) is much smaller than \\(p\\), then we can achieve a very efficient summary of our data.\n\n\n\n\n\n\nNotice that the solution to this maximization problem is not unique because \\(||\\mathbf{X} \\mathbf{a}|| = ||-\\mathbf{X} \\mathbf{a}||\\). Also note that if we multiply a column of \\(\\mathbf{A}\\) by \\(-1\\) we still represent \\(\\mathbf{X}\\) as \\(\\mathbf{Z}\\mathbf{A}^\\top\\) as long as we also multiple the corresponsing column of \\(\\matbf{Z}\\) by -1. This implies that sign of each column of the rotation \\(\\mathbf{A}\\) and principal component matrix \\(\\mathbf{Z}\\) is arbitrary.\n\n\n\nIn R we can find the principal components of any matrix with the function prcomp:\n\npca &lt;- prcomp(x, center = FALSE)\n\nNote that default behavior is to center the columns of x before computing the PCs, an operation we don’t need because our matrix is scaled.\nThe object pca includes the rotated data \\(Z\\) in pca$x and the rotation \\(\\mathbf{A}\\) in pca$rotation.\nWe can see that columns of the pca$rotation are indeed the rotation obtained with -45 (remember the sign is arbitrary)\n\npca$rotation\n#&gt;         PC1    PC2\n#&gt; [1,] -0.707  0.707\n#&gt; [2,] -0.707 -0.707\n\nThe sqaure root of the variation of each column is included in the pca$sdev component. This implies we can compute the variance explained by each PC using:\n\npca$sdev^2/sum(pca$sdev^2)\n#&gt; [1] 0.9848 0.0152\n\nThe function summary performs this calculation for us:\n\nsummary(pca)\n#&gt; Importance of components:\n#&gt;                          PC1    PC2\n#&gt; Standard deviation     1.403 0.1745\n#&gt; Proportion of Variance 0.985 0.0152\n#&gt; Cumulative Proportion  0.985 1.0000\n\nWe also see that we can transform between x (\\(\\mathbf{X}\\)) and pca$x (\\(\\mathbf{Z}\\)) as explained with mathematical formulas above:\n\nall.equal(pca$x, x %*% pca$rotation)\n#&gt; [1] TRUE\nall.equal(x, pca$x %*% t(pca$rotation))\n#&gt; [1] TRUE"
  },
  {
    "objectID": "highdim/dimension-reduction.html#iris-example",
    "href": "highdim/dimension-reduction.html#iris-example",
    "title": "\n21  Dimension reduction\n",
    "section": "\n21.6 Iris example",
    "text": "21.6 Iris example\nThe iris data is a widely used example in data analysis courses. It includes four botanical measurements related to three flower species:\n\nnames(iris)\n#&gt; [1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\" \n#&gt; [5] \"Species\"\n\nIf you print iris$Species you will see that the data is ordered by the species.\nLet’s compute the distance between each observation. You can clearly see the three species with one species very different from the other two:\n\nx &lt;- iris[,1:4] |&gt; as.matrix()\nd &lt;- dist(x)\nimage(as.matrix(d), col = rev(RColorBrewer::brewer.pal(9, \"RdBu\")))\n\n\n\n\n\n\n\n\n\nOur predictors here have four dimensions, but three are very correlated:\n\ncor(x)\n#&gt;              Sepal.Length Sepal.Width Petal.Length Petal.Width\n#&gt; Sepal.Length        1.000      -0.118        0.872       0.818\n#&gt; Sepal.Width        -0.118       1.000       -0.428      -0.366\n#&gt; Petal.Length        0.872      -0.428        1.000       0.963\n#&gt; Petal.Width         0.818      -0.366        0.963       1.000\n\nIf we apply PCA, we should be able to approximate this distance with just two dimensions, compressing the highly correlated dimensions. Using the summary function we can see the variability explained by each PC:\n\npca &lt;- prcomp(x)\nsummary(pca)\n#&gt; Importance of components:\n#&gt;                          PC1    PC2    PC3     PC4\n#&gt; Standard deviation     2.056 0.4926 0.2797 0.15439\n#&gt; Proportion of Variance 0.925 0.0531 0.0171 0.00521\n#&gt; Cumulative Proportion  0.925 0.9777 0.9948 1.00000\n\nThe first two dimensions account for 97% of the variability. Thus we should be able to approximate the distance very well with two dimensions. We can visualize the results of PCA:\n\n\n\n\n\n\n\n\nAnd see that the first pattern is sepal length, petal length, and petal width (red) in one direction and sepal width (blue) in the other. The second pattern is the sepal length and petal width in one direction (blue) and petal length and petal width in the other (red). You can see from the weights that the first PC1 drives most of the variability and it clearly separates the first third of samples (setosa) from the second two thirds (versicolor and virginica). If you look at the second column of the weights, you notice that it somewhat separates versicolor (red) from virginica (blue).\nWe can see this better by plotting the first two PCs with color representing the species:\n\ndata.frame(pca$x[,1:2], Species = iris$Species) |&gt;\n  ggplot(aes(PC1, PC2, fill = Species)) +\n  geom_point(cex = 3, pch = 21) +\n  coord_fixed(ratio = 1)\n\n\n\n\n\n\n\nWe see that the first two dimensions preserve the distance:\n\nd_approx &lt;- dist(pca$x[, 1:2])\nplot(d, d_approx); abline(0, 1, color = \"red\")\n#&gt; Warning in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...):\n#&gt; \"color\" is not a graphical parameter\n\n\n\n\n\n\n\nThis example is more realistic than the first artificial example we used, since we showed how we can visualize the data using two dimensions when the data was four-dimensional."
  },
  {
    "objectID": "highdim/dimension-reduction.html#mnist-example",
    "href": "highdim/dimension-reduction.html#mnist-example",
    "title": "\n21  Dimension reduction\n",
    "section": "\n21.7 MNIST example",
    "text": "21.7 MNIST example\nThe written digits example has 784 features. Is there any room for data reduction? Can we create simple machine learning algorithms using fewer features?\nLet’s load the data:\n\nlibrary(dslabs)\nif (!exists(\"mnist\")) mnist &lt;- read_mnist()\n\nBecause the pixels are so small, we expect pixels close to each other on the grid to be correlated, meaning that dimension reduction should be possible.\nLet’s try PCA and explore the variance of the PCs. This will take a few seconds as it is a rather large matrix.\n\ncol_means &lt;- colMeans(mnist$test$images)\npca &lt;- prcomp(mnist$train$images)\n\n\npc &lt;- 1:ncol(mnist$test$images)\nqplot(pc, pca$sdev)\n#&gt; Warning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\nWe can see that the first few PCs already explain a large percent of the variability:\n\nsummary(pca)$importance[,1:5]\n#&gt;                            PC1     PC2      PC3      PC4      PC5\n#&gt; Standard deviation     576.823 493.238 459.8993 429.8562 408.5668\n#&gt; Proportion of Variance   0.097   0.071   0.0617   0.0539   0.0487\n#&gt; Cumulative Proportion    0.097   0.168   0.2297   0.2836   0.3323\n\nAnd just by looking at the first two PCs we see information about the class. Here is a random sample of 2,000 digits:\n\ndata.frame(PC1 = pca$x[,1], PC2 = pca$x[,2],\n           label = factor(mnist$train$label)) |&gt;\n  sample_n(2000) |&gt;\n  ggplot(aes(PC1, PC2, fill = label)) +\n  geom_point(cex = 3, pch = 21)\n\n\n\n\n\n\n\nWe can also see the linear combinations on the grid to get an idea of what is getting weighted:\n\n\n\n\n\n\n\n\nThe lower variance PCs appear related to unimportant variability in the corners:"
  },
  {
    "objectID": "highdim/dimension-reduction.html#exercises",
    "href": "highdim/dimension-reduction.html#exercises",
    "title": "\n21  Dimension reduction\n",
    "section": "\n21.7 Exercises",
    "text": "21.7 Exercises\n1. We want to explore the tissue_gene_expression predictors by plotting them.\n\ndim(tissue_gene_expression$x)\n\nWe want to get an idea of which observations are close to each other, but the predictors are 500-dimensional so plotting is difficult. Plot the first two principal components with color representing tissue type.\n2. The predictors for each observation are measured on the same measurement device (a gene expression microarray) after an experimental procedure. A different device and procedure is used for each observation. This may introduce biases that affect all predictors for each observation in the same way. To explore the effect of this potential bias, for each observation, compute the average across all predictors and then plot this against the first PC with color representing tissue. Report the correlation.\n3. We see an association with the first PC and the observation averages. Redo the PCA but only after removing the center.\n4. For the first 10 PCs, make a boxplot showing the values for each tissue.\n5. Plot the percent variance explained by PC number. Hint: use the summary function."
  },
  {
    "objectID": "highdim/regularization.html#sec-recommendation-systems",
    "href": "highdim/regularization.html#sec-recommendation-systems",
    "title": "\n22  Regularization\n",
    "section": "\n22.1 Case study: recommendation systems",
    "text": "22.1 Case study: recommendation systems\nRecommendation systems use ratings that users have given items to make specific recommendations. Companies that sell many products to many customers and permit these customers to rate their products, like Amazon, are able to collect massive datasets that can be used to predict what rating a particular user will give a specific item. Items for which a high rating is predicted for a given user are then recommended to that user.\nNetflix uses a recommendation system to predict how many stars a user will give a specific movie. One star suggests it is not a good movie, whereas five stars suggests it is an excellent movie. Here, we provide the basics of how these recommendations are made, motivated by some of the approaches taken by the winners of the Netflix challenges.\nIn October 2006, Netflix offered a challenge to the data science community: improve our recommendation algorithm by 10% and win a million dollars. In September 2009, the winners were announced1. You can read a good summary of how the winning algorithm was put together here: http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/ and a more detailed explanation here: https://www2.seas.gwu.edu/~simhaweb/champalg/cf/papers/KorenBellKor2009.pdf. We will now show you some of the data analysis strategies used by the winning team.\n\n22.1.1 Movielens data\nThe Netflix data is not publicly available, but the GroupLens research lab2 generated their own database with over 20 million ratings for over 27,000 movies by more than 138,000 users. We make a small subset of this data available via the dslabs package:\n\nmovielens |&gt; as_tibble()\n#&gt; # A tibble: 100,004 × 7\n#&gt;   movieId title                      year genres userId rating timestamp\n#&gt;     &lt;int&gt; &lt;chr&gt;                     &lt;int&gt; &lt;fct&gt;   &lt;int&gt;  &lt;dbl&gt;     &lt;int&gt;\n#&gt; 1      31 Dangerous Minds            1995 Drama       1    2.5    1.26e9\n#&gt; 2    1029 Dumbo                      1941 Anima…      1    3      1.26e9\n#&gt; 3    1061 Sleepers                   1996 Thril…      1    3      1.26e9\n#&gt; 4    1129 Escape from New York       1981 Actio…      1    2      1.26e9\n#&gt; 5    1172 Cinema Paradiso (Nuovo c…  1989 Drama       1    4      1.26e9\n#&gt; # ℹ 99,999 more rows\n\nEach row represents a rating given by one user to one movie.\nWe can see the number of unique users that provided ratings and how many unique movies were rated:\n\nmovielens |&gt; \n  summarize(n_users = n_distinct(userId),\n            n_movies = n_distinct(movieId))\n#&gt;   n_users n_movies\n#&gt; 1     671     9066\n\nIf we multiply those two numbers, we get a number larger than 5 million, yet our data table has about 100,000 rows. This implies that not every user rated every movie. So we can think of these data as a very large matrix, with users on the rows and movies on the columns, with many empty cells. The pivot_wider function permits us to convert it to this format, but if we try it for the entire matrix, it will crash R. Let’s show the matrix for six users and four movies.\n\n\n\n\nuserId\nPulp Fiction\nShawshank Redemption\nForrest Gump\nSilence of the Lambs\n\n\n\n13\n3.5\n4.5\n5.0\nNA\n\n\n15\n5.0\n2.0\n1.0\n5.0\n\n\n16\nNA\n4.0\nNA\nNA\n\n\n17\n5.0\n5.0\n2.5\n4.5\n\n\n19\n5.0\n4.0\n5.0\n3.0\n\n\n20\n0.5\n4.5\n2.0\n0.5\n\n\n\n\n\nYou can think of the task of a recommendation system as filling in the NAs in the table above. To see how sparse the matrix is, here is the matrix for a random sample of 100 movies and 100 users with yellow indicating a user/movie combination for which we have a rating.\n\n\n\n\n\n\n\n\nLet’s look at some of the general properties of the data to better understand the challenges.\nThe first thing we notice is that some movies get rated more than others. Below is the distribution. This should not surprise us given that there are blockbuster movies watched by millions and artsy, independent movies watched by just a few. Our second observation is that some users are more active than others at rating movies:\n\n\n\n\n\n\n\n\nWe need to build an algorithm with data we have collected that will then be applied outside our control, as users look for movie recommendations. So let’s create a test set to assess the accuracy of the models we implement. We only consider movies rated five times or more, and users that have rated more than 100 of these movies. We then split the data into a training set and test set by assiging 20% of the ratings made by each user to the test set:\n\nset.seed(2006)\nindexes &lt;- split(1:nrow(movielens), movielens$userId)\ntest_ind &lt;- sapply(indexes, function(ind) sample(ind, ceiling(length(ind)*.2))) |&gt;\n  unlist(use.names = TRUE) |&gt; sort()\ntest_set &lt;- movielens[test_ind,]\ntrain_set &lt;- movielens[-test_ind,]\n\nTo make sure we don’t include movies that are not in both test and train sets, we remove entries using the semi_join function:\n\ntest_set &lt;- test_set |&gt; \n  semi_join(train_set, by = \"movieId\")\ntrain_set &lt;- train_set |&gt; \n  semi_join(test_set, by = \"movieId\")\n\nFinally we use pivot_wider to make a matrix with users represented by rows and movies by the columns\n\ny &lt;- select(train_set, movieId, userId, rating) |&gt;\n  pivot_wider(names_from = movieId, values_from = rating) \nrnames &lt;- y$userId\ny &lt;- as.matrix(y[,-1])\nrownames(y) &lt;- rnames\n\nalong with a table to map movie ids to titles:\n\nmovie_map &lt;- train_set |&gt; select(movieId, title) |&gt; distinct(movieId, .keep_all = TRUE)"
  },
  {
    "objectID": "highdim/regularization.html#sec-netflix-loss-function",
    "href": "highdim/regularization.html#sec-netflix-loss-function",
    "title": "\n22  Regularization\n",
    "section": "\n22.2 Loss function",
    "text": "22.2 Loss function\nThe Netflix challenge decided on a winner based on the residual mean squared error (RMSE) on a test set. We define \\(y_{u,i}\\) as the rating for movie \\(i\\) by user \\(u\\) and denote our prediction with \\(\\hat{y}_{u,i}\\). The RMSE is then defined as:\n\\[\n\\mbox{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{u,i}^{} \\left( \\hat{y}_{u,i} - y_{u,i} \\right)^2 }\n\\] with \\(N\\) being the number of user/movie combinations and the sum occurring over all these combinations.\nWe can interpret the RMSE similarly to a standard deviation: it is the typical error we make when predicting a movie rating. If this number is larger than 1, it means our typical error is larger than one star, which is not good. In R, we can define a function to compute this quantity like this:\n\nRMSE &lt;- function(true_ratings, predicted_ratings){\n    sqrt(mean((true_ratings - predicted_ratings)^2))\n  }\n\nIn the next two chapters we introduce two concepts, regularization and matrix factorization, that were used by the winners of the Netflix challenge to obtain lowest RMSE."
  },
  {
    "objectID": "highdim/regularization.html#a-first-model",
    "href": "highdim/regularization.html#a-first-model",
    "title": "\n22  Regularization\n",
    "section": "\n22.3 A first model",
    "text": "22.3 A first model\nLet’s start by building the simplest possible recommendation system: we predict the same rating for all movies regardless of user. What number should this prediction be? We can use a model based approach to answer this. A model that assumes the same rating for all movies and users with all the differences explained by random variation would look like this:\n\\[\nY_{u,i} = \\mu + \\varepsilon_{u,i}\n\\]\nwith \\(\\varepsilon_{i,u}\\) independent errors sampled from the same distribution centered at 0 and \\(\\mu\\) the true rating for all movies. We know that the estimate that minimizes the RMSE is the least squares estimate of \\(\\mu\\) and, in this case, is the average of all ratings:\n\nmu &lt;- mean(y, na.rm = TRUE)\nmu\n#&gt; [1] 3.58\n\nIf we predict all unknown ratings with \\(\\hat{\\mu}\\) we obtain the following RMSE:\n\nnaive_rmse &lt;- RMSE(test_set$rating, mu)\nnaive_rmse\n#&gt; [1] 1.05\n\nKeep in mind that if you plug in any other number, you get a higher RMSE. For example:\n\npredictions &lt;- rep(3, nrow(test_set))\nRMSE(test_set$rating, predictions)\n#&gt; [1] 1.19\n\nFrom looking at the distribution of ratings, we can visualize that this is the standard deviation of that distribution. We get a RMSE of about 1. To win the grand prize of $1,000,000, a participating team had to get an RMSE of about 0.857. So we can definitely do better!\nAs we go along, we will be comparing different approaches."
  },
  {
    "objectID": "highdim/regularization.html#modeling-movie-effects",
    "href": "highdim/regularization.html#modeling-movie-effects",
    "title": "\n22  Regularization\n",
    "section": "\n22.4 Modeling movie effects",
    "text": "22.4 Modeling movie effects\nWe know from experience that some movies are just generally rated higher than others. This intuition, that different movies are rated differently, is confirmed by data. We can use a linear models with a treatment effect \\(b_i\\) for each movie, which can be interpreted as movie effect or the difference between the average ranking for movie \\(i\\) and the overall average \\(\\mu\\):\n\\[\nY_{u,i} = \\mu + b_i + \\varepsilon_{u,i}\n\\]\nStatistics textbooks refer to the \\(b\\)s as treatment effects, however, in the Netflix challenge papers, they refer to them as bias, thus the \\(b\\) notation.\nWe can again use least squares to estimate the \\(b_i\\) in the following way:\n\nfit &lt;- lm(rating ~ as.factor(movieId), data = movielens)\n\nBecause there are thousands of \\(b_i\\) as each movie gets one, the lm() function will be very slow here. We therefore don’t recommend running the code above. But in this particular situation, we know that the least squares estimate \\(\\hat{b}_i\\) is just the average of \\(Y_{u,i} - \\hat{\\mu}\\) for each movie \\(i\\). So we can compute them this way (we will drop the hat notation in the code to represent estimates going forward):\n\nb_i &lt;- colMeans(y - mu, na.rm = TRUE)\n\nWe can see that these estimates vary substantially:\n\nhist(b_i)\n\n\n\n\n\n\n\nRemember \\(\\hat{\\mu}=3.5\\) so a \\(b_i = 1.5\\) implies a perfect five star rating.\nLet’s see how much our prediction improves once we use \\(\\hat{y}_{u,i} = \\hat{\\mu} + \\hat{b}_i\\):\n\nfit_movies &lt;- data.frame(movieId = as.integer(colnames(y)), \n                         mu = mu, b_i = b_i)\nleft_join(test_set, fit_movies, by = \"movieId\") |&gt; \n  mutate(pred = mu + b_i) |&gt; \n  summarize(rmse = RMSE(rating, pred))\n#&gt;    rmse\n#&gt; 1 0.991\n\nWe already see an improvement. But can we make it better?"
  },
  {
    "objectID": "highdim/regularization.html#user-effects",
    "href": "highdim/regularization.html#user-effects",
    "title": "\n22  Regularization\n",
    "section": "\n22.5 User effects",
    "text": "22.5 User effects\nLet’s compute the average rating for user \\(u\\) for those that have rated 100 or more movies:\n\nb_u &lt;- rowMeans(y, na.rm = TRUE)\nhist(b_u, nclass = 30)\n\n\n\n\n\n\n\nNotice that there is substantial variability across users as well: some users are very cranky and others love every movie. This implies that a further improvement to our model may be:\n\\[\nY_{u,i} = \\mu + b_i + b_u + \\varepsilon_{u,i}\n\\]\nwhere \\(b_u\\) is a user-specific effect. Now if a cranky user (negative \\(b_u\\)) rates a great movie (positive \\(b_i\\)), the effects counter each other and we may be able to correctly predict that this user gave this great movie a 3 rather than a 5.\nTo fit this model, we could again use lm like this:\n\nlm(rating ~ as.factor(movieId) + as.factor(userId))\n\nbut, for the reasons described earlier, we won’t. Instead, we will compute an approximation by computing \\(\\hat{\\mu}\\) and \\(\\hat{b}_i\\) and estimating \\(\\hat{b}_u\\) as the average of \\(y_{u,i} - \\hat{\\mu} - \\hat{b}_i\\):\n\nb_u &lt;- rowMeans(sweep(y - mu, 2, b_i), na.rm = TRUE)\n\nWe can now construct predictors and see how much the RMSE improves:\n\nfit_users &lt;- data.frame(userId = as.integer(rownames(y)), b_u = b_u)\n\nleft_join(test_set, fit_movies, by = \"movieId\") |&gt; \n  left_join(fit_users, by = \"userId\") |&gt; \n  mutate(pred = mu + b_i + b_u) |&gt; \n  summarize(rmse = RMSE(rating, pred))\n#&gt;   rmse\n#&gt; 1 0.91"
  },
  {
    "objectID": "highdim/regularization.html#penalized-least-squares",
    "href": "highdim/regularization.html#penalized-least-squares",
    "title": "\n22  Regularization\n",
    "section": "\n22.6 Penalized least squares",
    "text": "22.6 Penalized least squares\nLet’s look at the top 3 movies, based on our estimates of the movie effect \\(b_i\\), along with the number of ratings this rating was based on. Several movies get a perfect score. Here are the ones with more than 1 rating:\n\nn &lt;-  colSums(!is.na(y))\nfit_movies$n &lt;- n\nbest &lt;- fit_movies |&gt; left_join(movie_map, by = \"movieId\") |&gt; \n  mutate(average_rating = mu + b_i) |&gt;\n  filter(average_rating == 5 & n &gt; 1) \ntest_set |&gt; \n  group_by(movieId) |&gt;\n  summarize(test_set_averge_rating = mean(rating)) |&gt;\n  right_join(best, by = \"movieId\") |&gt;\n  select(title, average_rating, n, test_set_averge_rating) \n#&gt; # A tibble: 5 × 4\n#&gt;   title                 average_rating     n test_set_averge_rating\n#&gt;   &lt;chr&gt;                          &lt;dbl&gt; &lt;dbl&gt;                  &lt;dbl&gt;\n#&gt; 1 Mother Night                       5     2                    4  \n#&gt; 2 Village of the Damned              5     3                    3.5\n#&gt; 3 Face in the Crowd, A               5     3                    5  \n#&gt; 4 Pawnbroker, The                    5     2                    4  \n#&gt; 5 In a Lonely Place                  5     2                    4.5\n\nThese all seem like obscure movies. Do we really think these are the top 3 movies in our database? Will this prediction hold on the test set? Note that all, except one, are lower in the test, some considerable lower.\nThese supposed best movies were rated by very few users and small sample sizes lead to uncertainty. Therefore, larger estimates of \\(b_i\\), negative or positive, are more likely. Therefore, these are noisy estimates that we should not trust, especially when it comes to prediction. Large errors can increase our RMSE, so we would rather be conservative when unsure.\nIn previous sections, we computed standard error and constructed confidence intervals to account for different levels of uncertainty. However, when making predictions, we need one number, one prediction, not an interval. For this, we introduce the concept of regularization.\nRegularization permits us to penalize large estimates that are formed using small sample sizes. It has commonalities with the Bayesian approach that shrunk predictions described in Section Chapter 11.\nThe general idea behind regularization is to constrain the total variability of the effect sizes. Why does this help? Consider a case in which we have movie \\(i=1\\) with 100 user ratings and 4 movies \\(i=2,3,4,5\\) with just one user rating. We intend to fit the model\n\\[\nY_{u,i} = \\mu + b_i + \\varepsilon_{u,i}\n\\]\nSuppose we know the average rating is, say, \\(\\mu = 3\\). If we use least squares, the estimate for the first movie effect \\(b_1\\) is the average of the 100 user ratings, \\(1/100 \\sum_{i=1}^{100} (Y_{i,1} - \\mu)\\), which we expect to be a quite precise. However, the estimate for movies 2, 3, 4, and 5 will simply be the observed deviation from the average rating \\(\\hat{b}_i = Y_{u,i} - \\hat{\\mu}\\) which is an estimate based on just one number so it won’t be precise at all. Note these estimates make the error \\(Y_{u,i} - \\mu + \\hat{b}_i\\) equal to 0 for \\(i=2,3,4,5\\), but this is a case of over-training. In fact, ignoring the one user and guessing that movies 2,3,4, and 5 are just average movies (\\(b_i = 0\\)) might provide a better prediction. The general idea of penalized regression is to control the total variability of the movie effects: \\(\\sum_{i=1}^5 b_i^2\\). Specifically, instead of minimizing the least squares equation, we minimize an equation that adds a penalty:\n\\[ \\sum_{u,i} \\left(y_{u,i} - \\mu - b_i\\right)^2 + \\lambda \\sum_{i} b_i^2 \\] The first term is just the sum of squares and the second is a penalty that gets larger when many \\(b_i\\) are large. Using calculus we can actually show that the values of \\(b_i\\) that minimize this equation are:\n\\[\n\\hat{b}_i(\\lambda) = \\frac{1}{\\lambda + n_i} \\sum_{u=1}^{n_i} \\left(Y_{u,i} - \\hat{\\mu}\\right)\n\\]\nwhere \\(n_i\\) is the number of ratings made for movie \\(i\\). This approach will have our desired effect: when our sample size \\(n_i\\) is very large, a case which will give us a stable estimate, then the penalty \\(\\lambda\\) is effectively ignored since \\(n_i+\\lambda \\approx n_i\\). However, when the \\(n_i\\) is small, then the estimate \\(\\hat{b}_i(\\lambda)\\) is shrunken towards 0. The larger \\(\\lambda\\), the more we shrink.\nTo select \\(\\lambda\\), we can use cross validation:\n\nlambdas &lt;- seq(0, 10, 0.1)\n\nsums &lt;- colSums(y - mu, na.rm = TRUE)\nrmses &lt;- sapply(lambdas, function(lambda){\n  b_i &lt;-  sums / (n + lambda)\n  fit_movies$b_i &lt;- b_i\n  left_join(test_set, fit_movies, by = \"movieId\") |&gt; mutate(pred = mu + b_i) |&gt; \n    summarize(rmse = RMSE(rating, pred)) |&gt;\n    pull(rmse)\n})\n\nWe can then select the value that minimizes the RMSE:\n\nplot(lambdas, rmses, type = \"l\")\nlambda &lt;- lambdas[which.min(rmses)]\nprint(lambda)\n#&gt; [1] 3.1\n\n\n\n\n\n\n\nOnce we select a \\(\\lambda\\) we can compute the regularized estimates and add to our table of estimates:\n\nfit_movies$b_i_reg &lt;- colSums(y - mu, na.rm = TRUE) / (n + lambda)\n\nTo see how the estimates shrink, let’s make a plot of the regularized estimates versus the least squares estimates.\n\n\n\n\n\n\n\n\nNow, let’s look at the top 5 best movies based on the penalized estimates \\(\\hat{b}_i(\\lambda)\\):\n\n#&gt; # A tibble: 5 × 4\n#&gt;   title                     average_rating     n test_set_averge_rating\n#&gt;   &lt;chr&gt;                              &lt;dbl&gt; &lt;dbl&gt;                  &lt;dbl&gt;\n#&gt; 1 Shawshank Redemption, The           4.49   244                   4.43\n#&gt; 2 Godfather, The                      4.47   163                   4.5 \n#&gt; 3 Thin Man, The                       4.40    10                   3.57\n#&gt; 4 African Queen, The                  4.38    37                   4.35\n#&gt; 5 Roger & Me                          4.37    35                   4.14\n\nThese make much more sense! These movies are watched more and have more ratings.\nDo we improve our results? Let’s estimate the user effects with the new movie effect estimates and compute the new RMSE:\n\nfit_users$b_u &lt;- rowMeans(sweep(y - mu, 2, b_i), na.rm = TRUE)\nleft_join(test_set, fit_movies, by = \"movieId\") |&gt; \n  left_join(fit_users, by = \"userId\") |&gt; \n  mutate(pred = mu + b_i_reg + b_u) |&gt; \n  summarize(rmse = RMSE(rating, pred))\n#&gt;    rmse\n#&gt; 1 0.887\n\nThe penalized estimates provide an improvement over the least squares estimates:\n\n\n\n\nmethod\nRMSE\n\n\n\nJust the average\n1.051\n\n\nMovie Effect Model\n0.991\n\n\nMovie + User Effects Model\n0.910\n\n\nRegularized Movie + User Effect Model\n0.887"
  },
  {
    "objectID": "highdim/regularization.html#exercises",
    "href": "highdim/regularization.html#exercises",
    "title": "\n22  Regularization\n",
    "section": "\n22.7 Exercises",
    "text": "22.7 Exercises\n1. For the movielens data, compute the number of ratings for each movie and then plot it against the year the movie came out. Use the square root transformation on the counts.\n2. We see that, on average, movies that came out after 1993 get more ratings. We also see that with newer movies, starting in 1993, the number of ratings decreases with year: the more recent a movie is, the less time users have had to rate it.\nAmong movies that came out in 1993 or later, what are the 25 movies with the most ratings per year? Also report their average rating.\n3. From the table constructed in the previous example, we see that the most rated movies tend to have above average ratings. This is not surprising: more people watch popular movies. To confirm this, stratify the post 1993 movies by ratings per year and compute their average ratings. Make a plot of average rating versus ratings per year and show an estimate of the trend.\n4. In the previous exercise, we see that the more a movie is rated, the higher the rating. Suppose you are doing a predictive analysis in which you need to fill in the missing ratings with some value. Which of the following strategies would you use?\n\nFill in the missing values with average rating of all movies.\nFill in the missing values with 0.\nFill in the value with a lower value than the average since lack of rating is associated with lower ratings. Try out different values and evaluate prediction in a test set.\nNone of the above.\n\n5. The movielens dataset also includes a time stamp. This variable represents the time and data in which the rating was provided. The units are seconds since January 1, 1970. Create a new column date with the date. Hint: use the as_datetime function in the lubridate package.\n6. Compute the average rating for each week and plot this average against day. Hint: use the round_date function before you group_by.\n7. The plot shows some evidence of a time effect. If we define \\(d_{u,i}\\) as the day for user’s \\(u\\) rating of movie \\(i\\), which of the following models is most appropriate:\n\n\n\\(Y_{u,i} = \\mu + b_i + b_u + d_{u,i} + \\varepsilon_{u,i}\\).\n\n\\(Y_{u,i} = \\mu + b_i + b_u + d_{u,i}\\beta + \\varepsilon_{u,i}\\).\n\n\\(Y_{u,i} = \\mu + b_i + b_u + d_{u,i}\\beta_i + \\varepsilon_{u,i}\\).\n\n\\(Y_{u,i} = \\mu + b_i + b_u + f(d_{u,i}) + \\varepsilon_{u,i}\\), with \\(f\\) a smooth function of \\(d_{u,i}\\).\n\n8. The movielens data also has a genres column. This column includes every genre that applies to the movie. Some movies fall under several genres. Define a category as whatever combination appears in this column. Keep only categories with more than 1,000 ratings. Then compute the average and standard error for each category. Plot these as error bar plots.\n9. The plot shows strong evidence of a genre effect. If we define \\(g_{u,i}\\) as the genre for user’s \\(u\\) rating of movie \\(i\\), which of the following models is most appropriate:\n\n\n\\(Y_{u,i} = \\mu + b_i + b_u + d_{u,i} + \\varepsilon_{u,i}\\).\n\n\\(Y_{u,i} = \\mu + b_i + b_u + d_{u,i}\\beta + \\varepsilon_{u,i}\\).\n\n\\(Y_{u,i} = \\mu + b_i + b_u + \\sum_{k=1}^K x_{u,i} \\beta_k + \\varepsilon_{u,i}\\), with \\(x^k_{u,i} = 1\\) if \\(g_{u,i}\\) is genre \\(k\\).\n\n\\(Y_{u,i} = \\mu + b_i + b_u + f(d_{u,i}) + \\varepsilon_{u,i}\\), with \\(f\\) a smooth function of \\(d_{u,i}\\).\n\nAn education expert is advocating for smaller schools. The expert bases this recommendation on the fact that among the best performing schools, many are small schools. Let’s simulate a dataset for 100 schools. First, let’s simulate the number of students in each school.\n\nset.seed(1986)\nn &lt;- round(2^rnorm(1000, 8, 1))\n\nNow let’s assign a true quality for each school completely independent from size. This is the parameter we want to estimate.\n\nmu &lt;- round(80 + 2 * rt(1000, 5))\nrange(mu)\nschools &lt;- data.frame(id = paste(\"PS\",1:100), \n                      size = n, \n                      quality = mu,\n                      rank = rank(-mu))\n\nWe can see that the top 10 schools are:\n\nschools |&gt; top_n(10, quality) |&gt; arrange(desc(quality))\n\nNow let’s have the students in the school take a test. There is random variability in test taking so we will simulate the test scores as normally distributed with the average determined by the school quality and standard deviations of 30 percentage points:\n\nscores &lt;- sapply(1:nrow(schools), function(i){\n  scores &lt;- rnorm(schools$size[i], schools$quality[i], 30)\n  scores\n})\nschools &lt;- schools |&gt; mutate(score = sapply(scores, mean))\n\n10. What are the top schools based on the average score? Show just the ID, size, and the average score.\n11. Compare the median school size to the median school size of the top 10 schools based on the score.\n12. According to this test, it appears small schools are better than large schools. Five out of the top 10 schools have 100 or fewer students. But how can this be? We constructed the simulation so that quality and size are independent. Repeat the exercise for the worst 10 schools.\n13. The same is true for the worst schools! They are small as well. Plot the average score versus school size to see what’s going on. Highlight the top 10 schools based on the true quality. Use the log scale transform for the size.\n14. We can see that the standard error of the score has larger variability when the school is smaller. This is a basic statistical reality we learned in the probability and inference sections. In fact, note that 4 of the top 10 schools are in the top 10 schools based on the exam score.\nLet’s use regularization to pick the best schools. Remember regularization shrinks deviations from the average towards 0. So to apply regularization here, we first need to define the overall average for all schools:\n\noverall &lt;- mean(sapply(scores, mean))\n\nand then define, for each school, how it deviates from that average. Write code that estimates the score above average for each school but dividing by \\(n + \\lambda\\) instead of \\(n\\), with \\(n\\) the school size and \\(\\lambda\\) a regularization parameter. Try \\(\\lambda = 3\\).\n15. Notice that this improves things a bit. The number of small schools that are not highly ranked is now 4. Is there a better \\(\\lambda\\)? Find the \\(\\lambda\\) that minimizes the RMSE = \\(1/100 \\sum_{i=1}^{100} (\\mbox{quality} - \\mbox{estimate})^2\\).\n16. Rank the schools based on the average obtained with the best \\(\\alpha\\). Note that no small school is incorrectly included.\n17. A common mistake to make when using regularization is shrinking values towards 0 that are not centered around 0. For example, if we don’t subtract the overall average before shrinking, we actually obtain a very similar result. Confirm this by re-running the code from exercise 6 but without removing the overall mean."
  },
  {
    "objectID": "highdim/regularization.html#footnotes",
    "href": "highdim/regularization.html#footnotes",
    "title": "\n22  Regularization\n",
    "section": "",
    "text": "http://bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/↩︎\nhttps://grouplens.org/↩︎"
  },
  {
    "objectID": "highdim/matrix-factorization.html#sec-factor-analysis",
    "href": "highdim/matrix-factorization.html#sec-factor-analysis",
    "title": "\n23  Matrix factorization\n",
    "section": "\n23.1 Factor analysis",
    "text": "23.1 Factor analysis\nHere is an illustration, using a simulation, of how we can use some structure to predict the \\(r_{u,i}\\). Suppose our residuals r look like this:\n\nround(r, 1)\n#&gt;    Godfather Godfather2 Goodfellas You've Got Sleepless\n#&gt; 1        2.1        2.5        2.4       -1.6      -1.7\n#&gt; 2        1.9        1.4        2.0       -1.8      -1.3\n#&gt; 3        1.8        2.7        2.3       -2.7      -2.0\n#&gt; 4       -0.5        0.7        0.6       -0.8      -0.5\n#&gt; 5       -0.6       -0.8        0.6        0.4       0.6\n#&gt; 6       -0.1        0.2        0.5       -0.7       0.4\n#&gt; 7       -0.3       -0.1       -0.4       -0.4       0.7\n#&gt; 8        0.3        0.4        0.3        0.0       0.7\n#&gt; 9       -1.4       -2.2       -1.5        2.0       2.8\n#&gt; 10      -2.6       -1.5       -1.3        1.6       1.3\n#&gt; 11      -1.5       -2.0       -2.2        1.7       2.7\n#&gt; 12      -1.5       -1.4       -2.3        2.5       2.0\n\nThere seems to be a pattern here. In fact, we can see very strong correlation patterns:\n\ncor(r) \n#&gt;            Godfather Godfather2 Goodfellas You've Got Sleepless\n#&gt; Godfather      1.000      0.923      0.911     -0.898    -0.863\n#&gt; Godfather2     0.923      1.000      0.937     -0.950    -0.969\n#&gt; Goodfellas     0.911      0.937      1.000     -0.949    -0.956\n#&gt; You've Got    -0.898     -0.950     -0.949      1.000     0.945\n#&gt; Sleepless     -0.863     -0.969     -0.956      0.945     1.000\n\nWe can create vectors q and p, that can explain much of the structure we see. The q would look like this:\n\nt(q) \n#&gt;      Godfather Godfather2 Goodfellas You've Got Sleepless\n#&gt; [1,]         1          1          1         -1        -1\n\nand it narrows down movies to two groups: gangster (coded with 1) and romance (coded with -1). We can also reduce the users to three groups:\n\nt(p)\n#&gt;      1 2 3 4 5 6 7 8  9 10 11 12\n#&gt; [1,] 2 2 2 0 0 0 0 0 -2 -2 -2 -2\n\nthose that like gangster movies and dislike romance movies (coded as 2), those that like romance movies and dislike gangster movies (coded as -2), and those that don’t care (coded as 0). The main point here is that we can almost reconstruct \\(r\\), which has 60 values, with a couple of vectors totaling 17 values. Note that p and q are equivalent to the patterns and weights we described in Section Section 21.5.\nIf \\(r\\) contains the residuals for users \\(u=1,\\dots,12\\) for movies \\(i=1,\\dots,5\\) we can write the following mathematical formula for our residuals \\(r_{u,i}\\).\n\\[\nr_{u,i} \\approx p_u q_i\n\\]\nThis implies that we can explain more variability by modifying our previous model for movie recommendations to:\n\\[\nY_{u,i} = \\mu + b_i + b_u + p_u q_i + \\varepsilon_{u,i}\n\\]\nHowever, we motivated the need for the \\(p_u q_i\\) term with a simple simulation. The structure found in data is usually more complex. For example, in this first simulation we assumed there were was just one factor \\(p_u\\) that determined which of the two genres movie \\(u\\) belongs to. But the structure in our movie data seems to be much more complicated than gangster movie versus romance. We may have many other factors. Here we present a slightly more complex simulation. We now add a sixth movie, Scent of Woman.\n\nround(r, 1)\n#&gt;    Godfather Godfather2 Goodfellas You've Got Sleepless Scent\n#&gt; 1        0.0        0.3        2.2        0.2       0.1  -2.3\n#&gt; 2        2.0        1.7        0.0       -1.9      -1.7   0.3\n#&gt; 3        1.9        2.4        0.1       -2.3      -2.0   0.0\n#&gt; 4       -0.3        0.3        0.3       -0.4      -0.3   0.3\n#&gt; 5       -0.3       -0.4        0.3        0.2       0.3  -0.3\n#&gt; 6        0.9        1.1       -0.8       -1.3      -0.8   1.2\n#&gt; 7        0.9        1.0       -1.2       -1.2      -0.7   0.7\n#&gt; 8        1.2        1.2       -0.9       -1.0      -0.6   0.8\n#&gt; 9       -0.7       -1.1       -0.8        1.0       1.4   0.7\n#&gt; 10      -2.3       -1.8        0.3        1.8       1.7  -0.1\n#&gt; 11      -1.7       -2.0       -0.1        1.9       2.3   0.2\n#&gt; 12      -1.8       -1.7       -0.1        2.3       2.0   0.4\n\nBy exploring the correlation structure of this new dataset\n\n#&gt;            Godfather Godfather2 Goodfellas    YGM      SS      SW\n#&gt; Godfather      1.000     0.9760    -0.1748 -0.973 -0.9588  0.1299\n#&gt; Godfather2     0.976     1.0000    -0.1051 -0.986 -0.9903  0.0876\n#&gt; Goodfellas    -0.175    -0.1051     1.0000  0.180  0.0801 -0.9426\n#&gt; YGM           -0.973    -0.9864     0.1799  1.000  0.9868 -0.1632\n#&gt; SS            -0.959    -0.9903     0.0801  0.987  1.0000 -0.0817\n#&gt; SW             0.130     0.0876    -0.9426 -0.163 -0.0817  1.0000\n\nwe note that perhaps we need a second factor to account for the fact that some users like Al Pacino, while others dislike him or don’t care. Notice that the overall structure of the correlation obtained from the simulated data is not that far off the real correlation:\n\n#&gt;            Godfather Godfather2 Goodfellas    YGM       SS     SW\n#&gt; Godfather      1.000    0.82696      0.438 -0.285 -0.10748  0.362\n#&gt; Godfather2     0.827    1.00000      0.574 -0.268 -0.00675  0.340\n#&gt; Goodfellas     0.438    0.57445      1.000 -0.293 -0.27153  0.278\n#&gt; YGM           -0.285   -0.26767     -0.293  1.000  0.53617 -0.289\n#&gt; SS            -0.107   -0.00675     -0.272  0.536  1.00000 -0.307\n#&gt; SW             0.362    0.34008      0.278 -0.289 -0.30732  1.000\n\nTo explain this more complicated structure, we need two factors. For example something like this:\n\nt(q) \n#&gt;      Godfather Godfather2 Goodfellas You've Got Sleepless Scent\n#&gt; [1,]         1          1          1         -1        -1    -1\n#&gt; [2,]         1          1         -1         -1        -1     1\n\nWith the first factor (the first column of q) used to code the gangster versus romance groups and a second factor (the second column of q) to explain the Al Pacino versus no Al Pacino groups. We will also need two sets of coefficients to explain the variability introduced by the \\(3\\times 3\\) types of groups:\n\nt(p)\n#&gt;       1 2 3 4 5 6 7 8  9 10 11 12\n#&gt; [1,]  1 1 1 0 0 0 0 0 -1 -1 -1 -1\n#&gt; [2,] -1 1 1 0 0 1 1 1  0 -1 -1 -1\n\nThe model with two factors has 36 parameters that can be used to explain much of the variability in the 72 ratings:\n\\[\nY_{u,i} = \\mu + b_i + b_u + p_{u,1} q_{1,i} + p_{u,2} q_{2,i} + \\varepsilon_{u,i}\n\\]\nNote that in an actual data application, we need to fit this model to data. To explain the complex correlation we observe in real data, we usually permit the entries of \\(p\\) and \\(q\\) to be continuous values, rather than discrete ones as we used in the simulation. For example, rather than dividing movies into gangster or romance, we define a continuum. Also note that this is not a linear model and to fit it we need to use an algorithm other than the one used by lm to find the parameters that minimize the least squares. The winning algorithms for the Netflix challenge fit a model similar to the above and used regularization to penalize for large values of \\(p\\) and \\(q\\), rather than using least squares. Implementing this approach is beyond the scope of this book."
  },
  {
    "objectID": "highdim/matrix-factorization.html#connection-to-svd-and-pca",
    "href": "highdim/matrix-factorization.html#connection-to-svd-and-pca",
    "title": "\n23  Matrix factorization\n",
    "section": "\n23.2 Connection to SVD and PCA",
    "text": "23.2 Connection to SVD and PCA\nThe decomposition:\n\\[\nr_{u,i} \\approx p_{u,1} q_{1,i} + p_{u,2} q_{2,i}\n\\]\nis very much related to SVD and PCA. SVD and PCA are complicated concepts, but one way to understand them is that SVD is an algorithm that finds the vectors \\(p\\) and \\(q\\) that permit us to rewrite the matrix \\(\\mbox{r}\\) with \\(m\\) rows and \\(n\\) columns as:\n\\[\nr_{u,i} = p_{u,1} q_{1,i} + p_{u,2} q_{2,i} + \\dots + p_{u,n} q_{n,i}\n\\]\nwith the variability of each term decreasing and with the \\(p\\)s uncorrelated. The algorithm also computes this variability so that we can know how much of the matrices, total variability is explained as we add new terms. This may permit us to see that, with just a few terms, we can explain most of the variability. To illustrate this we will only consider a small subset of movies with many ratings and users that have rated many movies:\n\nkeep &lt;- c(\"Godfather, The\", \"Godfather: Part II, The\", \"Goodfellas\", \"Ghost\", \"Titanic\", \n          \"Scent of a Woman\")\ndat &lt;- movielens  |&gt; \n  group_by(userId) |&gt;\n  filter(n() &gt;= 250) |&gt; \n  ungroup() |&gt;\n  group_by(movieId) |&gt;\n  filter(n() &gt;= 50 | title %in% keep) |&gt; \n  ungroup() \n\ny &lt;- select(dat, movieId, userId, rating) |&gt;\n  pivot_wider(names_from = movieId, values_from = rating) \ny &lt;- as.matrix(y[,-1])\n\ncolnames(y) &lt;- dat |&gt; select(movieId, title) |&gt; \n  distinct(movieId, .keep_all = TRUE) |&gt;\n  right_join(data.frame(movieId = as.integer(colnames(y))), by = \"movieId\") |&gt;\n  pull(title)\n\nWe first remove the overall movie and user effects as we are interested in the variability not explained by these. We start by removing the movie effects:\n\nr &lt;- sweep(y, 2, colMeans(y, na.rm = TRUE))\n\nBecause for the techniques shown here we can’t have missing values we need to replace the missing ratings. There are advanced techniques for doing this, some are explained in the description of the winning entry for the Netflix competition. Here we will use a simple approach: replace with a constant. Now because an unrated movie is more likely to be a movie the user does not want to see, we will replace the missing ratings with -1 rather than a 0, which represents a neutral rating.\n\nr[is.na(r)] &lt;- -1\n\nFinally we will remove the overall user effect:\n\nr &lt;- r - rowMeans(r)\n\nNow we can perform principal component analysis:\n\npca &lt;- prcomp(r)\n\nThe \\(q\\) vectors are called the principal components and they are stored in this matrix:\n\ndim(pca$rotation)\n#&gt; [1] 138 105\n\nWhile the \\(p\\), or the user effects, are here:\n\ndim(pca$x)\n#&gt; [1] 105 105\n\nWe can see the variability of each of the vectors:\n\nqplot(1:nrow(pca$x), pca$sdev, xlab = \"PC\")\n#&gt; Warning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\nWe also notice that the first two principal components are related to the structure in opinions about movies:\n\n\n\n\n\n\n\n\nJust by looking at the top 10 in each direction, we see a meaningful patterns. The first PC shows the difference between Hollywood blockbusters on one side:\n\n#&gt;  [1] \"Independence Day (a.k.a. ID4)\"  \"Armageddon\"                    \n#&gt;  [3] \"Spider-Man\"                     \"Mummy, The\"                    \n#&gt;  [5] \"Aladdin\"                        \"Lion King, The\"                \n#&gt;  [7] \"Harry Potter and the Sorcer...\" \"Twister\"                       \n#&gt;  [9] \"X-Men\"                          \"Lord of the Rings: The Retu...\"\n\nand critically acclaimed movies on the other:\n\n#&gt;  [1] \"2001: A Space Odyssey\"          \"Apocalypse Now\"                \n#&gt;  [3] \"Fargo\"                          \"Being John Malkovich\"          \n#&gt;  [5] \"One Flew Over the Cuckoo's ...\" \"Clockwork Orange, A\"           \n#&gt;  [7] \"Blade Runner\"                   \"Shining, The\"                  \n#&gt;  [9] \"Godfather, The\"                 \"Big Lebowski, The\"\n\nWhile the second PC seems to be related to nerd favorites or violent movies on one side\n\n#&gt;  [1] \"Fight Club\"                     \"Lord of the Rings: The Two ...\"\n#&gt;  [3] \"Lord of the Rings: The Retu...\" \"Matrix, The\"                   \n#&gt;  [5] \"X-Men\"                          \"Lord of the Rings: The Fell...\"\n#&gt;  [7] \"Kill Bill: Vol. 2\"              \"Léon: The Professional (a.k...\"\n#&gt;  [9] \"Kill Bill: Vol. 1\"              \"Memento\"\n\nand romantic movies on the other:\n\n#&gt;  [1] \"Babe\"                 \"Grease\"              \n#&gt;  [3] \"Sleepless in Seattle\" \"Beauty and the Beast\"\n#&gt;  [5] \"Ghost\"                \"Jerry Maguire\"       \n#&gt;  [7] \"Pretty Woman\"         \"Titanic\"             \n#&gt;  [9] \"Aladdin\"              \"Big\"\n\nFitting a model that incorporates these estimates is complicated. For those interested in implementing an approach that incorporates these ideas, we recommend trying the recommenderlab package. The details are beyond the scope of this book."
  },
  {
    "objectID": "highdim/matrix-factorization.html#exercises",
    "href": "highdim/matrix-factorization.html#exercises",
    "title": "\n23  Matrix factorization\n",
    "section": "\n23.3 Exercises",
    "text": "23.3 Exercises\nIn this exercise set, we will be covering a topic useful for understanding matrix factorization: the singular value decomposition (SVD). SVD is a mathematical result that is widely used in machine learning, both in practice and to understand the mathematical properties of some algorithms. This is a rather advanced topic and to complete this exercise set you will have to be familiar with linear algebra concepts such as matrix multiplication, orthogonal matrices, and diagonal matrices.\nThe SVD tells us that we can decompose an \\(N\\times p\\) matrix \\(Y\\) with \\(p &lt; N\\) as\n\\[ Y = U D V^{\\top} \\]\nWith \\(U\\) and \\(V\\) orthogonal of dimensions \\(N\\times p\\) and \\(p\\times p\\), respectively, and \\(D\\) a \\(p \\times p\\) diagonal matrix with the values of the diagonal decreasing:\n\\[d_{1,1} \\geq d_{2,2} \\geq \\dots d_{p,p}.\\]\nIn this exercise, we will see one of the ways that this decomposition can be useful. To do this, we will construct a dataset that represents grade scores for 100 students in 24 different subjects. The overall average has been removed so this data represents the percentage point each student received above or below the average test score. So a 0 represents an average grade (C), a 25 is a high grade (A+), and a -25 represents a low grade (F). You can simulate the data like this:\n\nset.seed(1987)\nn &lt;- 100\nk &lt;- 8\nSigma &lt;- 64  * matrix(c(1, .75, .5, .75, 1, .5, .5, .5, 1), 3, 3) \nm &lt;- MASS::mvrnorm(n, rep(0, 3), Sigma)\nm &lt;- m[order(rowMeans(m), decreasing = TRUE),]\ny &lt;- m %x% matrix(rep(1, k), nrow = 1) +\n  matrix(rnorm(matrix(n * k * 3)), n, k * 3)\ncolnames(y) &lt;- c(paste(rep(\"Math\",k), 1:k, sep=\"_\"),\n                 paste(rep(\"Science\",k), 1:k, sep=\"_\"),\n                 paste(rep(\"Arts\",k), 1:k, sep=\"_\"))\n\nOur goal is to describe the student performances as succinctly as possible. For example, we want to know if these test results are all just random independent numbers. Are all students just about as good? Does being good in one subject imply you will be good in another? How does the SVD help with all this? We will go step by step to show that with just three relatively small pairs of vectors we can explain much of the variability in this \\(100 \\times 24\\) dataset.\nYou can visualize the 24 test scores for the 100 students by plotting an image:\n\nmy_image &lt;- function(x, zlim = range(x), ...){\n  colors = rev(RColorBrewer::brewer.pal(9, \"RdBu\"))\n  cols &lt;- 1:ncol(x)\n  rows &lt;- 1:nrow(x)\n  image(cols, rows, t(x[rev(rows),,drop=FALSE]), xaxt = \"n\", yaxt = \"n\",\n        xlab=\"\", ylab=\"\",  col = colors, zlim = zlim, ...)\n  abline(h=rows + 0.5, v = cols + 0.5)\n  axis(side = 1, cols, colnames(x), las = 2)\n}\n\nmy_image(y)\n\n1. How would you describe the data based on this figure?\n\nThe test scores are all independent of each other.\nThe students that test well are at the top of the image and there seem to be three groupings by subject.\nThe students that are good at math are not good at science.\nThe students that are good at math are not good at humanities.\n\n2. You can examine the correlation between the test scores directly like this:\n\nmy_image(cor(y), zlim = c(-1,1))\nrange(cor(y))\naxis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)\n\nWhich of the following best describes what you see?\n\nThe test scores are independent.\nMath and science are highly correlated but the humanities are not.\nThere is high correlation between tests in the same subject but no correlation across subjects.\nThere is a correlation among all tests, but higher if the tests are in science and math and even higher within each subject.\n\n3. Remember that orthogonality means that \\(U^{\\top}U\\) and \\(V^{\\top}V\\) are equal to the identity matrix. This implies that we can also rewrite the decomposition as\n\\[ Y V = U D \\mbox{ or } U^{\\top}Y = D V^{\\top}\\]\nWe can think of \\(YV\\) and \\(U^{\\top}V\\) as two transformations of Y that preserve the total variability of \\(Y\\) since \\(U\\) and \\(V\\) are orthogonal.\nUse the function svd to compute the SVD of y. This function will return \\(U\\), \\(V\\) and the diagonal entries of \\(D\\).\n\ns &lt;- svd(y)\nnames(s)\n\nYou can check that the SVD works by typing:\n\ny_svd &lt;- s$u %*% diag(s$d) %*% t(s$v)\nmax(abs(y - y_svd))\n\nCompute the sum of squares of the columns of \\(Y\\) and store them in ss_y. Then compute the sum of squares of columns of the transformed \\(YV\\) and store them in ss_yv. Confirm that sum(ss_y) is equal to sum(ss_yv).\n4. We see that the total sum of squares is preserved. This is because \\(V\\) is orthogonal. Now to start understanding how \\(YV\\) is useful, plot ss_y against the column number and then do the same for ss_yv. What do you observe?\n5. We see that the variability of the columns of \\(YV\\) is decreasing. Furthermore, we see that, relative to the first three, the variability of the columns beyond the third is almost 0. Now notice that we didn’t have to compute ss_yv because we already have the answer. How? Remember that \\(YV = UD\\) and because \\(U\\) is orthogonal, we know that the sum of squares of the columns of \\(UD\\) are the diagonal entries of \\(D\\) squared. Confirm this by plotting the square root of ss_yv versus the diagonal entries of \\(D\\).\n6. From the above we know that the sum of squares of the columns of \\(Y\\) (the total sum of squares) add up to the sum of s$d^2 and that the transformation \\(YV\\) gives us columns with sums of squares equal to s$d^2. Now compute what percent of the total variability is explained by just the first three columns of \\(YV\\).\n7. We see that almost 99% of the variability is explained by the first three columns of \\(YV = UD\\). So we get the sense that we should be able to explain much of the variability and structure we found while exploring the data with a few columns. Before we continue, let’s show a useful computational trick to avoid creating the matrix diag(s$d). To motivate this, we note that if we write \\(U\\) out in its columns \\([U_1, U_2, \\dots, U_p]\\) then \\(UD\\) is equal to\n\\[UD = [U_1 d_{1,1}, U_2 d_{2,2}, \\dots, U_p d_{p,p}]\\]\nUse the sweep function to compute \\(UD\\) without constructing diag(s$d) nor matrix multiplication.\n8. We know that \\(U_1 d_{1,1}\\), the first column of \\(UD\\), has the most variability of all the columns of \\(UD\\). Earlier we saw an image of \\(Y\\):\n\nmy_image(y)\n\nin which we can see that the student to student variability is quite large and that it appears that students that are good in one subject are good in all. This implies that the average (across all subjects) for each student should explain a lot of the variability. Compute the average score for each student and plot it against \\(U_1 d_{1,1}\\) and describe what you find.\n9. We note that the signs in SVD are arbitrary because:\n\\[ U D V^{\\top} = (-U) D (-V)^{\\top} \\]\nWith this in mind we see that the first column of \\(UD\\) is almost identical to the average score for each student except for the sign.\nThis implies that multiplying \\(Y\\) by the first column of \\(V\\) must be performing a similar operation to taking the average. Make an image plot of \\(V\\) and describe the first column relative to others and how this relates to taking an average.\n10. We already saw that we can rewrite \\(UD\\) as\n\\[U_1 d_{1,1} + U_2 d_{2,2} + \\dots + U_p d_{p,p}\\]\nwith \\(U_j\\) the j-th column of \\(U\\). This implies that we can rewrite the entire SVD as:\n\\[Y = U_1 d_{1,1} V_1 ^{\\top} + U_2 d_{2,2} V_2 ^{\\top} + \\dots + U_p d_{p,p} V_p ^{\\top}\\]\nwith \\(V_j\\) the jth column of \\(V\\). Plot \\(U_1\\), then plot \\(V_1^{\\top}\\) using the same range for the y-axis limits, then make an image of \\(U_1 d_{1,1} V_1 ^{\\top}\\) and compare it to the image of \\(Y\\). Hint: use the my_image function defined above and use the drop=FALSE argument to assure the subsets of matrices are matrices.\n11. We see that with just a vector of length 100, a scalar, and a vector of length 24, we actually come close to reconstructing the original \\(100 \\times 24\\) matrix. This is our first matrix factorization:\n\\[ Y \\approx d_{1,1} U_1 V_1^{\\top}\\] We know it explains s$d[1]^2/sum(s$d^2) * 100 percent of the total variability. Our approximation only explains the observation that good students tend to be good in all subjects. But another aspect of the original data that our approximation does not explain was the higher similarity we observed within subjects. We can see this by computing the difference between our approximation and original data and then computing the correlations. You can see this by running this code:\n\nresid &lt;- y - with(s,(u[,1, drop=FALSE]*d[1]) %*% t(v[,1, drop=FALSE]))\nmy_image(cor(resid), zlim = c(-1,1))\naxis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)\n\nNow that we have removed the overall student effect, the correlation plot reveals that we have not yet explained the within subject correlation nor the fact that math and science are closer to each other than to the arts. So let’s explore the second column of the SVD. Repeat the previous exercise but for the second column: Plot \\(U_2\\), then plot \\(V_2^{\\top}\\) using the same range for the y-axis limits, then make an image of \\(U_2 d_{2,2} V_2 ^{\\top}\\) and compare it to the image of resid.\n12. The second column clearly relates to a student’s difference in ability in math/science versus the arts. We can see this most clearly from the plot of s$v[,2]. Adding the matrix we obtain with these two columns will help with our approximation:\n\\[ Y \\approx d_{1,1} U_1 V_1^{\\top} + d_{2,2} U_2 V_2^{\\top} \\]\nWe know it will explain\n\nsum(s$d[1:2]^2)/sum(s$d^2) * 100\n\npercent of the total variability. We can compute new residuals like this:\n\nresid &lt;- y - with(s,sweep(u[,1:2], 2, d[1:2], FUN=\"*\") %*% t(v[,1:2]))\nmy_image(cor(resid), zlim = c(-1,1))\naxis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)\n\nand see that the structure that is left is driven by the differences between math and science. Confirm this by plotting \\(U_3\\), then plot \\(V_3^{\\top}\\) using the same range for the y-axis limits, then make an image of \\(U_3 d_{3,3} V_3 ^{\\top}\\) and compare it to the image of resid.\n13. The third column clearly relates to a student’s difference in ability in math and science. We can see this most clearly from the plot of s$v[,3]. Adding the matrix we obtain with these two columns will help with our approximation:\n\\[ Y \\approx d_{1,1} U_1 V_1^{\\top} + d_{2,2} U_2 V_2^{\\top} + d_{3,3} U_3 V_3^{\\top}\\]\nWe know it will explain:\n\nsum(s$d[1:3]^2)/sum(s$d^2) * 100\n\npercent of the total variability. We can compute new residuals like this:\n\nresid &lt;- y - with(s,sweep(u[,1:3], 2, d[1:3], FUN=\"*\") %*% t(v[,1:3]))\nmy_image(cor(resid), zlim = c(-1,1))\naxis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)\n\nWe no longer see structure in the residuals: they seem to be independent of each other. This implies that we can describe the data with the following model:\n\\[ Y =  d_{1,1} U_1 V_1^{\\top} + d_{2,2} U_2 V_2^{\\top} + d_{3,3} U_3 V_3^{\\top} + \\varepsilon\\]\nwith \\(\\varepsilon\\) a matrix of independent identically distributed errors. This model is useful because we summarize of \\(100 \\times 24\\) observations with \\(3 \\times (100+24+1) = 375\\) numbers. Furthermore, the three components of the model have useful interpretations: 1) the overall ability of a student, 2) the difference in ability between the math/sciences and arts, and 3) the remaining differences between the three subjects. The sizes \\(d_{1,1}, d_{2,2}\\) and \\(d_{3,3}\\) tell us the variability explained by each component. Finally, note that the components \\(d_{j,j} U_j V_j^{\\top}\\) are equivalent to the jth principal component.\nFinish the exercise by plotting an image of \\(Y\\), an image of \\(d_{1,1} U_1 V_1^{\\top} + d_{2,2} U_2 V_2^{\\top} + d_{3,3} U_3 V_3^{\\top}\\) and an image of the residuals, all with the same zlim.\n14. Advanced. The movielens dataset included in the dslabs package is a small subset of a larger dataset with millions of ratings. You can find the entire latest dataset here https://grouplens.org/datasets/movielens/20m/. Create your own recommendation system using all the tools we have shown you."
  },
  {
    "objectID": "ml/intro-ml.html",
    "href": "ml/intro-ml.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Machine learning has achieved remarkable successes, ranging from the postal service’s handwritten zip code readers to voice recognition systems like Apple’s Siri. These advances also include movie recommendation systems, spam and malware detection, housing price prediction algorithms, and the development of driverless cars. Although Artificial Intelligence (AI) and Machine Learning are terms frequently used interchangeably today, here we distinguish between them. Traditional AI systems, exemplified by chess-playing machines, employed decision-making based on preset rules stemming from theories or fundamental principles. In contrast, machine learning makes decisions using algorithms trained with data. Furthermore, while AI typically refers to tools complete with user interfaces and ready for real-world application, the term Machine Learning is often reserved for the underlying ideas, concepts, and methodologies, regardless of whether a tangible tool has been developed. In this part of the book we focus on these ideas, concepts, and methodologies, but also demonstrate their application to handwritten digits."
  },
  {
    "objectID": "ml/notation-and-terminology.html#terminology",
    "href": "ml/notation-and-terminology.html#terminology",
    "title": "\n24  Notation and Terminology\n",
    "section": "\n24.1 Terminology",
    "text": "24.1 Terminology\nIn machine learning, data comes in the form of the outcome we want to predict and the features that we will use to predict the outcome. We build algorithms that take feature values as input and returns a prediction for the outcome when we don’t know the outcome. The machine learning approach is to train an algorithm using a dataset for which we do know the outcome, and then apply this algorithm in the future to make a prediction when we don’t know the outcome.\nPrediction problems can be divided into categorical and continuous outcomes. For categorical outcomes, \\(Y\\) can be any one of \\(K\\) classes. The number of classes can vary greatly across applications. For example, in the digit reader data, \\(K=10\\) with the classes being the digits 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9. In speech recognition, the outcomes are all possible words or phrases we are trying to detect. Spam detection has two outcomes: spam or not spam. In this book, we denote the \\(K\\) categories with indexes \\(k=1,\\dots,K\\). However, for binary data we will use \\(k=0,1\\) for mathematical conveniences that we demonstrate later."
  },
  {
    "objectID": "ml/notation-and-terminology.html#notation",
    "href": "ml/notation-and-terminology.html#notation",
    "title": "\n24  Notation and Terminology\n",
    "section": "\n24.2 Notation",
    "text": "24.2 Notation\nHere we will use \\(Y\\) to denote the outcome and \\(X_1, \\dots, X_p\\) to denote features. Note that features are sometimes referred to as predictors or covariates. We consider all these to be synonyms.\nThe first step in building an algorithm is to understand what are the outcomes and features. In Section Section 19.1 we showed that associated with each digitized image \\(i\\), there is a categorical outcome \\(Y_i\\) and features \\(X_{i,1}, \\dots, X_{i,p}\\), with \\(p=784\\). We use bold face \\(\\mathbf{X}_i = (X_{i,1}, \\dots, X_{i,p})\\) to denote the vector of predictors. Note that we are using the matrix notation described in Section 19.2. When referring to an arbitrary set of features rather than a specific image, we drop the index \\(i\\) and use \\(Y\\) and \\(\\mathbf{X} = (X_{1}, \\dots, X_{p})\\). We use upper case variables because, in general, we think of the outcome and predictors as random variables. We use lower case, for example \\(\\mathbf{X} = \\mathbf{x}\\), to denote observed values. Although, when we code, we stick to lower case.\nThe machine learning task is to build an algorithm that returns a prediction for any of the possible values of the features. Here, we will learn several approaches to building these algorithms. Although at this point it might seem impossible to achieve this, we will start with simple examples and build up our knowledge until we can attack more complex ones. In fact, we start with an artificially simple example with just one predictor and then move on to a slightly more realistic example with two predictors. Once we understand these, we will attack real-world machine learning challenges involving many predictors."
  },
  {
    "objectID": "ml/notation-and-terminology.html#the-machine-learning-challenge",
    "href": "ml/notation-and-terminology.html#the-machine-learning-challenge",
    "title": "\n24  Notation and Terminology\n",
    "section": "\n24.3 The machine learning challenge",
    "text": "24.3 The machine learning challenge\nThe general setup is as follows. We have a series of features and an unknown outcome we want to predict:\n\n\n\n\noutcome\nfeature 1\nfeature 2\nfeature 3\n\\(\\dots\\)\nfeature p\n\n\n?\n\\(X_1\\)\n\\(X_2\\)\n\\(X_3\\)\n\\(\\dots\\)\n\\(X_p\\)\n\n\n\n\nTo build a model that provides a prediction for any set of observed values \\(X_1=x_1, X_2=x_2, \\dots X_p=x_p\\), we collect data for which we know the outcome:\n\n\n\n\noutcome\nfeature 1\nfeature 2\nfeature 3\n\\(\\dots\\)\nfeature 5\n\n\n\n\\(y_{1}\\)\n\\(x_{1,1}\\)\n\\(x_{1,2}\\)\n\\(x_{1,3}\\)\n\\(\\dots\\)\n\\(x_{1,p}\\)\n\n\n\\(y_{2}\\)\n\\(x_{2,1}\\)\n\\(x_{2,2}\\)\n\\(x_{2,3}\\)\n\\(\\dots\\)\n\\(x_{2,p}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\n\n\\(y_n\\)\n\\(x_{n,1}\\)\n\\(x_{n,2}\\)\n\\(x_{n,3}\\)\n\\(\\dots\\)\n\\(x_{n,p}\\)\n\n\n\n\n\nWhen the output is continuous we refer to the machine learning task as prediction, and the main output of the model is a function \\(f\\) that automatically produces a prediction, denoted with \\(\\hat{y}\\), for any set of predictors: \\(\\hat{y} = f(x_1, x_2, \\dots, x_p)\\). We use the term actual outcome to denote what we ended up observing. So we want the prediction \\(\\hat{y}\\) to match the actual outcome \\(y\\) as well as possible. Because our outcome is continuous, our predictions \\(\\hat{y}\\) will not be either exactly right or wrong, but instead we will determine an error defined as the difference between the prediction and the actual outcome \\(y - \\hat{y}\\).\nWhen the outcome is categorical, we refer to the machine learning task as classification, and the main output of the model will be a decision rule which prescribes which of the \\(K\\) classes we should predict. In this scenario, most models provide functions of the predictors for each class \\(k\\), \\(f_k(x_1, x_2, \\dots, x_p)\\), that are used to make this decision. When the data is binary a typical decision rules looks like this: if \\(f_1(x_1, x_2, \\dots, x_p) &gt; C\\), predict category 1, if not the other category, with \\(C\\) a predetermined cutoff. Because the outcomes are categorical, our predictions will be either right or wrong.\nNotice that these terms vary among courses, text books, and other publications. Often prediction is used for both categorical and continuous outcomes, and the term regression can be used for the continuous case. Here we avoid using regression to avoid confusion with our previous use of the term linear regression. In most cases it will be clear if our outcomes are categorical or continuous, so we will avoid using these terms when possible."
  },
  {
    "objectID": "ml/evaluation-metrics.html#sec-training-test",
    "href": "ml/evaluation-metrics.html#sec-training-test",
    "title": "\n25  Evaluation metrics\n",
    "section": "\n25.1 Training and test sets",
    "text": "25.1 Training and test sets\nUltimately, a machine learning algorithm is evaluated on how it performs in the real world with completely new datasets. However, when developing an algorithm, we usually have a dataset for which we know the outcomes, as we do with the heights: we know the sex of every student in our dataset. Therefore, to mimic the ultimate evaluation process, we typically split the data into two parts and act as if we don’t know the outcome for one of these. We stop pretending we don’t know the outcome to evaluate the algorithm, but only after we are done constructing it. We refer to the group for which we know the outcome, and use to develop the algorithm, as the training set. We refer to the group for which we pretend we don’t know the outcome as the test set.\nA standard way of generating the training and test sets is by randomly splitting the data. The caret package includes the function createDataPartition that helps us generates indexes for randomly splitting the data into training and test sets:\n\nset.seed(2007)\ntest_index &lt;- createDataPartition(y, times = 1, p = 0.5, list = FALSE)\n\nThe argument times is used to define how many random samples of indexes to return, the argument p is used to define what proportion of the data is represented by the index, and the argument list is used to decide if we want the indexes returned as a list or not. We can use the result of the createDataPartition function call to define the training and test sets like this:\n\ntest_set &lt;- heights[test_index, ]\ntrain_set &lt;- heights[-test_index, ]\n\nWe will now develop an algorithm using only the training set. Once we are done developing the algorithm, we will freeze it and evaluate it using the test set. The simplest way to evaluate the algorithm when the outcomes are categorical is by simply reporting the proportion of cases that were correctly predicted in the test set. This metric is usually referred to as overall accuracy."
  },
  {
    "objectID": "ml/evaluation-metrics.html#overall-accuracy",
    "href": "ml/evaluation-metrics.html#overall-accuracy",
    "title": "\n25  Evaluation metrics\n",
    "section": "\n25.2 Overall accuracy",
    "text": "25.2 Overall accuracy\nTo demonstrate the use of overall accuracy, we will build two competing algorithms and compare them.\nLet’s start by developing the simplest possible machine algorithm: guessing the outcome.\n\ny_hat &lt;- sample(c(\"Male\", \"Female\"), length(test_index), replace = TRUE)\n\nNote that we are completely ignoring the predictor and simply guessing the sex.\nIn machine learning applications, it is useful to use factors to represent the categorical outcomes because R functions developed for machine learning, such as those in the caret package, require or recommend that categorical outcomes be coded as factors. So convert y_hat to factors using the factor function:\n\ny_hat &lt;- sample(c(\"Male\", \"Female\"), length(test_index), replace = TRUE) |&gt;\n  factor(levels = levels(test_set$sex))\n\nThe overall accuracy is simply defined as the overall proportion that is predicted correctly:\n\nmean(y_hat == test_set$sex)\n#&gt; [1] 0.51\n\nNot surprisingly, our accuracy is about 50%. We are guessing!\nCan we do better? Exploratory data analysis suggests we can because, on average, males are slightly taller than females:\n\nheights |&gt; group_by(sex) |&gt; summarize(mean(height), sd(height))\n#&gt; # A tibble: 2 × 3\n#&gt;   sex    `mean(height)` `sd(height)`\n#&gt;   &lt;fct&gt;           &lt;dbl&gt;        &lt;dbl&gt;\n#&gt; 1 Female           64.9         3.76\n#&gt; 2 Male             69.3         3.61\n\nBut how do we make use of this insight? Let’s try another simple approach: predict Male if height is within two standard deviations from the average male:\n\ny_hat &lt;- ifelse(x &gt; 62, \"Male\", \"Female\") |&gt; \n  factor(levels = levels(test_set$sex))\n\nThe accuracy goes up from 0.50 to about 0.80:\n\nmean(y == y_hat)\n#&gt; [1] 0.793\n\nBut can we do even better? In the example above, we used a cutoff of 62, but we can examine the accuracy obtained for other cutoffs and then pick the value that provides the best results. But remember, it is important that we optimize the cutoff using only the training set: the test set is only for evaluation. Although for this simplistic example it is not much of a problem, later we will learn that evaluating an algorithm on the training set can lead to overfitting, which often results in dangerously over-optimistic assessments.\nHere we examine the accuracy of 10 different cutoffs and pick the one yielding the best result:\n\ncutoff &lt;- seq(61, 70)\naccuracy &lt;- map_dbl(cutoff, function(x){\n  y_hat &lt;- ifelse(train_set$height &gt; x, \"Male\", \"Female\") |&gt; \n    factor(levels = levels(test_set$sex))\n  mean(y_hat == train_set$sex)\n})\n\nWe can make a plot showing the accuracy obtained on the training set for males and females:\n\n\n\n\n\n\n\n\nWe see that the maximum value is:\n\nmax(accuracy)\n#&gt; [1] 0.85\n\nwhich is much higher than 0.5. The cutoff resulting in this accuracy is:\n\nbest_cutoff &lt;- cutoff[which.max(accuracy)]\nbest_cutoff\n#&gt; [1] 64\n\nWe can now test this cutoff on our test set to make sure our accuracy is not overly optimistic:\n\ny_hat &lt;- ifelse(test_set$height &gt; best_cutoff, \"Male\", \"Female\") |&gt; \n  factor(levels = levels(test_set$sex))\ny_hat &lt;- factor(y_hat)\nmean(y_hat == test_set$sex)\n#&gt; [1] 0.804\n\nWe see that it is a bit lower than the accuracy observed for the training set, but it is still better than guessing. And by testing on a dataset that we did not train on, we know our result is not due to cherry-picking a good result."
  },
  {
    "objectID": "ml/evaluation-metrics.html#the-confusion-matrix",
    "href": "ml/evaluation-metrics.html#the-confusion-matrix",
    "title": "\n25  Evaluation metrics\n",
    "section": "\n25.3 The confusion matrix",
    "text": "25.3 The confusion matrix\nThe prediction rule we developed in the previous section predicts Male if the student is taller than 64 inches. Given that the average female is about 64 inches, this prediction rule seems wrong. What happened? If a student is the height of the average female, shouldn’t we predict Female?\nGenerally speaking, overall accuracy can be a deceptive measure. To see this, we will start by constructing what is referred to as the confusion matrix, which basically tabulates each combination of prediction and actual value. We can do this in R using the function table:\n\ntable(predicted = y_hat, actual = test_set$sex)\n#&gt;          actual\n#&gt; predicted Female Male\n#&gt;    Female     48   32\n#&gt;    Male       71  374\n\nIf we study this table closely, it reveals a problem. If we compute the accuracy separately for each sex, we get:\n\ntest_set |&gt; \n  mutate(y_hat = y_hat) |&gt;\n  group_by(sex) |&gt; \n  summarize(accuracy = mean(y_hat == sex))\n#&gt; # A tibble: 2 × 2\n#&gt;   sex    accuracy\n#&gt;   &lt;fct&gt;     &lt;dbl&gt;\n#&gt; 1 Female    0.403\n#&gt; 2 Male      0.921\n\nThere is an imbalance in the accuracy for males and females: too many females are predicted to be male. We are calling almost half of the females male! How can our overall accuracy be so high then? This is because the prevalence of males in this dataset is high. These heights were collected from three data sciences courses, two of which had more males enrolled:\n\nprev &lt;- mean(y == \"Male\")\nprev\n#&gt; [1] 0.773\n\nSo when computing overall accuracy, the high percentage of mistakes made for females is outweighed by the gains in correct calls for men. This can actually be a big problem in machine learning. If your training data is biased in some way, you are likely to develop algorithms that are biased as well. The fact that we used a test set does not matter because it is also derived from the original biased dataset. This is one of the reasons we look at metrics other than overall accuracy when evaluating a machine learning algorithm.\nThere are several metrics that we can use to evaluate an algorithm in a way that prevalence does not cloud our assessment, and these can all be derived from the confusion matrix. A general improvement to using overall accuracy is to study sensitivity and specificity separately."
  },
  {
    "objectID": "ml/evaluation-metrics.html#sensitivity-and-specificity",
    "href": "ml/evaluation-metrics.html#sensitivity-and-specificity",
    "title": "\n25  Evaluation metrics\n",
    "section": "\n25.4 Sensitivity and specificity",
    "text": "25.4 Sensitivity and specificity\nTo define sensitivity and specificity, we need a binary outcome. When the outcomes are categorical, we can define these terms for a specific category. In the digits example, we can ask for the specificity in the case of correctly predicting 2 as opposed to some other digit. Once we specify a category of interest, then we can talk about positive outcomes, \\(Y=1\\), and negative outcomes, \\(Y=0\\).\nIn general, sensitivity is defined as the ability of an algorithm to predict a positive outcome when the actual outcome is positive: \\(\\hat{Y}=1\\) when \\(Y=1\\). Because an algorithm that calls everything positive (\\(\\hat{Y}=1\\) no matter what) has perfect sensitivity, this metric on its own is not enough to judge an algorithm. For this reason, we also examine specificity, which is generally defined as the ability of an algorithm to not predict a positive \\(\\hat{Y}=0\\) when the actual outcome is not a positive \\(Y=0\\). We can summarize in the following way:\n\nHigh sensitivity: \\(Y=1 \\implies \\hat{Y}=1\\)\n\nHigh specificity: \\(Y=0 \\implies \\hat{Y} = 0\\)\n\n\nAlthough the above is often considered the definition of specificity, another way to think of specificity is by the proportion of positive calls that are actually positive:\n\nHigh specificity: \\(\\hat{Y}=1 \\implies Y=1\\).\n\nTo provide precise definitions, we name the four entries of the confusion matrix:\n\n\n\n\n\nActually Positive\nActually Negative\n\n\n\nPredicted positive\nTrue positives (TP)\nFalse positives (FP)\n\n\nPredicted negative\nFalse negatives (FN)\nTrue negatives (TN)\n\n\n\n\n\nSensitivity is typically quantified by \\(TP/(TP+FN)\\), the proportion of actual positives (the first column = \\(TP+FN\\)) that are called positives (\\(TP\\)). This quantity is referred to as the true positive rate (TPR) or recall.\nSpecificity is defined as \\(TN/(TN+FP)\\) or the proportion of negatives (the second column = \\(FP+TN\\)) that are called negatives (\\(TN\\)). This quantity is also called the true negative rate (TNR). There is another way of quantifying specificity which is \\(TP/(TP+FP)\\) or the proportion of outcomes called positives (the first row or \\(TP+FP\\)) that are actually positives (\\(TP\\)). This quantity is referred to as positive predictive value (PPV) and also as precision. Note that, unlike TPR and TNR, precision depends on prevalence since higher prevalence implies you can get higher precision even when guessing.\nThe multiple names can be confusing, so we include a table to help us remember the terms. The table includes a column that shows the definition if we think of the proportions as probabilities.\n\n\n\n\n\n\n\n\n\nMeasure of\nName 1\nName 2\nDefinition\nProbability representation\n\n\n\nsensitivity\nTPR\nRecall\n\\(\\frac{\\mbox{TP}}{\\mbox{TP} + \\mbox{FN}}\\)\n\\(\\mbox{Pr}(\\hat{Y}=1 \\mid Y=1)\\)\n\n\nspecificity\nTNR\n1-FPR\n\\(\\frac{\\mbox{TN}}{\\mbox{TN}+\\mbox{FP}}\\)\n\\(\\mbox{Pr}(\\hat{Y}=0 \\mid Y=0)\\)\n\n\nspecificity\nPPV\nPrecision\n\\(\\frac{\\mbox{TP}}{\\mbox{TP}+\\mbox{FP}}\\)\n\\(\\mbox{Pr}(Y=1 \\mid \\hat{Y}=1)\\)\n\n\n\nHere TPR is True Positive Rate, FPR is False Positive Rate, and PPV is Positive Predictive Value. The caret function confusionMatrix computes all these metrics for us once we define what category “positive” is. The function expects factors as input, and the first level is considered the positive outcome or \\(Y=1\\). In our example, Female is the first level because it comes before Male alphabetically. If you type this into R you will see several metrics including accuracy, sensitivity, specificity, and PPV.\n\ncm &lt;- confusionMatrix(data = y_hat, reference = test_set$sex)\n\nYou can acceess these directly, for example, like this:\n\ncm$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.804\ncm$byClass[c(\"Sensitivity\",\"Specificity\", \"Prevalence\")]\n#&gt; Sensitivity Specificity  Prevalence \n#&gt;       0.403       0.921       0.227\n\nWe can see that the high overall accuracy is possible despite relatively low sensitivity. As we hinted at above, the reason this happens is because of the low prevalence (0.23): the proportion of females is low. Because prevalence is low, failing to predict actual females as females (low sensitivity) does not lower the accuracy as much as failing to predict actual males as males (low specificity). This is an example of why it is important to examine sensitivity and specificity and not just accuracy. Before applying this algorithm to general datasets, we need to ask ourselves if prevalence will be the same."
  },
  {
    "objectID": "ml/evaluation-metrics.html#balanced-accuracy-and-f_1-score",
    "href": "ml/evaluation-metrics.html#balanced-accuracy-and-f_1-score",
    "title": "\n25  Evaluation metrics\n",
    "section": "\n25.5 Balanced accuracy and \\(F_1\\) score",
    "text": "25.5 Balanced accuracy and \\(F_1\\) score\nAlthough we usually recommend studying both specificity and sensitivity, very often it is useful to have a one-number summary, for example for optimization purposes. One metric that is preferred over overall accuracy is the average of specificity and sensitivity, referred to as balanced accuracy. Because specificity and sensitivity are rates, it is more appropriate to compute the harmonic average. In fact, the \\(F_1\\)-score, a widely used one-number summary, is the harmonic average of precision and recall:\n\\[\n\\frac{1}{\\frac{1}{2}\\left(\\frac{1}{\\mbox{recall}} +\n    \\frac{1}{\\mbox{precision}}\\right) }\n\\]\nBecause it is easier to write, you often see this harmonic average rewritten as:\n\\[\n2 \\times \\frac{\\mbox{precision} \\cdot \\mbox{recall}}\n{\\mbox{precision} + \\mbox{recall}}\n\\]\nwhen defining \\(F_1\\).\nRemember that, depending on the context, some types of errors are more costly than others. For example, in the case of plane safety, it is much more important to maximize sensitivity over specificity: failing to predict a plane will malfunction before it crashes is a much more costly error than grounding a plane when, in fact, the plane is in perfect condition. In a capital murder criminal case, the opposite is true since a false positive can lead to executing an innocent person. The \\(F_1\\)-score can be adapted to weigh specificity and sensitivity differently. To do this, we define \\(\\beta\\) to represent how much more important sensitivity is compared to specificity and consider a weighted harmonic average:\n\\[\n\\frac{1}{\\frac{\\beta^2}{1+\\beta^2}\\frac{1}{\\mbox{recall}} +\n    \\frac{1}{1+\\beta^2}\\frac{1}{\\mbox{precision}} }\n\\]\nThe F_meas function in the caret package computes this summary with beta defaulting to 1.\nLet’s rebuild our prediction algorithm, but this time maximizing the F-score instead of overall accuracy:\n\ncutoff &lt;- seq(61, 70)\nF_1 &lt;- map_dbl(cutoff, function(x){\n  y_hat &lt;- ifelse(train_set$height &gt; x, \"Male\", \"Female\") |&gt; \n    factor(levels = levels(test_set$sex))\n  F_meas(data = y_hat, reference = factor(train_set$sex))\n})\n\nAs before, we can plot these \\(F_1\\) measures versus the cutoffs:\n\n\n\n\n\n\n\n\nWe see that it is maximized at \\(F_1\\) value of:\n\nmax(F_1)\n#&gt; [1] 0.647\n\nThis maximum is achieved when we use the following cutoff:\n\nbest_cutoff &lt;- cutoff[which.max(F_1)]\nbest_cutoff\n#&gt; [1] 66\n\nA cutoff of 66 makes more sense than 64. Furthermore, it balances the specificity and sensitivity of our confusion matrix:\n\ny_hat &lt;- ifelse(test_set$height &gt; best_cutoff, \"Male\", \"Female\") |&gt; \n  factor(levels = levels(test_set$sex))\nsensitivity(data = y_hat, reference = test_set$sex)\n#&gt; [1] 0.63\nspecificity(data = y_hat, reference = test_set$sex)\n#&gt; [1] 0.833\n\nWe now see that we do much better than guessing, that both sensitivity and specificity are relatively high, and that we have built our first machine learning algorithm. It takes height as a predictor and predicts female if you are 65 inches or shorter."
  },
  {
    "objectID": "ml/evaluation-metrics.html#prevalence-matters-in-practice",
    "href": "ml/evaluation-metrics.html#prevalence-matters-in-practice",
    "title": "\n25  Evaluation metrics\n",
    "section": "\n25.6 Prevalence matters in practice",
    "text": "25.6 Prevalence matters in practice\nA machine learning algorithm with very high sensitivity and specificity may not be useful in practice when prevalence is close to either 0 or 1. To see this, consider the case of a doctor that specializes in a rare disease and is interested in developing an algorithm for predicting who has the disease. The doctor shares data with you and you then develop an algorithm with very high sensitivity. You explain that this means that if a patient has the disease, the algorithm is very likely to predict correctly. You also tell the doctor that you are also concerned because, based on the dataset you analyzed, 1/2 the patients have the disease: \\(\\mbox{Pr}(\\hat{Y}=1)\\). The doctor is neither concerned nor impressed and explains that what is important is the precision of the test: \\(\\mbox{Pr}(Y=1 | \\hat{Y}=1)\\). Using Bayes theorem, we can connect the two measures:\n\\[ \\mbox{Pr}(Y = 1\\mid \\hat{Y}=1) = \\mbox{Pr}(\\hat{Y}=1 \\mid Y=1) \\frac{\\mbox{Pr}(Y=1)}{\\mbox{Pr}(\\hat{Y}=1)}\\]\nThe doctor knows that the prevalence of the disease is 5 in 1,000, which implies that \\(\\mbox{Pr}(Y=1) \\, / \\,\\mbox{Pr}(\\hat{Y}=1) = 1/100\\) and therefore the precision of your algorithm is less than 0.01. The doctor does not have much use for your algorithm."
  },
  {
    "objectID": "ml/evaluation-metrics.html#roc-and-precision-recall-curves",
    "href": "ml/evaluation-metrics.html#roc-and-precision-recall-curves",
    "title": "\n25  Evaluation metrics\n",
    "section": "\n25.7 ROC and precision-recall curves",
    "text": "25.7 ROC and precision-recall curves\nWhen comparing the two methods (guessing versus using a height cutoff), we looked at accuracy and \\(F_1\\). The second method clearly outperformed the first. However, while we considered several cutoffs for the second method, for the first we only considered one approach: guessing with equal probability. Note that guessing Male with higher probability would give us higher accuracy due to the bias in the sample:\n\np &lt;- 0.9\nn &lt;- length(test_index)\ny_hat &lt;- sample(c(\"Male\", \"Female\"), n, replace = TRUE, prob = c(p, 1 - p)) |&gt; \n  factor(levels = levels(test_set$sex))\nmean(y_hat == test_set$sex)\n#&gt; [1] 0.739\n\nBut, as described above, this would come at the cost of lower sensitivity. The curves we describe in this section will help us see this.\nRemember that for each of these parameters, we can get a different sensitivity and specificity. For this reason, a very common approach to evaluating methods is to compare them graphically by plotting both.\nA widely used plot that does this is the receiver operating characteristic (ROC) curve. If you are wondering where this name comes from, you can consult the ROC Wikipedia page1.\nThe ROC curve plots sensitivity (TPR) versus 1 - specificity or the false positive rate (FPR). Here we compute the TPR and FPR needed for different probabilities of guessing male:\n\nprobs &lt;- seq(0, 1, length.out = 10)\nguessing &lt;- map_df(probs, function(p){\n  y_hat &lt;- \n    sample(c(\"Male\", \"Female\"), n, replace = TRUE, prob = c(p, 1 - p)) |&gt; \n    factor(levels = c(\"Female\", \"Male\"))\n  list(method = \"Guessing\",\n       FPR = 1 - specificity(y_hat, test_set$sex),\n       TPR = sensitivity(y_hat, test_set$sex))\n})\n\nWe can use similar code to compute these values for our our second approach. By plotting both curves together, we are able to compare sensitivity for different values of specificity:\n\n\n\n\n\n\n\n\nWe can see that we obtain higher sensitivity with this approach for all values of specificity, which implies it is in fact a better method. Note that ROC curves for guessing always fall on the identiy line. Also note that when making ROC curves, it is often nice to add the cutoff associated with each point.\nThe packages pROC and plotROC are useful for generating these plots.\nROC curves have one weakness and it is that neither of the measures plotted depends on prevalence. In cases in which prevalence matters, we may instead make a precision-recall plot. The idea is similar, but we instead plot precision against recall:\n\n\n\n\n\n\n\n\nFrom this plot we immediately see that the precision of guessing is not high. This is because the prevalence is low. We also see that if we change positives to mean Male instead of Female, the ROC curve remains the same, but the precision recall plot changes."
  },
  {
    "objectID": "ml/evaluation-metrics.html#sec-loss-function",
    "href": "ml/evaluation-metrics.html#sec-loss-function",
    "title": "\n25  Evaluation metrics\n",
    "section": "\n25.8 The loss function",
    "text": "25.8 The loss function\nUp to now we have described evaluation metrics that apply exclusively to categorical data. Specifically, for binary outcomes, we have described how sensitivity, specificity, accuracy, and \\(F_1\\) can be used as quantification. However, these metrics are not useful for continuous outcomes. In this section, we describe how the general approach to defining “best” in machine learning is to define a loss function, which can be applied to both categorical and continuous data.\nThe most commonly used loss function is the squared loss function. If \\(\\hat{y}\\) is our predictor and \\(y\\) is the observed outcome, the squared loss function is simply:\n\\[\n(\\hat{y} - y)^2\n\\]\nBecause we often have a test set with many observations, say \\(N\\), we use the mean squared error (MSE):\n\\[\n\\mbox{MSE} = \\frac{1}{N} \\mbox{RSS} = \\frac{1}{N}\\sum_{i=1}^N (\\hat{y}_i - y_i)^2\n\\]\nIn practice, we often report the root mean squared error (RMSE), which is \\(\\sqrt{\\mbox{MSE}}\\), because it is in the same units as the outcomes. But doing the math is often easier with the MSE and it is therefore more commonly used in textbooks, since these usually describe theoretical properties of algorithms.\nIf the outcomes are binary, both RMSE and MSE are equivalent to one minus accuracy, since \\((\\hat{y} - y)^2\\) is 0 if the prediction was correct and 1 otherwise. In general, our goal is to build an algorithm that minimizes the loss so it is as close to 0 as possible.\nBecause our data is usually a random sample, we can think of the MSE as a random variable and the observed MSE can be thought of as an estimate of the expected MSE, which in mathematical notation we write like this:\n\\[\n\\mbox{E}\\left\\{ \\frac{1}{N}\\sum_{i=1}^N (\\hat{Y}_i - Y_i)^2 \\right\\}\n\\]\nThis is a theoretical concept because in practice we only have one dataset to work with. But in theory, we think of having a very large number of random samples (call it \\(B\\)), apply our algorithm to each, obtain an MSE for each random sample, and think of the expected MSE as:\n\\[\n\\frac{1}{B} \\sum_{b=1}^B \\frac{1}{N}\\sum_{i=1}^N \\left(\\hat{y}_i^b - y_i^b\\right)^2\n\\]\nwith \\(y_{i}^b\\) denoting the \\(i\\)th observation in the \\(b\\)th random sample and \\(\\hat{y}_i^b\\) the resulting prediction obtained from applying the exact same algorithm to the \\(b\\)th random sample. Again, in practice we only observe one random sample, so the expected MSE is only theoretical. However, in Chapter Chapter 28 we describe an approach to estimating the MSE that tries to mimic this theoretical quantity.\nNote that there are loss functions other than the squared loss. For example, the Mean Absolute Error uses absolute values, \\(|\\hat{Y}_i - Y_i|\\) instead of squaring the errors \\((\\hat{Y}_i - Y_i)^2\\). However, in this book we focus on minimizing square loss since it is the most widely used."
  },
  {
    "objectID": "ml/evaluation-metrics.html#exercises",
    "href": "ml/evaluation-metrics.html#exercises",
    "title": "\n25  Evaluation metrics\n",
    "section": "\n25.9 Exercises",
    "text": "25.9 Exercises\nThe reported_height and height datasets were collected from three classes taught in the Departments of Computer Science and Biostatistics, as well as remotely through the Extension School. The biostatistics class was taught in 2016 along with an online version offered by the Extension School. On 2016-01-25 at 8:15 AM, during one of the lectures, the instructors asked students to fill in the sex and height questionnaire that populated the reported_height dataset. The online students filled the survey during the next few days, after the lecture was posted online. We can use this insight to define a variable, call it type, to denote the type of student: inclass or online:\n\nlibrary(lubridate)\ndat &lt;- mutate(reported_heights, date_time = ymd_hms(time_stamp)) |&gt;\n  filter(date_time &gt;= make_date(2016, 01, 25) & \n           date_time &lt; make_date(2016, 02, 1)) |&gt;\n  mutate(type = ifelse(day(date_time) == 25 & hour(date_time) == 8 & \n                         between(minute(date_time), 15, 30),\n                       \"inclass\", \"online\")) |&gt; select(sex, type)\nx &lt;- dat$type\ny &lt;- factor(dat$sex, c(\"Female\", \"Male\"))\n\n1. Show summary statistics that indicate that the type is predictive of sex.\n2. Instead of using height to predict sex, use the type variable.\n3. Show the confusion matrix.\n4. Use the confusionMatrix function in the caret package to report accuracy.\n5. Now use the sensitivity and specificity functions to report specificity and sensitivity.\n6. What is the prevalence (% of females) in the dat dataset defined above?"
  },
  {
    "objectID": "ml/evaluation-metrics.html#footnotes",
    "href": "ml/evaluation-metrics.html#footnotes",
    "title": "\n25  Evaluation metrics\n",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Receiver_operating_characteristic↩︎"
  },
  {
    "objectID": "ml/conditionals.html#conditional-probabilities",
    "href": "ml/conditionals.html#conditional-probabilities",
    "title": "\n26  Conditional probabilities and expectations\n",
    "section": "\n26.1 Conditional probabilities",
    "text": "26.1 Conditional probabilities\nWe use the notation \\((X_1 = x_1,\\dots,X_p=x_p)\\) to represent the fact that we have observed values \\(x_1,\\dots,x_p\\) for covariates \\(X_1, \\dots, X_p\\). This does not imply that the outcome \\(Y\\) will take a specific value. Instead, it implies a specific probability. In particular, we denote the conditional probabilities for each class \\(k\\):\n\\[\n\\mbox{Pr}(Y=k \\mid X_1 = x_1,\\dots,X_p=x_p), \\, \\mbox{for}\\,k=1,\\dots,K\n\\]\nTo avoid writing out all the predictors, we will use the bold letters like this: \\(\\mathbf{X} \\equiv (X_1,\\dots,X_p)\\) and \\(\\mathbf{x} \\equiv (x_1,\\dots,x_p)\\). We will also use the following notation for the conditional probability of being class \\(k\\):\n\\[\np_k(\\mathbf{x}) = \\mbox{Pr}(Y=k \\mid \\mathbf{X}=\\mathbf{x}), \\, \\mbox{for}\\, k=1,\\dots,K\n\\]\nNote: We will be using the \\(p(x)\\) notation to represent conditional probabilities as functions of the predictors. Do not confuse it with the \\(p\\) that represents the number of predictors.\nThese probabilities guide the construction of an algorithm that makes the best prediction: for any given \\(\\mathbf{x}\\), we will predict the class \\(k\\) with the largest probability among \\(p_1(x), p_2(x), \\dots p_K(x)\\). In mathematical notation, we write it like this: \\(\\hat{Y} = \\max_k p_k(\\mathbf{x})\\).\nIn machine learning, we refer to this as Bayes’ Rule. But keep in mind that this is a theoretical rule since in practice we don’t know \\(p_k(\\mathbf{x}), k=1,\\dots,K\\). In fact, estimating these conditional probabilities can be thought of as the main challenge of machine learning. The better our probability estimates \\(\\hat{p}_k(\\mathbf{x})\\), the better our predictor:\n\\[\\hat{Y} = \\max_k \\hat{p}_k(\\mathbf{x})\\]\nSo what we will predict depends on two things: 1) how close are the \\(\\max_k p_k(\\mathbf{x})\\) to 1 or 0 (perfect certainty) and 2) how close our estimates \\(\\hat{p}_k(\\mathbf{x})\\) are to \\(p_k(\\mathbf{x})\\). We can’t do anything about the first restriction as it is determined by the nature of the problem, so our energy goes into finding ways to best estimate conditional probabilities. The first restriction does imply that we have limits as to how well even the best possible algorithm can perform. You should get used to the idea that while in some challenges we will be able to achieve almost perfect accuracy, with digit readers for example, in others our success is restricted by the randomness of the process, with movie recommendations for example.\nBefore we continue, it is important to remember that defining our prediction by maximizing the probability is not always optimal in practice and depends on the context. As discussed above, sensitivity and specificity may differ in importance. But even in these cases, having a good estimate of the \\(p_k(x), k=1,\\dots,K\\) will suffice for us to build optimal prediction models, since we can control the balance between specificity and sensitivity however we wish. For instance, we can simply change the cutoffs used to predict one outcome or the other. In the plane example, we may ground the plane anytime the probability of malfunction is higher than 1 in a million as opposed to the default 1/2 used when error types are equally undesired."
  },
  {
    "objectID": "ml/conditionals.html#conditional-expectations",
    "href": "ml/conditionals.html#conditional-expectations",
    "title": "\n26  Conditional probabilities and expectations\n",
    "section": "\n26.2 Conditional expectations",
    "text": "26.2 Conditional expectations\nFor binary data, you can think of the probability \\(\\mbox{Pr}(Y=1 \\mid \\mathbf{X}=\\mathbf{x})\\) as the proportion of 1s in the stratum of the population for which \\(\\mathbf{X}=\\mathbf{x}\\). Many of the algorithms we will learn can be applied to both categorical and continuous data due to the connection between conditional probabilities and conditional expectations.\nBecause the expectation is the average of values \\(y_1,\\dots,y_n\\) in the population, in the case in which the \\(y\\)s are 0 or 1, the expectation is equivalent to the probability of randomly picking a one since the average is simply the proportion of ones:\n\\[\n\\mbox{E}(Y \\mid \\mathbf{X}=\\mathbf{x})=\\mbox{Pr}(Y=1 \\mid \\mathbf{X}=\\mathbf{x}).\n\\]\nAs a result, we often only use the expectation to denote both the conditional probability and conditional expectation.\nJust like with categorical outcomes, in most applications the same observed predictors do not guarantee the same continuous outcomes. Instead, we assume that the outcome follows the same conditional distribution. We will now explain why we use the conditional expectation to define our predictors."
  },
  {
    "objectID": "ml/conditionals.html#conditional-expectation-minimizes-squared-loss-function",
    "href": "ml/conditionals.html#conditional-expectation-minimizes-squared-loss-function",
    "title": "\n26  Conditional probabilities and expectations\n",
    "section": "\n26.3 Conditional expectation minimizes squared loss function",
    "text": "26.3 Conditional expectation minimizes squared loss function\nWhy do we care about the conditional expectation in machine learning? This is because the expected value has an attractive mathematical property: it minimizes the MSE. Specifically, of all possible predictions \\(\\hat{Y}\\),\n\\[\n\\hat{Y} = \\mbox{E}(Y \\mid \\mathbf{X}=\\mathbf{x}) \\, \\mbox{ minimizes } \\, \\mbox{E}\\{ (\\hat{Y} - Y)^2  \\mid  \\mathbf{X}=\\mathbf{x} \\}\n\\]\nDue to this property, a succinct description of the main task of machine learning is that we use data to estimate:\n\\[\nf(\\mathbf{x}) \\equiv \\mbox{E}( Y  \\mid  \\mathbf{X}=\\mathbf{x} )\n\\]\nfor any set of features \\(\\mathbf{x} = (x_1, \\dots, x_p)\\). Of course this is easier said than done, since this function can take any shape and \\(p\\) can be very large. Consider a case in which we only have one predictor \\(x\\). The expectation \\(\\mbox{E}\\{ Y \\mid X=x \\}\\) can be any function of \\(x\\): a line, a parabola, a sine wave, a step function, anything. It gets even more complicated when we consider instances with large \\(p\\), in which case \\(f(\\mathbf{x})\\) is a function of a multidimensional vector \\(\\mathbf{x}\\). For example, in our digit reader example \\(p = 784\\)! The main way in which competing machine learning algorithms differ is in their approach to estimating this expectation."
  },
  {
    "objectID": "ml/conditionals.html#exercises",
    "href": "ml/conditionals.html#exercises",
    "title": "\n26  Conditional probabilities and expectations\n",
    "section": "\n26.4 Exercises",
    "text": "26.4 Exercises\n1. Compute conditional probabilities for being Male for the heights dataset. Round the heights to the closest inch. Plot the estimated conditional probability \\(P(x) = \\mbox{Pr}(\\mbox{Male} | \\mbox{height}=x)\\) for each \\(x\\).\n2. In the plot we just made, we see high variability for low values of height. This is because we have few data points in these strata. This time use the quantile function for quantiles \\(0.1,0.2,\\dots,0.9\\) and the cut function to assure each group has the same number of points. Hint: for any numeric vector x, you can create groups based on quantiles like this:\n\ncut(x, quantile(x, seq(0, 1, 0.1)), include.lowest = TRUE)\n\n3. Generate data from a bivariate normal distribution using the MASS package like this:\n\nSigma &lt;- 9*matrix(c(1,0.5,0.5,1), 2, 2)\ndat &lt;- MASS::mvrnorm(n = 10000, c(69, 69), Sigma) |&gt;\n  data.frame() |&gt; setNames(c(\"x\", \"y\"))\n\nYou can make a quick plot of the data using plot(dat). Use an approach similar to the previous exercise to estimate the conditional expectations and make a plot."
  },
  {
    "objectID": "ml/smoothing.html#sec-two-or-seven",
    "href": "ml/smoothing.html#sec-two-or-seven",
    "title": "\n27  Smoothing\n",
    "section": "\n27.1 Simplified MNIST: Is it a 2 or a 7?",
    "text": "27.1 Simplified MNIST: Is it a 2 or a 7?\nTo motivate the need for smoothing and make the connection with machine learning, we will construct a simplifyed version of the MNIST dataset with just two classes for the outcome and two predictors. Specifically, we define the challenge as building an algorithm that can determine if a digit is a 2 or 7 from the proportion of dark pixels in the upper left quadrant (\\(X_1\\)) and the lower right quadrant (\\(X_2\\)). We also selected a random sample of 1,000 digits, 500 in the training set and 500 in the test set. We provide this dataset in the dslabs package:\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(dslabs)\nmnist_27$train |&gt; ggplot(aes(x_1, x_2, color = y)) + geom_point()\n\n\n\n\n\n\n\nWe can immediately see some patterns. For example, if \\(X_1\\) (the upper left panel) is very large, then the digit is probably a 7. Also, for smaller values of \\(X_1\\), the 2s appear to be in the mid range values of \\(X_2\\).\nTo illustrate how to interpret \\(X_1\\) and \\(X_2\\), we include four example images. On the left are the original images of the two digits with the largest and smallest values for \\(X_1\\) and on the right we have the images corresponding to the largest and smallest values of \\(X_2\\):\n\n\n\n\n\n\n\n\nWe can start getting a sense for why these predictors are useful, but also why the problem will be somewhat challenging.\nWe haven’t really learned any algorithms yet, so let’s try building an algorithm using multivariable regression. The model is simply:\n\\[\np(x_1, x_2) = \\mbox{Pr}(Y=1 \\mid X_1=x_1 , X_2 = x_2) =\n\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n\\]\nWe fit it like this:\n\nfit &lt;- mnist_27$train |&gt; mutate(y = ifelse(y == 7, 1, 0)) |&gt; lm(y ~ x_1 + x_2, data = _)\n\nWe can now build a decision rule based on the estimate of \\(\\hat{p}(x_1, x_2)\\):\n\np_hat &lt;- predict(fit, newdata = mnist_27$test)\ny_hat &lt;- factor(ifelse(p_hat &gt; 0.5, 7, 2))\nconfusionMatrix(y_hat, mnist_27$test$y)$overall[[\"Accuracy\"]]\n#&gt; [1] 0.75\n\nWe get an accuracy well above 50%. Not bad for our first try. But can we do better?\nBecause we constructed the mnist_27 example and we had at our disposal 60,000 digits in just the MNIST dataset, we used this to build the true conditional distribution \\(p(x_1, x_2)\\). Keep in mind that this is something we don’t have access to in practice, but we include it in this example because it permits the comparison of \\(\\hat{p}(x_1, x_2)\\) to the true \\(p(x_1, x_2)\\). This comparison teaches us the limitations of different algorithms. Let’s do that here. We have stored the true \\(p(x_1,x_2)\\) in the mnist_27 object and can plot the image using the ggplot2 function geom_raster(). We choose better colors and use the stat_contour function to draw a curve that separates pairs \\((x_1,x_2)\\) for which \\(p(x_1,x_2) &gt; 0.5\\) and pairs for which \\(p(x_1,x_2) &lt; 0.5\\):\n\nmnist_27$true_p |&gt; \n  ggplot(aes(x_1, x_2, z = p)) +\n  geom_raster(aes(fill = p)) +\n  scale_fill_gradientn(colors = c(\"#F8766D\", \"white\", \"#00BFC4\")) +\n  stat_contour(breaks = c(0.5), color = \"black\")\n\n\n\n\n\n\n\nAbove you see a plot of the true \\(p(x_1, x_2)\\). To start understanding the limitations of regression here, first note that with regression \\(\\hat{p}(x_1,x_2)\\) has to be a plane, and as a result the boundary defined by the decision rule is given by: \\(\\hat{p}(x_1,x_2) = 0.5\\):\n\\[\n\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 = 0.5 \\implies\n\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 = 0.5  \\implies\nx_2 = (0.5-\\hat{\\beta}_0)/\\hat{\\beta}_2  -\\hat{\\beta}_1/\\hat{\\beta}_2 x_1\n\\]\nNote that for this boundary, \\(x_2\\) is a linear function of \\(x_1\\). This implies that our regression approach has no chance of capturing the non-linear nature of the true \\(p(x_1,x_2)\\). Below is a visual representation of \\(\\hat{p}(x_1, x_2)\\). Regression can’t catch this.\n\n\n\n\n\n\n\n\nWe need something more flexible: a method that permits estimates with shapes other than a plane. Smoothing techniques permit this flexibility. We will start by describing nearest neighbor and kernel approaches. To understand why we cover this topic, remember that the concepts behind smoothing techniques are extremely useful in machine learning because conditional expectations/probabilities can be thought of as trends of unknown shapes that we need to estimate in the presence of uncertainty."
  },
  {
    "objectID": "ml/smoothing.html#signal-plus-noise-model",
    "href": "ml/smoothing.html#signal-plus-noise-model",
    "title": "\n27  Smoothing\n",
    "section": "\n27.2 Signal plus noise model",
    "text": "27.2 Signal plus noise model\nTo explain these concepts, we will focus first on a problem with just one predictor. Specifically, we try to estimate the time trend in the 2008 US popular vote poll margin (difference between Obama and McCain). Later we will learn about methods such as k-nearest neighbors that can be used to smooth with higher dimensions.\n\nlibrary(tidyverse)\nlibrary(dslabs)\npolls_2008 |&gt; ggplot(aes(day, margin)) + geom_point()\n\n\n\n\n\n\n\nFor the purposes of the popular vote example, do not think of it as a forecasting problem. Instead, we are simply interested in learning the shape of the trend after the election is over.\nWe assume that for any given day \\(x\\), there is a true preference among the electorate \\(f(x)\\), but due to the uncertainty introduced by the polling, each data point comes with an error \\(\\varepsilon\\). A mathematical model for the observed poll margin \\(Y_i\\) is:\n\\[\nY_i = f(x_i) + \\varepsilon_i\n\\]\nTo think of this as a machine learning problem, consider that we want to predict \\(Y\\) given a day \\(x\\). If we knew the conditional expectation \\(f(x) = \\mbox{E}(Y \\mid X=x)\\), we would use it. But since we don’t know this conditional expectation, we have to estimate it. Let’s use regression, since it is the only method we have learned up to now.\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nThe line we see does not appear to describe the trend very well. For example, on September 4 (day -62), the Republican Convention was held and the data suggest that it gave John McCain a boost in the polls. However, the regression line does not capture this potential trend. To see the lack of fit more clearly, we note that points above the fitted line (blue) and those below (red) are not evenly distributed across days. We therefore need an alternative, more flexible approach."
  },
  {
    "objectID": "ml/smoothing.html#bin-smoothing",
    "href": "ml/smoothing.html#bin-smoothing",
    "title": "\n27  Smoothing\n",
    "section": "\n27.3 Bin smoothing",
    "text": "27.3 Bin smoothing\nThe general idea of smoothing is to group data points into strata in which the value of \\(f(x)\\) can be assumed to be constant. We can make this assumption because we think \\(f(x)\\) changes slowly and, as a result, \\(f(x)\\) is almost constant in small windows of time. An example of this idea for the poll_2008 data is to assume that public opinion remained approximately the same within a week’s time. With this assumption in place, we have several data points with the same expected value.\nIf we fix a day to be in the center of our week, call it \\(x_0\\), then for any other day \\(x\\) such that \\(|x - x_0| \\leq 3.5\\), we assume \\(f(x)\\) is a constant \\(f(x) = \\mu\\). This assumption implies that:\n\\[\nE[Y_i | X_i = x_i ] \\approx \\mu \\mbox{   if   }  |x_i - x_0| \\leq 3.5\n\\]\nIn smoothing, we call the size of the interval satisfying \\(|x_i - x_0| \\leq 3.5\\) the window size, bandwidth or span. Later we will see that we try to optimize this parameter.\nThis assumption implies that a good estimate for \\(f(x)\\) is the average of the \\(Y_i\\) values in the window. If we define \\(A_0\\) as the set of indexes \\(i\\) such that \\(|x_i - x_0| \\leq 3.5\\) and \\(N_0\\) as the number of indexes in \\(A_0\\), then our estimate is:\n\\[\n\\hat{f}(x_0) = \\frac{1}{N_0} \\sum_{i \\in A_0}  Y_i\n\\]\nThe idea behind bin smoothing is to make this calculation with each value of \\(x\\) as the center. In the poll example, for each day, we would compute the average of the values within a week with that day in the center. Here are two examples: \\(x_0 = -125\\) and \\(x_0 = -55\\). The blue segment represents the resulting average.\n\n\n\n\n\n\n\n\nBy computing this mean for every point, we form an estimate of the underlying curve \\(f(x)\\). Below we show the procedure happening as we move from the -155 up to 0. At each value of \\(x_0\\), we keep the estimate \\(\\hat{f}(x_0)\\) and move on to the next point:\n\n\n\n\n\n\n\n\nThe final code and resulting estimate look like this:\n\nspan &lt;- 7 \nfit &lt;- with(polls_2008, ksmooth(day, margin, kernel = \"box\", bandwidth = span))\n\npolls_2008 |&gt; mutate(smooth = fit$y) |&gt;\n  ggplot(aes(day, margin)) +\n    geom_point(size = 3, alpha = .5, color = \"grey\") + \n  geom_line(aes(day, smooth), color = \"red\")"
  },
  {
    "objectID": "ml/smoothing.html#kernels",
    "href": "ml/smoothing.html#kernels",
    "title": "\n27  Smoothing\n",
    "section": "\n27.4 Kernels",
    "text": "27.4 Kernels\nThe final result from the bin smoother is quite wiggly. One reason for this is that each time the window moves, two points change. We can attenuate this somewhat by taking weighted averages that give the center point more weight than far away points, with the two points at the edges receiving very little weight.\nYou can think of the bin smoother approach as a weighted average:\n\\[\n\\hat{f}(x_0) = \\sum_{i=1}^N w_0(x_i) Y_i\n\\]\nin which each point receives a weight of either \\(0\\) or \\(1/N_0\\), with \\(N_0\\) the number of points in the week. In the code above, we used the argument kernel=\"box\" in our call to the function ksmooth. This is because the weight function looks like a box. The ksmooth function provides a “smoother” option which uses the normal density to assign weights.\n\n\n\n\n\n\n\n\n\nThe final code and resulting plot for the normal kernel look like this:\n\nspan &lt;- 7\nfit &lt;- with(polls_2008, ksmooth(day, margin, kernel = \"normal\", bandwidth = span))\n\npolls_2008 |&gt; mutate(smooth = fit$y) |&gt;\n  ggplot(aes(day, margin)) +\n  geom_point(size = 3, alpha = .5, color = \"grey\") + \n  geom_line(aes(day, smooth), color = \"red\")\n\n\n\n\n\n\n\nNotice that the final estimate now looks smoother.\nThere are several functions in R that implement bin smoothers. One example is ksmooth, shown above. In practice, however, we typically prefer methods that use slightly more complex models than fitting a constant. The final result above, for example, is still somewhat wiggly in parts we don’t expect it to be (between -125 and -75, for example). Methods such as loess, which we explain next, improve on this."
  },
  {
    "objectID": "ml/smoothing.html#local-weighted-regression-loess",
    "href": "ml/smoothing.html#local-weighted-regression-loess",
    "title": "\n27  Smoothing\n",
    "section": "\n27.5 Local weighted regression (loess)",
    "text": "27.5 Local weighted regression (loess)\nA limitation of the bin smoother approach just described is that we need small windows for the approximately constant assumptions to hold. As a result, we end up with a small number of data points to average and obtain imprecise estimates \\(\\hat{f}(x)\\). Here we describe how local weighted regression (loess) permits us to consider larger window sizes. To do this, we will use a mathematical result, referred to as Taylor’s theorem, which tells us that if you look closely enough at any smooth function \\(f(x)\\), it will look like a line. To see why this makes sense, consider the curved edges gardeners make using straight-edged spades:\n\n\n\n\n\n\n\n\n(“Downing Street garden path edge”1 by Flckr user Number 102. CC-BY 2.0 license3.)\nInstead of assuming the function is approximately constant in a window, we assume the function is locally linear. We can consider larger window sizes with the linear assumption than with a constant. Instead of the one-week window, we consider a larger one in which the trend is approximately linear. We start with a three-week window and later consider and evaluate other options:\n\\[\nE[Y_i | X_i = x_i ] = \\beta_0 + \\beta_1 (x_i-x_0) \\mbox{   if   }  |x_i - x_0| \\leq 21\n\\]\nFor every point \\(x_0\\), loess defines a window and fits a line within that window. Here is an example showing the fits for \\(x_0=-125\\) and \\(x_0 = -55\\):\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nThe fitted value at \\(x_0\\) becomes our estimate \\(\\hat{f}(x_0)\\). Below we show the procedure happening as we move from the -155 up to 0.\n\n\n\n\n\n\n\n\nThe final result is a smoother fit than the bin smoother since we use larger sample sizes to estimate our local parameters:\n\ntotal_days &lt;- diff(range(polls_2008$day))\nspan &lt;- 21/total_days\nfit &lt;- loess(margin ~ day, degree = 1, span = span, data = polls_2008)\npolls_2008 |&gt; mutate(smooth = fit$fitted) |&gt;\n  ggplot(aes(day, margin)) +\n  geom_point(size = 3, alpha = .5, color = \"grey\") +\n  geom_line(aes(day, smooth), color = \"red\")\n\n\n\n\n\n\n\nDifferent spans give us different estimates. We can see how different window sizes lead to different estimates:\n\n\n\n\n\n\n\n\nHere are the final estimates:\n\n\n\n\n\n\n\n\nThere are three other differences between loess and the typical bin smoother.\n1. Rather than keeping the bin size the same, loess keeps the number of points used in the local fit the same. This number is controlled via the span argument, which expects a proportion. For example, if N is the number of data points and span=0.5, then for a given \\(x\\), loess will use the 0.5 * N closest points to \\(x\\) for the fit.\n2. When fitting a line locally, loess uses a weighted approach. Basically, instead of using least squares, we minimize a weighted version:\n\\[\n\\sum_{i=1}^N w_0(x_i) \\left[Y_i - \\left\\{\\beta_0 + \\beta_1 (x_i-x_0)\\right\\}\\right]^2\n\\]\nHowever, instead of the Gaussian kernel, loess uses a function called the Tukey tri-weight:\n\\[\nW(u)= \\left( 1  - |u|^3\\right)^3 \\mbox{ if } |u| \\leq 1 \\mbox{ and } W(u) = 0 \\mbox{ if } |u| &gt; 1\n\\]\nTo define the weights, we denote \\(2h\\) as the window size and define:\n\\[\nw_0(x_i) = W\\left(\\frac{x_i - x_0}{h}\\right)\n\\]\nThis kernel differs from the Gaussian kernel in that more points get values closer to the max:\n\n\n\n\n\n\n\n\n3. loess has the option of fitting the local model robustly. An iterative algorithm is implemented in which, after fitting a model in one iteration, outliers are detected and down-weighted for the next iteration. To use this option, we use the argument family=\"symmetric\".\n\n27.5.1 Fitting parabolas\nTaylor’s theorem also tells us that if you look at any mathematical function closely enough, it looks like a parabola. The theorem also states that you don’t have to look as closely when approximating with parabolas as you do when approximating with lines. This means we can make our windows even larger and fit parabolas instead of lines.\n\\[\nE[Y_i | X_i = x_i ] = \\beta_0 + \\beta_1 (x_i-x_0) + \\beta_2 (x_i-x_0)^2 \\mbox{   if   }  |x_i - x_0| \\leq h\n\\]\nThis is actually the default procedure of the function loess. You may have noticed that when we showed the code for using loess, we set degree = 1. This tells loess to fit polynomials of degree 1, a fancy name for lines. If you read the help page for loess, you will see that the argument degree defaults to 2. By default, loess fits parabolas not lines. Here is a comparison of the fitting lines (red dashed) and fitting parabolas (orange solid):\n\ntotal_days &lt;- diff(range(polls_2008$day))\nspan &lt;- 28/total_days\nfit_1 &lt;- loess(margin ~ day, degree = 1, span = span, data = polls_2008)\nfit_2 &lt;- loess(margin ~ day, span = span, data = polls_2008)\n\npolls_2008 |&gt; mutate(smooth_1 = fit_1$fitted, smooth_2 = fit_2$fitted) |&gt;\n  ggplot(aes(day, margin)) +\n  geom_point(size = 3, alpha = .5, color = \"grey\") +\n  geom_line(aes(day, smooth_1), color = \"red\", lty = 2) +\n  geom_line(aes(day, smooth_2), color = \"orange\", lty = 1) \n\n\n\n\n\n\n\nThe degree = 2 gives us more wiggly results. We actually prefer degree = 1 as it is less prone to this kind of noise.\n\n27.5.2 Beware of default smoothing parameters\nggplot uses loess in its geom_smooth function:\n\npolls_2008 |&gt; ggplot(aes(day, margin)) +\n  geom_point() + \n  geom_smooth(method = loess)\n\n\n\n\n\n\n\nBut be careful with default parameters as they are rarely optimal. However, you can conveniently change them:\n\npolls_2008 |&gt; ggplot(aes(day, margin)) +\n  geom_point() + \n  geom_smooth(method = loess, method.args = list(span = 0.15, degree = 1))"
  },
  {
    "objectID": "ml/smoothing.html#sec-smoothing-ml-connection",
    "href": "ml/smoothing.html#sec-smoothing-ml-connection",
    "title": "\n27  Smoothing\n",
    "section": "\n27.6 Connecting smoothing to machine learning",
    "text": "27.6 Connecting smoothing to machine learning\nTo see how smoothing relates to machine learning with a concrete example, consider again our Section 27.1 example. If we define the outcome \\(Y = 1\\) for digits that are seven and \\(Y=0\\) for digits that are 2, then we are interested in estimating the conditional probability:\n\\[\np(x_1, x_2) = \\mbox{Pr}(Y=1 \\mid X_1=x_1 , X_2 = x_2).\n\\]\nwith \\(X_1\\) and \\(X_2\\) the two predictors defined in Section Section 27.1). In this example, the 0s and 1s we observe are “noisy” because for some regions the probabilities \\(p(x_1, x_2)\\) are not that close to 0 or 1. So we need to estimate \\(p(x_1, x_2)\\). Smoothing is an alternative to accomplishing this. In Section @ref(two-or-seven) we saw that linear regression was not flexible enough to capture the non-linear nature of \\(p(x_1, x_2)\\), thus smoothing approaches provide an improvement. In the next chapter we describe a popular machine learning algorithm, k-nearest neighbors, which is based on bin smoothing."
  },
  {
    "objectID": "ml/smoothing.html#exercises",
    "href": "ml/smoothing.html#exercises",
    "title": "\n27  Smoothing\n",
    "section": "\n27.7 Exercises",
    "text": "27.7 Exercises\n1. In the wrangling part of this book, we used the code below to obtain mortality counts for Puerto Rico for 2015-2018.\n\nlibrary(dslabs)\nhead(pr_death_counts)\n\nRemove data from before May 2018, then use the loess function to obtain a smooth estimate of the expected number of deaths as a function of date. Plot this resulting smooth function. Make the span about two months long.\n2. Plot the smooth estimates against day of the year, all on the same plot but with different colors.\n3. Suppose we want to predict 2s and 7s in our mnist_27 dataset with just the second covariate. Can we do this? On first inspection it appears the data does not have much predictive power. In fact, if we fit a regular logistic regression, the coefficient for x_2 is not significant!\n\nlibrary(broom)\nlibrary(dslabs)\nmnist_27$train |&gt; \n  glm(y ~ x_2, family = \"binomial\", data = _) |&gt; \n  tidy()\n\nPlotting a scatterplot here is not useful since y is binary:\n\nqplot(x_2, y, data = mnist_27$train)\n\nFit a loess line to the data above and plot the results. Notice that there is predictive power, except the conditional probability is not linear."
  },
  {
    "objectID": "ml/smoothing.html#footnotes",
    "href": "ml/smoothing.html#footnotes",
    "title": "\n27  Smoothing\n",
    "section": "",
    "text": "https://www.flickr.com/photos/49707497@N06/7361631644↩︎\nhttps://www.flickr.com/photos/number10gov/↩︎\nhttps://creativecommons.org/licenses/by/2.0/↩︎"
  },
  {
    "objectID": "ml/cross-validation.html#sec-knn-cv-intro",
    "href": "ml/cross-validation.html#sec-knn-cv-intro",
    "title": "28  Cross validation",
    "section": "\n28.1 Motivation with k-nearest neighbors",
    "text": "28.1 Motivation with k-nearest neighbors\nLet’s start by loading the data and showing a plot of the predictors with outcome represented with color.\n\nlibrary(tidyverse)\nlibrary(dslabs)\nmnist_27$test |&gt; ggplot(aes(x_1, x_2, color = y)) +  geom_point()\n\n\n\n\n\n\n\nWe will use these data to estimate the conditional probability function\n\\[\np(x_1, x_2) = \\mbox{Pr}(Y = 1 \\mid X_1 = x_1 , X_2 = x_2).\n\\] as defined in Section Section 27.6). With k-nearest neighbors (kNN) we estimate \\(p(x_1, x_2)\\) in a similar way to bin smoothing. However, as we will see, kNN is easier to adapt to multiple dimensions. First we define the distance between all observations based on the features. Then, for any point \\((x_1,x_2)\\) for which we want an estimate of \\(p(x_1, x_2)\\), we look for the \\(k\\) nearest points to \\((x_1,x_2)\\) and then take an average of the 0s and 1s associated with these points. We refer to the set of points used to compute the average as the neighborhood. Due to the connection we described earlier between conditional expectations and conditional probabilities, this gives us a \\(\\hat{p}(x_1,x_2\\), just like the bin smoother gave us an estimate of a trend. As with bin smoothers, we can control the flexibility of our estimate, in this case through the \\(k\\) parameter: larger \\(k\\)s result in smoother estimates, while smaller \\(k\\)s result in more flexible and more wiggly estimates.\nTo implement the algorithm, we can use the knn3 function from the caret package. Looking at the help file for this package, we see that we can call it in one of two ways. We will use the first in which we specify a formula and a data frame. The data frame contains all the data to be used. The formula has the form outcome ~ predictor_1 + predictor_2 + predictor_3 and so on. Therefore, we would type y ~ x_1 + x_2. If we are going to use all the predictors, we can use the . like this y ~ .. For this function, we also need to pick a parameter: the number of neighbors to include. Let’s start with the default \\(k = 5\\). The final call looks like this:\n\nlibrary(caret)\nknn_fit &lt;- knn3(y ~ ., data = mnist_27$train, k = 5)\n\nIn this case, since our dataset is balanced and we care just as much about sensitivity as we do about specificity, we will use accuracy to quantify performance.\nThe predict function for knn produces a probability for each class. We keep the probability of being a 7 as the estimate \\(\\hat{p}(x_1, x_2)\\)\n\ny_hat_knn &lt;- predict(knn_fit, mnist_27$test, type = \"class\")\nconfusionMatrix(y_hat_knn, mnist_27$test$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.815\n\nIn Section Section 27.1 we used linear regression to generate an estimate.\n\nfit_lm &lt;- mnist_27$train |&gt; \n  mutate(y = ifelse(y == 7, 1, 0)) |&gt; \n  lm(y ~ x_1 + x_2, data = _)\np_hat_lm &lt;- predict(fit_lm, mnist_27$test)\ny_hat_lm &lt;- factor(ifelse(p_hat_lm &gt; 0.5, 7, 2))\nconfusionMatrix(y_hat_lm, mnist_27$test$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;     0.75\n\nAnd we see that kNN, with the default parameter, already beats regression. To see why this is the case, we will plot \\(\\hat{p}(x_1, x_2)\\) and compare it to the true conditional probability \\(p(x_1, x_2)\\):\n\n\n\n\n\n\n\n\nWe see that kNN better adapts to the non-linear shape of \\(p(x_1, x_2)\\). However, our estimate has some islands of blue in the red area, which intuitively does not make much sense. This is due to what we call over-training. We describe over-training in detail below. Over-training is the reason that we have higher accuracy in the train set compared to the test set:\n\ny_hat_knn &lt;- predict(knn_fit, mnist_27$train, type = \"class\")\nconfusionMatrix(y_hat_knn, mnist_27$train$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.882\n\ny_hat_knn &lt;- predict(knn_fit, mnist_27$test, type = \"class\")\nconfusionMatrix(y_hat_knn, mnist_27$test$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.815"
  },
  {
    "objectID": "ml/cross-validation.html#over-training",
    "href": "ml/cross-validation.html#over-training",
    "title": "28  Cross validation",
    "section": "\n28.2 Over-training",
    "text": "28.2 Over-training\nOver-training is at its worst when we set \\(k = 1\\). With \\(k = 1\\), the estimate for each \\((x_1, x_2)\\) in the training set is obtained with just the \\(y\\) corresponding to that point. In this case, if the \\((x_1, x_2)\\) are unique, we will obtain perfect accuracy in the training set because each point is used to predict itself. Remember that if the predictors are not unique and have different outcomes for at least one set of predictors, then it is impossible to predict perfectly.\nHere we fit a kNN model with \\(k = 1\\):\n\nknn_fit_1 &lt;- knn3(y ~ ., data = mnist_27$train, k = 1)\ny_hat_knn_1 &lt;- predict(knn_fit_1, mnist_27$train, type = \"class\")\nconfusionMatrix(y_hat_knn_1, mnist_27$train$y)$overall[[\"Accuracy\"]]\n#&gt; [1] 0.996\n\nHowever, the test set accuracy is actually worse than regression:\n\ny_hat_knn_1 &lt;- predict(knn_fit_1, mnist_27$test, type = \"class\")\nconfusionMatrix(y_hat_knn_1, mnist_27$test$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;     0.73\n\nWe can see the over-fitting problem in this figure.\n\n\n\n\n\n\n\n\nThe black curves denote the decision rule boundaries.\nThe estimate \\(\\hat{p}(x_1, x_2)\\) follows the training data too closely (left). You can see that in the training set, boundaries have been drawn to perfectly surround a single red point in a sea of blue. Because most points \\((x_1, x_2)\\) are unique, the prediction is either 1 or 0 and the prediction for that point is the associated label. However, once we introduce the training set (right), we see that many of these small islands now have the opposite color and we end up making several incorrect predictions."
  },
  {
    "objectID": "ml/cross-validation.html#over-smoothing",
    "href": "ml/cross-validation.html#over-smoothing",
    "title": "28  Cross validation",
    "section": "\n28.3 Over-smoothing",
    "text": "28.3 Over-smoothing\nAlthough not as badly as with the previous examples, we saw that with \\(k = 5\\) we also over-trained. Hence, we should consider a larger \\(k\\). Let’s try, as an example, a much larger number: \\(k = 401\\).\n\nknn_fit_401 &lt;- knn3(y ~ ., data = mnist_27$train, k = 401)\ny_hat_knn_401 &lt;- predict(knn_fit_401, mnist_27$test, type = \"class\")\nconfusionMatrix(y_hat_knn_401, mnist_27$test$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;     0.79\n\nThis turns out to be similar to regression:\n\n#&gt; Warning: The following aesthetics were dropped during statistical\n#&gt; transformation: fill\n#&gt; ℹ This can happen when ggplot fails to infer the correct grouping\n#&gt;   structure in the data.\n#&gt; ℹ Did you forget to specify a `group` aesthetic or to convert a\n#&gt;   numerical variable into a factor?\n#&gt; The following aesthetics were dropped during statistical\n#&gt; transformation: fill\n#&gt; ℹ This can happen when ggplot fails to infer the correct grouping\n#&gt;   structure in the data.\n#&gt; ℹ Did you forget to specify a `group` aesthetic or to convert a\n#&gt;   numerical variable into a factor?\n\n\n\n\n\n\n\nThis size of \\(k\\) is so large that it does not permit enough flexibility. We call this over-smoothing."
  },
  {
    "objectID": "ml/cross-validation.html#picking-the-k-in-knn",
    "href": "ml/cross-validation.html#picking-the-k-in-knn",
    "title": "28  Cross validation",
    "section": "\n28.4 Picking the \\(k\\) in kNN",
    "text": "28.4 Picking the \\(k\\) in kNN\nSo how do we pick \\(k\\)? In principle we want to pick the \\(k\\) that maximizes accuracy, or minimizes the expected MSE as defined in Section 25.8. The goal of cross validation is to estimate these quantities for any given algorithm and set of tuning parameters such as \\(k\\). To understand why we need a special method to do this let’s repeat what we did above but for different values of \\(k\\):\n\nks &lt;- seq(3, 251, 2)\n\nWe do this using map_df function to repeat the above for each one.\n\nlibrary(purrr)\naccuracy &lt;- map_df(ks, function(k){\n  fit &lt;- knn3(y ~ ., data = mnist_27$train, k = k)\n  \n  y_hat &lt;- predict(fit, mnist_27$train, type = \"class\")\n  cm_train &lt;- confusionMatrix(y_hat, mnist_27$train$y)\n  train_error &lt;- cm_train$overall[\"Accuracy\"]\n  \n  y_hat &lt;- predict(fit, mnist_27$test, type = \"class\")\n  cm_test &lt;- confusionMatrix(y_hat, mnist_27$test$y)\n  test_error &lt;- cm_test$overall[\"Accuracy\"]\n  \n  tibble(train = train_error, test = test_error)\n})\n\nNote that we estimate accuracy by using both the training set and the test set. We can now plot the accuracy estimates for each value of \\(k\\):\n\n\n\n\n\n\n\n\nFirst, note that the estimate obtained on the training set is generally lower than the estimate obtained with the test set, with the difference larger for smaller values of \\(k\\). This is due to over-training. Also note that the accuracy versus \\(k\\) plot is quite jagged. We do not expect this because small changes in \\(k\\) should not affect the algorithm’s performance too much. The jaggedness is explained by the fact that the accuracy is computed on a sample and therefore is a random variable. This demonstrates why we prefer to minimize the expected loss rather than the loss we observe with one dataset.\nIf we were to use these estimates to pick the \\(k\\) that maximizes accuracy, we would use the estimates built on the test data:\n\nks[which.max(accuracy$test)]\n#&gt; [1] 41\nmax(accuracy$test)\n#&gt; [1] 0.86\n\nAnother reason we need a better estimate of accuracy is that if we use the test set to pick this \\(k\\), we should not expect the accompanying accuracy estimate to extrapolate to the real world. This is because even here we broke a golden rule of machine learning: we selected the \\(k\\) using the test set. Cross validation also provides an estimate that takes this into account."
  },
  {
    "objectID": "ml/cross-validation.html#mathematical-description-of-cross-validation",
    "href": "ml/cross-validation.html#mathematical-description-of-cross-validation",
    "title": "28  Cross validation",
    "section": "\n28.5 Mathematical description of cross validation",
    "text": "28.5 Mathematical description of cross validation\nIn Section Section 25.8, we described that a common goal of machine learning is to find an algorithm that produces predictors \\(\\hat{Y}\\) for an outcome \\(Y\\) that minimizes the MSE:\n\\[\n\\mbox{MSE} = \\mbox{E}\\left\\{ \\frac{1}{N}\\sum_{i = 1}^N (\\hat{Y}_i - Y_i)^2 \\right\\}\n\\]\nWhen all we have at our disposal is one dataset, we can estimate the MSE with the observed MSE like this:\n\\[\n\\hat{\\mbox{MSE}} = \\frac{1}{N}\\sum_{i = 1}^N (\\hat{y}_i - y_i)^2\n\\]\nThese two are often referred to as the true error and apparent error, respectively.\nThere are two important characteristics of the apparent error we should always keep in mind:\n\nBecause our data is random, the apparent error is a random variable. For example, the dataset we have may be a random sample from a larger population. An algorithm may have a lower apparent error than another algorithm due to luck.\nIf we train an algorithm on the same dataset that we use to compute the apparent error, we might be overtraining. In general, when we do this, the apparent error will be an underestimate of the true error. We will see an extreme example of this with k-nearest neighbors.\n\nCross validation is a technique that permits us to alleviate both these problems. To understand cross validation, it helps to think of the true error, a theoretical quantity, as the average of many apparent errors obtained by applying the algorithm to \\(B\\) new random samples of the data, none of them used to train the algorithm. As shown in a previous chapter, we think of the true error as:\n\\[\n\\frac{1}{B} \\sum_{b = 1}^B \\frac{1}{N}\\sum_{i = 1}^N \\left(\\hat{y}_i^b - y_i^b\\right)^2\n\\]\nwith \\(B\\) a large number that can be thought of as practically infinite. As already mentioned, this is a theoretical quantity because we only have available one set of outcomes: \\(y_1, \\dots, y_n\\). Cross validation is based on the idea of imitating the theoretical setup above as best we can with the data we have. To do this, we have to generate a series of different random samples. There are several approaches we can use, but the general idea for all of them is to randomly generate smaller datasets that are not used for training, and instead used to estimate the true error."
  },
  {
    "objectID": "ml/cross-validation.html#k-fold-cross-validation",
    "href": "ml/cross-validation.html#k-fold-cross-validation",
    "title": "28  Cross validation",
    "section": "\n28.6 K-fold cross validation",
    "text": "28.6 K-fold cross validation\nThe first one we describe is K-fold cross validation. Generally speaking, a machine learning challenge starts with a dataset (blue in the image below). We need to build an algorithm using this dataset that will eventually be used in completely independent datasets (yellow).\n\n\n\n\n\n\n\n\nBut we don’t get to see these independent datasets.\n\n\n\n\n\n\n\n\nSo to imitate this situation, we carve out a piece of our dataset and pretend it is an independent dataset: we divide the dataset into a training set (blue) and a test set (red). We will train our algorithm exclusively on the training set and use the test set only for evaluation purposes.\nWe usually try to select a small piece of the dataset so that we have as much data as possible to train. However, we also want the test set to be large so that we obtain a stable estimate of the loss without fitting an impractical number of models. Typical choices are to use 10%-20% of the data for testing.\n\n\n\n\n\n\n\n\nLet’s reiterate that it is indispensable that we not use the test set at all: not for filtering out rows, not for selecting features, nothing!\nNow this presents a new problem because for most machine learning algorithms we need to select parameters, for example the number of neighbors \\(k\\) in k-nearest neighbors. Here, we will refer to the set of parameters as \\(\\lambda\\). We need to optimize algorithm parameters without using our test set and we know that if we optimize and evaluate on the same dataset, we will overtrain. This is where cross validation is most useful.\nFor each set of algorithm parameters being considered, we want an estimate of the MSE and then we will choose the parameters with the smallest MSE. Cross validation provides this estimate.\nFirst, before we start the cross validation procedure, it is important to fix all the algorithm parameters. Although we will train the algorithm on the set of training sets, the parameters \\(\\lambda\\) will be the same across all training sets. We will use \\(\\hat{y}_i(\\lambda)\\) to denote the predictors obtained when we use parameters \\(\\lambda\\).\nSo, if we are going to imitate this definition:\n\\[\n\\mbox{MSE}(\\lambda) = \\frac{1}{B} \\sum_{b = 1}^B \\frac{1}{N}\\sum_{i = 1}^N \\left(\\hat{y}_i^b(\\lambda) - y_i^b\\right)^2\n\\]\nwe want to consider datasets that can be thought of as an independent random sample and we want to do this several times. With K-fold cross validation, we do it \\(K\\) times. In the cartoons, we are showing an example that uses \\(K = 5\\).\nWe will eventually end up with \\(K\\) samples, but let’s start by describing how to construct the first: we simply pick \\(M = N/K\\) observations at random (we round if \\(M\\) is not a round number) and think of these as a random sample \\(y_1^b, \\dots, y_M^b\\), with \\(b = 1\\). We call this the validation set:\n\n\n\n\n\n\n\n\nNow we can fit the model in the training set, then compute the apparent error on the independent set:\n\\[\n\\hat{\\mbox{MSE}}_b(\\lambda) = \\frac{1}{M}\\sum_{i = 1}^M \\left(\\hat{y}_i^b(\\lambda) - y_i^b\\right)^2\n\\]\nNote that this is just one sample and will therefore return a noisy estimate of the true error. This is why we take \\(K\\) samples, not just one. In K-cross validation, we randomly split the observations into \\(K\\) non-overlapping sets:\n\n\n\n\n\n\n\n\nNow we repeat the calculation above for each of these sets \\(b = 1,\\dots,K\\) and obtain \\(\\hat{\\mbox{MSE}}_1(\\lambda),\\dots, \\hat{\\mbox{MSE}}_K(\\lambda)\\). Then, for our final estimate, we compute the average:\n\\[\n\\hat{\\mbox{MSE}}(\\lambda) = \\frac{1}{K} \\sum_{b = 1}^K \\hat{\\mbox{MSE}}_b(\\lambda)\n\\]\nand obtain an estimate of our loss. A final step would be to select the \\(\\lambda\\) that minimizes the MSE.\nWe have described how to use cross validation to optimize parameters. However, we now have to take into account the fact that the optimization occurred on the training data and therefore we need an estimate of our final algorithm based on data that was not used to optimize the choice. Here is where we use the test set we separated early on:\n\n\n\n\n\n\n\n\nWe can do cross validation again:\n\n\n\n\n\n\n\n\nand obtain a final estimate of our expected loss. However, note that this means that our entire compute time gets multiplied by \\(K\\). You will soon learn that performing this task takes time because we are performing many complex computations. As a result, we are always looking for ways to reduce this time. For the final evaluation, we often just use the one test set.\nOnce we are satisfied with this model and want to make it available to others, we could refit the model on the entire dataset, without changing the optimized parameters.\n\n\n\n\n\n\n\n\nNow how do we pick the cross validation \\(K\\)? Large values of \\(K\\) are preferable because the training data better imitates the original dataset. However, larger values of \\(K\\) will have much slower computation time: for example, 100-fold cross validation will be 10 times slower than 10-fold cross validation. For this reason, the choices of \\(K = 5\\) and \\(K = 10\\) are popular.\nOne way we can improve the variance of our final estimate is to take more samples. To do this, we would no longer require the training set to be partitioned into non-overlapping sets. Instead, we would just pick \\(K\\) sets of some size at random.\nOne popular version of this technique, at each fold, picks observations at random with replacement (which means the same observation can appear twice). This approach has some advantages (not discussed here) and is generally referred to as the bootstrap. In fact, this is the default approach in the caret package. We describe how to implement cross validation with the caret package in the next chapter. In the next section, we include an explanation of how the bootstrap works in general.\n\n\n\n\n\n\nYou are ready to try exercises 1 through 8"
  },
  {
    "objectID": "ml/cross-validation.html#bootstrap",
    "href": "ml/cross-validation.html#bootstrap",
    "title": "28  Cross validation",
    "section": "\n28.7 Bootstrap",
    "text": "28.7 Bootstrap\nSuppose the income distribution of your population is as follows:\n\nset.seed(1995)\nn &lt;- 10^6\nincome &lt;- 10^(rnorm(n, log10(45000), log10(3)))\nqplot(log10(income), bins = 30, color = I(\"black\"))\n#&gt; Warning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\nThe population median is:\n\nm &lt;- median(income)\nm\n#&gt; [1] 44939\n\nSuppose we don’t have access to the entire population, but want to estimate the median \\(m\\). We take a sample of 100 and estimate the population median \\(m\\) with the sample median \\(M\\):\n\nN &lt;- 100\nX &lt;- sample(income, N)\nmedian(X)\n#&gt; [1] 38461\n\nCan we construct a confidence interval? What is the distribution of \\(M\\) ?\nBecause we are simulating the data, we can use a Monte Carlo simulation to learn the distribution of \\(M\\).\n\nlibrary(gridExtra)\nB &lt;- 10^4\nM &lt;- replicate(B, {\n  X &lt;- sample(income, N)\n  median(X)\n})\np1 &lt;- qplot(M, bins = 30, color = I(\"black\"))\np2 &lt;- qplot(sample = scale(M), xlab = \"theoretical\", ylab = \"sample\") + \n  geom_abline()\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\nIf we know this distribution, we can construct a confidence interval. The problem here is that, as we have already described, in practice we do not have access to the distribution. In the past, we have used the Central Limit Theorem, but the CLT we studied applies to averages and here we are interested in the median. We can see that the 95% confidence interval based on CLT\n\nmedian(X) + 1.96 * sd(X) / sqrt(N) * c(-1, 1)\n#&gt; [1] 21018 55905\n\nis quite different from the confidence interval we would generate if we know the actual distribution of \\(M\\):\n\nquantile(M, c(0.025, 0.975))\n#&gt;  2.5% 97.5% \n#&gt; 34438 59050\n\nThe bootstrap permits us to approximate a Monte Carlo simulation without access to the entire distribution. The general idea is relatively simple. We act as if the observed sample is the population. We then sample (with replacement) datasets, of the same sample size as the original dataset. Then we compute the summary statistic, in this case the median, on these bootstrap samples.\nTheory tells us that, in many situations, the distribution of the statistics obtained with bootstrap samples approximate the distribution of our actual statistic. This is how we construct bootstrap samples and an approximate distribution:\n\nB &lt;- 10^4\nM_star &lt;- replicate(B, {\n  X_star &lt;- sample(X, N, replace = TRUE)\n  median(X_star)\n})\n\nNote a confidence interval constructed with the bootstrap is much closer to one constructed with the theoretical distribution:\n\nquantile(M_star, c(0.025, 0.975))\n#&gt;  2.5% 97.5% \n#&gt; 30253 56909\n\nFor more on the Bootstrap, including corrections one can apply to improve these confidence intervals, please consult the book An introduction to the bootstrap by Efron, B., & Tibshirani, R. J.\nNote that we can use ideas similar to those used in the bootstrap in cross validation: instead of dividing the data into equal partitions, we simply bootstrap many times."
  },
  {
    "objectID": "ml/cross-validation.html#exercises",
    "href": "ml/cross-validation.html#exercises",
    "title": "28  Cross validation",
    "section": "\n28.8 Exercises",
    "text": "28.8 Exercises\nGenerate a set of random predictors and outcomes like this:\n\nset.seed(1996)\nn &lt;- 1000\np &lt;- 10000\nx &lt;- matrix(rnorm(n * p), n, p)\ncolnames(x) &lt;- paste(\"x\", 1:ncol(x), sep = \"_\")\ny &lt;- rbinom(n, 1, 0.5) |&gt; factor()\n\nx_subset &lt;- x[ ,sample(p, 100)]\n\n1. Because x and y are completely independent, you should not be able to predict y using x with accuracy larger than 0.5. Confirm this by running cross validation using logistic regression to fit the model. Because we have so many predictors, we selected a random sample x_subset. Use the subset when training the model. Hint: use the caret train function. The results component of the output of train shows you the accuracy. Ignore the warnings.\n2. Now, instead of a random selection of predictors, we are going to search for those that are most predictive of the outcome. We can do this by comparing the values for the \\(y = 1\\) group to those in the \\(y = 0\\) group, for each predictor, using a t-test. You can perform this step like this:\n\ndevtools::install_bioc(\"genefilter\")\ninstall.packages(\"genefilter\")\nlibrary(genefilter)\ntt &lt;- colttests(x, y)\n\nCreate a vector of the p-values and call it pvals.\n3. Create an index ind with the column numbers of the predictors that were “statistically significantly” associated with y. Use a p-value cutoff of 0.01 to define “statistically significant”. How many predictors survive this cutoff?\n4. Re-run the cross validation but after redefining x_subset to be the subset of x defined by the columns showing “statistically significant” association with y. What is the accuracy now?\n5. Re-run the cross validation again, but this time using kNN. Try out the following grid of tuning parameters: k = seq(101, 301, 25). Make a plot of the resulting accuracy.\n6. In exercises 3 and 4, we see that despite the fact that x and y are completely independent, we were able to predict y with accuracy higher than 70%. We must be doing something wrong then. What is it?\n\nThe function train estimates accuracy on the same data it uses to train the algorithm.\nWe are over-fitting the model by including 100 predictors.\nWe used the entire dataset to select the columns used in the model. This step needs to be included as part of the algorithm. The cross validation was done after this selection.\nThe high accuracy is just due to random variability.\n\n7. Advanced. Re-do the cross validation but this time include the selection step in the cross validation. The accuracy should now be close to 50%.\n8. Load the tissue_gene_expression dataset. Use the train function to predict tissue from gene expression. Use kNN. What k works best?\n9. The createResample function can be used to create bootstrap samples. For example, we can create 10 bootstrap samples for the mnist_27 dataset like this:\n\nset.seed(1995)\nindexes &lt;- createResample(mnist_27$train$y, 10)\n\nHow many times do 3, 4, and 7 appear in the first re-sampled index?\n10. We see that some numbers appear more than once and others appear no times. This has to be this way for each dataset to be independent. Repeat the exercise for all the re-sampled indexes.\n11. Generate a random dataset like this:\n\ny &lt;- rnorm(100, 0, 1)\n\nEstimate the 75th quantile, which we know is:\n\nqnorm(0.75)\n\nwith the sample quantile:\n\nquantile(y, 0.75)\n\nRun a Monte Carlo simulation to learn the expected value and standard error of this random variable.\n12. In practice, we can’t run a Monte Carlo simulation because we don’t know if rnorm is being used to simulate the data. Use the bootstrap to estimate the standard error using just the initial sample y. Use 10 bootstrap samples.\n13. Redo exercise 12, but with 10,000 bootstrap samples."
  },
  {
    "objectID": "ml/algorithms.html#linear-regression",
    "href": "ml/algorithms.html#linear-regression",
    "title": "\n29  Examples of algorithms\n",
    "section": "\n29.1 Linear regression",
    "text": "29.1 Linear regression\nLinear regression can be considered a machine learning algorithm. In Section Section 27.1 we demonstrated how linear regression can be too rigid to be useful. This is generally true, but for some challenges it works rather well. It also serves as a baseline approach: if you can’t beat it with a more complex approach, you probably want to stick to linear regression. To quickly make the connection between regression and machine learning, we will reformulate Galton’s study with heights, a continuous outcome.\n\nlibrary(HistData)\nset.seed(1983)\ngalton_heights &lt;- GaltonFamilies |&gt;\n  filter(gender == \"male\") |&gt;\n  group_by(family) |&gt;\n  sample_n(1) |&gt;\n  ungroup() |&gt;\n  select(father, childHeight) |&gt;\n  rename(son = childHeight)\n\nSuppose you are tasked with building a machine learning algorithm that predicts the son’s height \\(Y\\) using the father’s height \\(X\\). Let’s generate testing and training sets:\n\ny &lt;- galton_heights$son\ntest_index &lt;- createDataPartition(y, times = 1, p = 0.5, list = FALSE)\ntrain_set &lt;- galton_heights |&gt; slice(-test_index)\ntest_set &lt;- galton_heights |&gt; slice(test_index)\n\nIn this case, if we were just ignoring the father’s height and guessing the son’s height, we would guess the average height of sons.\n\nm &lt;- mean(train_set$son)\nm\n#&gt; [1] 69.2\n\nOur root mean squared error is:\n\nsqrt(mean((m - test_set$son)^2))\n#&gt; [1] 2.77\n\nCan we do better? In the regression chapter, we learned that if the pair \\((X,Y)\\) follow a bivariate normal distribution, the conditional expectation (what we want to estimate) is equivalent to the regression line:\n\\[\nf(x) = \\mbox{E}( Y  \\mid  X = x ) = \\beta_0 + \\beta_1 x\n\\]\nIn Section Section 13.10 we introduced least squares as a method for estimating the slope \\(\\beta_0\\) and intercept \\(\\beta_1\\):\n\nfit &lt;- lm(son ~ father, data = train_set)\nfit$coef\n#&gt; (Intercept)      father \n#&gt;      35.976       0.482\n\nThis gives us an estimate of the conditional expectation:\n\\[ \\hat{f}(x) = 35 + 0.5 x \\]\nWe can see that this does indeed provide an improvement over our guessing approach.\n\ny_hat &lt;- fit$coef[1] + fit$coef[2]*test_set$father\nsqrt(mean((y_hat - test_set$son)^2))\n#&gt; [1] 2.54\n\n\n29.1.1 The predict function\nThe predict function is very useful for machine learning applications. This function takes a fitted object from functions such as lm or glm (we learn about glm soon) and a data frame with the new predictors for which to predict. So in our current example, we would use predict like this:\n\ny_hat &lt;- predict(fit, test_set)\n\nUsing predict, we can get the same results as we did previously:\n\ny_hat &lt;- predict(fit, test_set)\nsqrt(mean((y_hat - test_set$son)^2))\n#&gt; [1] 2.54\n\npredict does not always return objects of the same types; it depends on what type of object is sent to it. To learn about the specifics, you need to look at the help file specific for the type of fit object that is being used. The predict is actually a special type of function in R (called a generic function) that calls other functions depending on what kind of object it receives. So if predict receives an object coming out of the lm function, it will call predict.lm. If it receives an object coming out of glm, it calls predict.glm. These two functions are similar but different. You can learn more about the differences by reading the help files:\n\n?predict.lm\n?predict.glm\n\nThere are many other versions of predict and many machine learning algorithms have a predict function.\n\n\n\n\n\n\nYou are ready to do exercises 1 - 8."
  },
  {
    "objectID": "ml/algorithms.html#logistic-regression",
    "href": "ml/algorithms.html#logistic-regression",
    "title": "\n29  Examples of algorithms\n",
    "section": "\n29.2 Logistic regression",
    "text": "29.2 Logistic regression\nThe regression approach can be extended to categorical data. In this section we first illustrate how, for binary data, one can simply assign numeric values of 0 and 1 to the outcomes \\(y\\), and apply regression as if the data were continuous. We will then point out a limitation with this approach and introduce logistic regression as a solution. Logistic regression is a specific case of a set of generalized linear models. To illustrate logistic regression, we will apply it to our previous predicting sex example defined in Section Section 25.1.\nIf we define the outcome \\(Y\\) as 1 for females and 0 for males, and \\(X\\) as the height, we are interested in the conditional probability:\n\\[\n\\mbox{Pr}( Y = 1 \\mid X = x)\n\\]\nAs an example, let’s provide a prediction for a student that is 66 inches tall. What is the conditional probability of being female if you are 66 inches tall? In our dataset, we can estimate this by rounding to the nearest inch and computing:\n\ntrain_set |&gt; \n  filter(round(height) == 66) |&gt;\n  summarize(y_hat = mean(sex == \"Female\"))\n#&gt;   y_hat\n#&gt; 1 0.347\n\nTo construct a prediction algorithm, we want to estimate the proportion of the population that is female for any given height \\(X = x\\), which we write as the conditional probability described above: \\(\\mbox{Pr}( Y = 1 | X = x)\\). Let’s see what this looks like for several values of \\(x\\) (we will remove strata of \\(x\\) with few data points):\n\nheights |&gt; \n  mutate(x = round(height)) |&gt;\n  group_by(x) |&gt;\n  filter(n() &gt;= 10) |&gt;\n  summarize(prop = mean(sex == \"Female\")) |&gt;\n  ggplot(aes(x, prop)) +\n  geom_point()\n\n\n\n\n\n\n\nSince the results from the plot above look close to linear, and it is the only approach we currently know, we will try regression. We assume that:\n\\[p(x) = \\mbox{Pr}( Y = 1 | X = x)  = \\beta_0 + \\beta_1 x\\]\nNote: because \\(p_0(x) = 1 - p_1(x)\\), we will only estimate \\(p_1(x)\\) and drop the \\(_1\\) index.\nIf we convert the factors to 0s and 1s, we can estimate \\(\\beta_0\\) and \\(\\beta_1\\) with least squares.\n\nlm_fit &lt;- mutate(train_set, y = as.numeric(sex == \"Female\")) |&gt; \n  lm(y ~ height, data = _)\n\nOnce we have estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), we can obtain an actual prediction. Our estimate of the conditional probability \\(p(x)\\) is:\n\\[\n\\hat{p}(x) = \\hat{\\beta}_0+ \\hat{\\beta}_1 x\n\\]\nTo form a prediction, we define a decision rule: predict female if \\(\\hat{p}(x) &gt; 0.5\\). We can compare our predictions to the outcomes using:\n\np_hat &lt;- predict(lm_fit, test_set)\ny_hat &lt;- ifelse(p_hat &gt; 0.5, \"Female\", \"Male\") |&gt; factor()\nconfusionMatrix(y_hat, test_set$sex)$overall[[\"Accuracy\"]]\n#&gt; [1] 0.798\n\nWe see this method does substantially better than guessing.\n\n29.2.1 Generalized linear models\nThe function \\(\\beta_0 + \\beta_1 x\\) can take any value including negatives and values larger than 1. In fact, the estimate \\(\\hat{p}(x)\\) computed in the linear regression section does indeed become negative.\n\nheights |&gt; \n  mutate(x = round(height)) |&gt;\n  group_by(x) |&gt;\n  filter(n() &gt;= 10) |&gt;\n  summarize(prop = mean(sex == \"Female\")) |&gt;\n  ggplot(aes(x, prop)) +\n  geom_point() + \n  geom_abline(intercept = lm_fit$coef[1], slope = lm_fit$coef[2])\n\n\n\n\n\n\n\nThe range is:\n\nrange(p_hat)\n#&gt; [1] -0.578  1.262\n\nBut we are estimating a probability: \\(\\mbox{Pr}( Y = 1 \\mid X = x)\\) which is constrained between 0 and 1.\nThe idea of generalized linear models (GLM) is to 1) define a distribution of \\(Y\\) that is consistent with it’s possible outcomes and 2) find a function \\(g\\) so that \\(g(\\mbox{Pr}( Y = 1 \\mid X = x))\\) can be modeled as a linear combination of predictors. Logistic regression is the most commonly used GLM. It is an extension of linear regression that assures that the estimate of \\(\\mbox{Pr}( Y = 1 \\mid X = x)\\) is between 0 and 1. This approach makes use of the logistic transformation defined as:\n\\[ g(p) = \\log \\frac{p}{1-p}.\\]\nThis logistic transformation converts probability to log odds. As discussed in the data visualization lecture, the odds tell us how much more likely it is something will happen compared to not happening. \\(p = 0.5\\) means the odds are 1 to 1, thus the odds are 1. If \\(p = 0.75\\), the odds are 3 to 1. A nice characteristic of this transformation is that it converts probabilities to be symmetric around 0. Here is a plot of \\(g(p)\\) versus \\(p\\):\n\n#&gt; Warning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\nWith logistic regression, we model the conditional probability directly with:\n\\[\ng\\left\\{ \\mbox{Pr}(Y = 1 \\mid X = x) \\right\\} = \\beta_0 + \\beta_1 x\n\\]\nWith this model, we can no longer use least squares. Instead we compute the maximum likelihood estimate (MLE). You can learn more about this concept in a statistical theory textbook1.\nIn R, we can fit the logistic regression model with the function glm: generalized linear models. This function is more general than logistic regression so we need to specify the model we want through the family parameter:\n\nglm_fit &lt;- train_set |&gt; \n  mutate(y = as.numeric(sex == \"Female\")) |&gt;\n  glm(y ~ height, data = _, family = \"binomial\")\n\nWe can obtain prediction using the predict function:\n\np_hat_logit &lt;- predict(glm_fit, newdata = test_set, type = \"response\")\n\nWhen using predict with a glm object, we have to specify that we want type = \"response\" if we want the conditional probabilities, since the default is to return the logistic transformed values.\nThis model fits the data slightly better than the line:\n\n\n\n\n\n\n\n\nBecause we have an estimate \\(\\hat{p}(x)\\), we can obtain predictions:\n\ny_hat_logit &lt;- ifelse(p_hat_logit &gt; 0.5, \"Female\", \"Male\") |&gt; factor()\nconfusionMatrix(y_hat_logit, test_set$sex)$overall[[\"Accuracy\"]]\n#&gt; [1] 0.808\n\nThe resulting predictions are similar. This is because the two estimates of \\(p(x)\\) are larger than 1/2 in about the same region of x:\n\n\n\n\n\n\n\n\nBoth linear and logistic regressions provide an estimate for the conditional expectation:\n\\[\n\\mbox{E}(Y \\mid X = x)\n\\] which in the case of binary data is equivalent to the conditional probability:\n\\[\n\\mbox{Pr}(Y = 1 \\mid X = x)\n\\]\n\n29.2.2 Logistic regression with more than one predictor\nIn this section we apply logistic regression to the two or seven data introduced in Section Section 27.1. In this case, we are interested in estimating a conditional probability that depends on two variables. The standard logistic regression model in this case will assume that\n\\[\ng\\{p(x_1, x_2)\\}= g\\{\\mbox{Pr}(Y = 1 \\mid X_1 = x_1 , X_2 = x_2)\\} =\n\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n\\]\nwith \\(g(p) = \\log \\frac{p}{1-p}\\) the logistic function described in the previous section. To fit the model we use the following code:\n\nfit_glm &lt;- glm(y ~ x_1 + x_2, data = mnist_27$train, family = \"binomial\")\np_hat_glm &lt;- predict(fit_glm, mnist_27$test, type = \"response\")\ny_hat_glm &lt;- factor(ifelse(p_hat_glm &gt; 0.5, 7, 2))\nconfusionMatrix(y_hat_glm, mnist_27$test$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;     0.75\n\nComparing to the results we obtained in Section Section 27.1, we see that logistic regression performs similarly to regression. This is not surprising, given that the estimate of \\(\\hat{p}(x_1, x_2)\\) looks similar as well:\n\np_hat &lt;- predict(fit_glm, newdata = mnist_27$true_p, type = \"response\")\nmnist_27$true_p |&gt; mutate(p_hat = p_hat) |&gt;\n  ggplot(aes(x_1, x_2,  z = p_hat, fill = p_hat)) +\n  geom_raster() +\n  scale_fill_gradientn(colors = c(\"#F8766D\",\"white\",\"#00BFC4\")) +\n  stat_contour(breaks = c(0.5), color = \"black\") \n#&gt; Warning: The following aesthetics were dropped during statistical\n#&gt; transformation: fill\n#&gt; ℹ This can happen when ggplot fails to infer the correct grouping\n#&gt;   structure in the data.\n#&gt; ℹ Did you forget to specify a `group` aesthetic or to convert a\n#&gt;   numerical variable into a factor?\n\n\n\n\n\n\n\nJust like regression, the decision rule is a line, a fact that can be corroborated mathematically since\n\\[\ng^{-1}(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2) = 0.5 \\implies\n\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 = g(0.5) = 0 \\implies\nx_2 = -\\hat{\\beta}_0/\\hat{\\beta}_2 -\\hat{\\beta}_1/\\hat{\\beta}_2 x_1\n\\]\nThus \\(x_2\\) is a linear function of \\(x_1\\). This implies that, just like regression, our logistic regression approach has no chance of capturing the non-linear nature of the true \\(p(x_1,x_2)\\). Once we move on to more complex examples, we will see that linear regression and generalized linear regression are limited and not flexible enough to be useful for most machine learning challenges. The new techniques we learn are essentially approaches to estimating the conditional probability in a way that is more flexible.\n\n\n\n\n\n\nYou can now do exeercises 9 - 11."
  },
  {
    "objectID": "ml/algorithms.html#k-nearest-neighbors",
    "href": "ml/algorithms.html#k-nearest-neighbors",
    "title": "\n29  Examples of algorithms\n",
    "section": "\n29.3 k-nearest neighbors",
    "text": "29.3 k-nearest neighbors\nWe introduced the kNN algorithm in Section Section 28.1) and demonstrated how we use cross validation to pick \\(k\\) in Section @ref(caret-cv). Here we quickly review how we fit a kNN model using the caret package. In Section @ref(caret-cv we introduced the following code to fit a kNN model:\n\ntrain_knn &lt;- train(y ~ ., method = \"knn\", \n                   data = mnist_27$train,\n                   tuneGrid = data.frame(k = seq(9, 71, 2)))\n\nWe saw that the parameter that maximized the estimated accuracy was:\n\ntrain_knn$bestTune\n#&gt;     k\n#&gt; 10 27\n\nThis model improves the accuracy over regression and logistic regression:\n\nconfusionMatrix(predict(train_knn, mnist_27$test, type = \"raw\"),\n                mnist_27$test$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.835\n\nA plot of the estimated conditional probability shows that the kNN estimate is flexible enough and does indeed capture the shape of the true conditional probability.\n\n#&gt; Warning: The following aesthetics were dropped during statistical\n#&gt; transformation: fill\n#&gt; ℹ This can happen when ggplot fails to infer the correct grouping\n#&gt;   structure in the data.\n#&gt; ℹ Did you forget to specify a `group` aesthetic or to convert a\n#&gt;   numerical variable into a factor?\n#&gt; The following aesthetics were dropped during statistical\n#&gt; transformation: fill\n#&gt; ℹ This can happen when ggplot fails to infer the correct grouping\n#&gt;   structure in the data.\n#&gt; ℹ Did you forget to specify a `group` aesthetic or to convert a\n#&gt;   numerical variable into a factor?\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou are ready to do exercises 12 - 13."
  },
  {
    "objectID": "ml/algorithms.html#generative-models",
    "href": "ml/algorithms.html#generative-models",
    "title": "\n29  Examples of algorithms\n",
    "section": "\n29.4 Generative models",
    "text": "29.4 Generative models\nWe have described how, when using squared loss, the conditional expectation/probabilities provide the best approach to developing a decision rule. In a binary case, the smallest true error we can achieve is determined by Bayes’ rule, which is a decision rule based on the true conditional probability:\n\\[\np(\\mathbf{x}) = \\mbox{Pr}(Y = 1 \\mid \\mathbf{X}=\\mathbf{x})\n\\]\nWe have described several approaches to estimating \\(p(\\mathbf{x})\\). In all these approaches, we estimate the conditional probability directly and do not consider the distribution of the predictors. In machine learning, these are referred to as discriminative approaches.\nHowever, Bayes’ theorem tells us that knowing the distribution of the predictors \\(\\mathbf{X}\\) may be useful. Methods that model the joint distribution of \\(Y\\) and \\(\\mathbf{X}\\) are referred to as generative models (we model how the entire data, \\(\\mathbf{X}\\) and \\(Y\\), are generated). We start by describing the most general generative model, Naive Bayes, and then proceed to describe two specific cases, quadratic discriminant analysis (QDA) and linear discriminant analysis (LDA).\n\n29.4.1 Naive Bayes\nRecall that Bayes rule tells us that we can rewrite \\(p(\\mathbf{x})\\) like this:\n\\[\np(\\mathbf{x}) = \\mbox{Pr}(Y = 1|\\mathbf{X}=\\mathbf{x}) = \\frac{f_{\\mathbf{X}|Y = 1}(\\mathbf{x}) \\mbox{Pr}(Y = 1)}\n{ f_{\\mathbf{X}|Y = 0}(\\mathbf{x})\\mbox{Pr}(Y = 0)  + f_{\\mathbf{X}|Y = 1}(\\mathbf{x})\\mbox{Pr}(Y = 1) }\n\\]\nwith \\(f_{\\mathbf{X}|Y = 1}\\) and \\(f_{\\mathbf{X}|Y = 0}\\) representing the distribution functions of the predictor \\(\\mathbf{X}\\) for the two classes \\(Y = 1\\) and \\(Y = 0\\). The formula implies that if we can estimate these conditional distributions of the predictors, we can develop a powerful decision rule. However, this is a big if. As we go forward, we will encounter examples in which \\(\\mathbf{X}\\) has many dimensions and we do not have much information about the distribution. In these cases, Naive Bayes will be practically impossible to implement. However, there are instances in which we have a small number of predictors (not much more than 2) and many categories in which generative models can be quite powerful. We describe two specific examples and use our previously described case studies to illustrate them.\nLet’s start with a very simple and uninteresting, yet illustrative, case: the example related to predicting sex from height.\n\nlibrary(tidyverse)\nlibrary(caret)\n\nlibrary(dslabs)\n\ny &lt;- heights$height\nset.seed(1995)\ntest_index &lt;- createDataPartition(y, times = 1, p = 0.5, list = FALSE)\ntrain_set &lt;- heights |&gt; slice(-test_index)\ntest_set &lt;- heights |&gt; slice(test_index)\n\nIn this case, the Naive Bayes approach is particularly appropriate because we know that the normal distribution is a good approximation for the conditional distributions of height given sex for both classes \\(Y = 1\\) (female) and \\(Y = 0\\) (male). This implies that we can approximate the conditional distributions \\(f_{X|Y = 1}\\) and \\(f_{X|Y = 0}\\) by simply estimating averages and standard deviations from the data:\n\nparams &lt;- train_set |&gt; \n  group_by(sex) |&gt; \n  summarize(avg = mean(height), sd = sd(height))\nparams\n#&gt; # A tibble: 2 × 3\n#&gt;   sex      avg    sd\n#&gt;   &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Female  64.8  4.14\n#&gt; 2 Male    69.2  3.57\n\nThe prevalence, which we will denote with \\(\\pi = \\mbox{Pr}(Y = 1)\\), can be estimated from the data with:\n\npi &lt;- train_set |&gt; summarize(pi = mean(sex == \"Female\")) |&gt; pull(pi)\npi\n#&gt; [1] 0.212\n\nNow we can use our estimates of average and standard deviation to get an actual rule:\n\nx &lt;- test_set$height\n\nf0 &lt;- dnorm(x, params$avg[2], params$sd[2])\nf1 &lt;- dnorm(x, params$avg[1], params$sd[1])\n\np_hat_bayes &lt;- f1*pi / (f1*pi + f0*(1 - pi))\n\nOur Naive Bayes estimate \\(\\hat{p}(x)\\) looks a lot like our logistic regression estimate:\n\n\n\n\n\n\n\n\nIn fact, we can show that the Naive Bayes approach is similar to the logistic regression prediction mathematically. However, we leave the demonstration to a more advanced text, such as the Elements of Statistical Learning2. We can see that they are similar empirically by comparing the two resulting curves.\n\n29.4.2 Controlling prevalence\nOne useful feature of the Naive Bayes approach is that it includes a parameter to account for differences in prevalence. Using our sample, we estimated \\(f_{X|Y = 1}\\), \\(f_{X|Y = 0}\\) and \\(\\pi\\). If we use hats to denote the estimates, we can write \\(\\hat{p}(x)\\) as:\n\\[\n\\hat{p}(x)= \\frac{\\hat{f}_{X|Y = 1}(x) \\hat{\\pi}}\n{ \\hat{f}_{X|Y = 0}(x)(1-\\hat{\\pi}) + \\hat{f}_{X|Y = 1}(x)\\hat{\\pi} }\n\\]\nAs we discussed earlier, our sample has a much lower prevalence, 0.21, than the general population. So if we use the rule \\(\\hat{p}(x)&gt;0.5\\) to predict females, our accuracy will be affected due to the low sensitivity:\n\ny_hat_bayes &lt;- ifelse(p_hat_bayes &gt; 0.5, \"Female\", \"Male\")\nsensitivity(data = factor(y_hat_bayes), reference = factor(test_set$sex))\n#&gt; [1] 0.213\n\nAgain, this is because the algorithm gives more weight to specificity to account for the low prevalence:\n\nspecificity(data = factor(y_hat_bayes), reference = factor(test_set$sex))\n#&gt; [1] 0.967\n\nThis is due mainly to the fact that \\(\\hat{\\pi}\\) is substantially less than 0.5, so we tend to predict Male more often. It makes sense for a machine learning algorithm to do this in our sample because we do have a higher percentage of males. But if we were to extrapolate this to a general population, our overall accuracy would be affected by the low sensitivity.\nThe Naive Bayes approach gives us a direct way to correct this since we can simply force \\(\\hat{\\pi}\\) to be whatever value we want it to be. So to balance specificity and sensitivity, instead of changing the cutoff in the decision rule, we could simply change \\(\\hat{\\pi}\\) to 0.5 like this:\n\np_hat_bayes_unbiased &lt;- f1 * 0.5 / (f1 * 0.5 + f0 * (1 - 0.5)) \ny_hat_bayes_unbiased &lt;- ifelse(p_hat_bayes_unbiased&gt; 0.5, \"Female\", \"Male\")\n\nNote the difference in sensitivity with a better balance:\n\nsensitivity(factor(y_hat_bayes_unbiased), factor(test_set$sex))\n#&gt; [1] 0.693\nspecificity(factor(y_hat_bayes_unbiased), factor(test_set$sex))\n#&gt; [1] 0.832\n\nThe new rule also gives us a very intuitive cutoff between 66-67, which is about the middle of the female and male average heights:\n\nqplot(x, p_hat_bayes_unbiased, geom = \"line\") + \n  geom_hline(yintercept = 0.5, lty = 2) + \n  geom_vline(xintercept = 67, lty = 2)\n\n\n\n\n\n\n\n\n29.4.3 Quadratic discriminant analysis\nQuadratic Discriminant Analysis (QDA) is a version of Naive Bayes in which we assume that the distributions \\(p_{\\mathbf{X}|Y = 1}(x)\\) and \\(p_{\\mathbf{X}|Y = 0}(\\mathbf{x})\\) are multivariate normal. The simple example we described in the previous section is actually QDA. Let’s now look at a slightly more complicated case: the 2 or 7 example.\nIn this case, we have two predictors so we assume each one is bivariate normal. This implies that we need to estimate two averages, two standard deviations, and a correlation for each case \\(Y = 1\\) and \\(Y = 0\\). Once we have these, we can approximate the distributions \\(f_{X_1,X_2|Y = 1}\\) and \\(f_{X_1, X_2|Y = 0}\\). We can easily estimate parameters from the data:\n\nparams &lt;- mnist_27$train |&gt; \n  group_by(y) |&gt; \n  summarize(avg_1 = mean(x_1), avg_2 = mean(x_2), \n            sd_1= sd(x_1), sd_2 = sd(x_2), \n            r = cor(x_1, x_2))\nparams\n#&gt; # A tibble: 2 × 6\n#&gt;   y     avg_1 avg_2   sd_1   sd_2     r\n#&gt;   &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 2     0.129 0.283 0.0702 0.0578 0.401\n#&gt; 2 7     0.234 0.288 0.0719 0.105  0.455\n\nHere we provide a visual way of showing the approach. We plot the data and use contour plots to give an idea of what the two estimated normal densities look like (we show the curve representing a region that includes 95% of the points):\n\nmnist_27$train |&gt; mutate(y = factor(y)) |&gt; \n  ggplot(aes(x_1, x_2, fill = y, color = y)) + \n  geom_point(show.legend = FALSE) + \n  stat_ellipse(type = \"norm\", lwd = 1.5)\n\n\n\n\n\n\n\nThis defines the following estimate of \\(f(x_1, x_2)\\).\nWe can use the train function from the caret package to fit the model and obtain predictors:\n\nlibrary(caret)\ntrain_qda &lt;- train(y ~ ., method = \"qda\", data = mnist_27$train)\n\nWe see that we obtain relatively good accuracy:\n\ny_hat &lt;- predict(train_qda, mnist_27$test)\nconfusionMatrix(y_hat, mnist_27$test$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;     0.82\n\nThe estimated conditional probability looks relatively good, although it does not fit as well as the kernel smoothers:\n\n\n\n\n\n\n\n\nOne reason QDA does not work as well as the kernel methods is perhaps because the assumption of normality does not quite hold. Although for the 2s it seems reasonable, for the 7s it does seem to be off. Notice the slight curvature in the points for the 7s:\n\nmnist_27$train |&gt; mutate(y = factor(y)) |&gt; \n  ggplot(aes(x_1, x_2, fill = y, color = y)) + \n  geom_point(show.legend = FALSE) + \n  stat_ellipse(type = \"norm\") +\n  facet_wrap(~y)\n\n\n\n\n\n\n\nQDA can work well here, but it becomes harder to use as the number of predictors increases. Here we have 2 predictors and had to compute 4 means, 4 SDs, and 2 correlations. How many parameters would we have if instead of 2 predictors, we had 10? The main problem comes from estimating correlations for 10 predictors. With 10, we have 45 correlations for each class. In general, the formula is \\(K\\times p(p-1)/2\\), which gets big fast. Once the number of parameters approaches the size of our data, the method becomes impractical due to overfitting.\n\n29.4.4 Linear discriminant analysis\nA relatively simple solution to the problem of having too many parameters is to assume that the correlation structure is the same for all classes, which reduces the number of parameters we need to estimate.\nIn this case, we would compute just one pair of standard deviations and one correlation,  and the distributions looks like this:\n\n\n\n\n\n\n\n\nNow the size of the ellipses as well as the angle are the same. This is because they have the same standard deviations and correlations.\nWe can fit the LDA model using caret:\n\ntrain_lda &lt;- train(y ~ ., method = \"lda\", data = mnist_27$train)\ny_hat &lt;- predict(train_lda, mnist_27$test)\nconfusionMatrix(y_hat, mnist_27$test$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;     0.75\n\nWhen we force this assumption, we can show mathematically that the boundary is a line, just as with logistic regression. For this reason, we call the method linear discriminant analysis (LDA). Similarly, for QDA, we can show that the boundary must be a quadratic function.\n\n#&gt; Warning: The following aesthetics were dropped during statistical\n#&gt; transformation: fill\n#&gt; ℹ This can happen when ggplot fails to infer the correct grouping\n#&gt;   structure in the data.\n#&gt; ℹ Did you forget to specify a `group` aesthetic or to convert a\n#&gt;   numerical variable into a factor?\n#&gt; The following aesthetics were dropped during statistical\n#&gt; transformation: fill\n#&gt; ℹ This can happen when ggplot fails to infer the correct grouping\n#&gt;   structure in the data.\n#&gt; ℹ Did you forget to specify a `group` aesthetic or to convert a\n#&gt;   numerical variable into a factor?\n\n\n\n\n\n\n\nIn the case of LDA, the lack of flexibility does not permit us to capture the non-linearity in the true conditional probability function.\n\n29.4.5 Connection to distance\nThe normal density is:\n\\[\np(x) = \\frac{1}{\\sqrt{2\\pi} \\sigma} \\exp\\left\\{ - \\frac{(x-\\mu)^2}{\\sigma^2}\\right\\}\n\\]\nIf we remove the constant \\(1/(\\sqrt{2\\pi} \\sigma)\\) and then take the log, we get:\n\\[\n- \\frac{(x-\\mu)^2}{\\sigma^2}\n\\]\nwhich is the negative of a distance squared scaled by the standard deviation. For higher dimensions, the same is true except the scaling is more complex and involves correlations.\n\n29.4.6 Case study: more than three classes\nWe can generate an example with three categories like this:\n\nif (!exists(\"mnist\")) mnist &lt;- read_mnist()\nset.seed(3456)\nindex_127 &lt;- sample(which(mnist$train$labels %in% c(1,2,7)), 2000)\ny &lt;- mnist$train$labels[index_127] \nx &lt;- mnist$train$images[index_127,]\nindex_train &lt;- createDataPartition(y, p = 0.8, list = FALSE)\n## get the quadrants\nrow_column &lt;- expand.grid(row = 1:28, col = 1:28) \nupper_left_ind &lt;- which(row_column$col &lt;= 14 & row_column$row &lt;= 14)\nlower_right_ind &lt;- which(row_column$col &gt; 14 & row_column$row &gt; 14)\n## binarize the values. Above 200 is ink, below is no ink\nx &lt;- x &gt; 200 \n## proportion of pixels in lower right quadrant\nx &lt;- cbind(rowSums(x[ ,upper_left_ind])/rowSums(x), \n           rowSums(x[ ,lower_right_ind])/rowSums(x)) \n##save data\ntrain_set &lt;- data.frame(y = factor(y[index_train]),\n                        x_1 = x[index_train,1], x_2 = x[index_train,2])\ntest_set &lt;- data.frame(y = factor(y[-index_train]),\n                       x_1 = x[-index_train,1], x_2 = x[-index_train,2])\n\nHere is the training data:\n\ntrain_set |&gt; ggplot(aes(x_1, x_2, color = y)) + geom_point()\n\n\n\n\n\n\n\nWe can use the caret package to train the QDA model:\n\ntrain_qda &lt;- train(y ~ ., method = \"qda\", data = train_set)\n\nNow we estimate three conditional probabilities (although they have to add to 1):\n\npredict(train_qda, test_set, type = \"prob\") |&gt; head()\n#&gt;        1       2       7\n#&gt; 1 0.7655 0.23043 0.00405\n#&gt; 2 0.2031 0.72514 0.07175\n#&gt; 3 0.5396 0.45909 0.00132\n#&gt; 4 0.0393 0.09419 0.86655\n#&gt; 5 0.9600 0.00936 0.03063\n#&gt; 6 0.9865 0.00724 0.00623\n\nOur predictions are one of the three classes:\n\npredict(train_qda, test_set) |&gt; head()\n#&gt; [1] 1 2 1 7 1 1\n#&gt; Levels: 1 2 7\n\nThe confusion matrix is therefore a 3 by 3 table:\n\nconfusionMatrix(predict(train_qda, test_set), test_set$y)$table\n#&gt;           Reference\n#&gt; Prediction   1   2   7\n#&gt;          1 111   9  11\n#&gt;          2  10  86  21\n#&gt;          7  21  28 102\n\nThe accuracy is 0.7493734\nNote that for sensitivity and specificity, we have a pair of values for each class. To define these terms, we need a binary outcome. We therefore have three columns: one for each class as the positives and the other two as the negatives.\nTo visualize what parts of the region are called 1, 2, and 7 we now need three colors:\n\n\n\n\n\n\n\n\nThe accuracy for LDA, 0.6290727, is much worse because the model is more rigid. This is what the decision rule looks like:\n\n\n\n\n\n\n\n\nThe results for kNN\n\ntrain_knn &lt;- train(y ~ ., method = \"knn\", data = train_set,\n                   tuneGrid = data.frame(k = seq(15, 51, 2)))\n\nare much better with an accuracy of 0.7493734. The decision rule looks like this:\n\n\n\n\n\n\n\n\nNote that one of the limitations of generative models here is due to the lack of fit of the normal assumption, in particular for class 1.\n\ntrain_set |&gt; mutate(y = factor(y)) |&gt; \n  ggplot(aes(x_1, x_2, fill = y, color = y)) + \n  geom_point(show.legend = FALSE) + \n  stat_ellipse(type = \"norm\") \n\n\n\n\n\n\n\nGenerative models can be very powerful, but only when we are able to successfully approximate the joint distribution of predictors conditioned on each class.\n:::{.callout-note}\nYou are now ready to do exercises 14-22."
  },
  {
    "objectID": "ml/algorithms.html#sec-trees",
    "href": "ml/algorithms.html#sec-trees",
    "title": "\n29  Examples of algorithms\n",
    "section": "\n29.5 Classification and regression trees (CART)",
    "text": "29.5 Classification and regression trees (CART)\n\n29.5.1 The curse of dimensionality\nWe described how methods such as LDA and QDA are not meant to be used with many predictors \\(p\\) because the number of parameters that we need to estimate becomes too large. For example, with the digits example \\(p = 784\\), we would have over 600,000 parameters with LDA, and we would multiply that by the number of classes for QDA. Kernel methods such as kNN or local regression do not have model parameters to estimate. However, they also face a challenge when multiple predictors are used due to what is referred to as the curse of dimensionality. The dimension here refers to the fact that when we have \\(p\\) predictors, the distance between two observations is computed in \\(p\\)-dimensional space.\nA useful way of understanding the curse of dimensionality is by considering how large we have to make a span/neighborhood/window to include a given percentage of the data. Remember that with larger neighborhoods, our methods lose flexibility.\nFor example, suppose we have one continuous predictor with equally spaced points in the [0,1] interval and we want to create windows that include 1/10th of data. Then it’s easy to see that our windows have to be of size 0.1:\n\n\n\n\n\n\n\n\nNow, for two predictors, if we decide to keep the neighborhood just as small, 10% for each dimension, we include only 1 point. If we want to include 10% of the data, then we need to increase the size of each side of the square to \\(\\sqrt{.10} \\approx .316\\):\n\n\n\n\n\n\n\n\nUsing the same logic, if we want to include 10% of the data in a three-dimensional space, then the side of each cube is \\(\\sqrt[3]{.10} \\approx 0.464\\). In general, to include 10% of the data in a case with \\(p\\) dimensions, we need an interval with each side of size \\(\\sqrt[p]{.10}\\) of the total. This proportion gets close to 1 quickly, and if the proportion is 1 it means we include all the data and are no longer smoothing.\n\nlibrary(tidyverse)\np &lt;- 1:100\nqplot(p, .1^(1/p), ylim = c(0,1))\n\n\n\n\n\n\n\nBy the time we reach 100 predictors, the neighborhood is no longer very local, as each side covers almost the entire dataset.\nHere we look at a set of elegant and versatile methods that adapt to higher dimensions and also allow these regions to take more complex shapes while still producing models that are interpretable. These are very popular, well-known and studied methods. We will concentrate on regression and decision trees and their extension to random forests.\n\n29.5.2 CART motivation\nTo motivate this section, we will use a new dataset that includes the breakdown of the composition of olive oil into 8 fatty acids:\n\nlibrary(tidyverse)\nlibrary(dslabs)\nnames(olive)\n#&gt;  [1] \"region\"      \"area\"        \"palmitic\"    \"palmitoleic\"\n#&gt;  [5] \"stearic\"     \"oleic\"       \"linoleic\"    \"linolenic\"  \n#&gt;  [9] \"arachidic\"   \"eicosenoic\"\n\nFor illustrative purposes, we will try to predict the region using the fatty acid composition values as predictors.\n\ntable(olive$region)\n#&gt; \n#&gt; Northern Italy       Sardinia Southern Italy \n#&gt;            151             98            323\n\nWe remove the area column because we won’t use it as a predictor.\n\nolive &lt;- select(olive, -area)\n\nLet’s very quickly try to predict the region using kNN:\n\nlibrary(caret)\nfit &lt;- train(region ~ .,  method = \"knn\", \n             tuneGrid = data.frame(k = seq(1, 15, 2)), \n             data = olive)\nggplot(fit)\n\n\n\n\n\n\n\nWe see that using just one neighbor, we can predict relatively well. However, a bit of data exploration reveals that we should be able to do even better. For example, if we look at the distribution of each predictor stratified by region we see that eicosenoic is only present in Southern Italy and that linoleic separates Northern Italy from Sardinia.\n\n\n\n\n\n\n\n\nThis implies that we should be able to build an algorithm that predicts perfectly! We can see this clearly by plotting the values for eicosenoic and linoleic.\n\n\n\n\n\n\n\n\nIn Section Section 20.5 we define predictor spaces. The predictor space here consists of eight-dimensional points with values between 0 and 100. In the plot above, we show the space defined by the two predictors eicosenoic and linoleic, and, by eye, we can construct a prediction rule that partitions the predictor space so that each partition contains only outcomes of a one category. This in turn can be used to define an algorithm with perfect accuracy. Specifically, we define the following decision rule. If eicosenoic is larger than 0.065, predict Southern Italy. If not, then if linoleic is larger than \\(10.535\\), predict Sardinia, and if lower, predict Northern Italy. We can draw this decision tree like this:\n\n\n\n\n\n\n\n\nDecision trees like this are often used in practice. For example, to decide on a person’s risk of poor outcome after having a heart attack, doctors use the following:\n\n\n\n\n\n\n\n\n(Source: Walton 2010 Informal Logic, Vol. 30, No. 2, pp. 159-1843.)\nA tree is basically a flow chart of yes or no questions. The general idea of the methods we are describing is to define an algorithm that uses data to create these trees with predictions at the ends, referred to as nodes. Regression and decision trees operate by predicting an outcome variable \\(Y\\) by partitioning the predictors.\n\n29.5.3 Regression trees\nWhen the outcome is continuous, we call the method a regression tree. To introduce regression trees, we will use the 2008 poll data used in previous sections to describe the basic idea of how we build these algorithms. As with other machine learning algorithms, we will try to estimate the conditional expectation \\(f(x) = \\mbox{E}(Y | X = x)\\) with \\(Y\\) the poll margin and \\(x\\) the day.\n\nqplot(day, margin, data = polls_2008)\n\n\n\n\n\n\n\nThe general idea here is to build a decision tree and, at the end of each node, obtain a predictor \\(\\hat{y}\\). A mathematical way to describe this is to say that we are partitioning the predictor space into \\(J\\) non-overlapping regions, \\(R_1, R_2, \\ldots, R_J\\), and then for any predictor \\(x\\) that falls within region \\(R_j\\), estimate \\(f(x)\\) with the average of the training observations \\(y_i\\) for which the associated predictor \\(x_i\\) is also in \\(R_j\\).\nBut how do we decide on the partition \\(R_1, R_2, \\ldots, R_J\\) and how do we choose \\(J\\)? Here is where the algorithm gets a bit complicated.\nRegression trees create partitions recursively. We start the algorithm with one partition, the entire predictor space. In our simple first example, this space is the interval [-155, 1]. But after the first step we will have two partitions. After the second step we will split one of these partitions into two and will have three partitions, then four, then five, and so on. We describe how we pick the partition to further partition, and when to stop, later.\nOnce we select a partition \\(\\mathbf{x}\\) to split in order to create the new partitions, we find a predictor \\(j\\) and value \\(s\\) that define two new partitions, which we will call \\(R_1(j,s)\\) and \\(R_2(j,s)\\), that split our observations in the current partition by asking if \\(x_j\\) is bigger than \\(s\\):\n\\[\nR_1(j,s) = \\{\\mathbf{x} \\mid x_j &lt; s\\} \\mbox{  and  } R_2(j,s) = \\{\\mathbf{x} \\mid x_j \\geq s\\}\n\\]\nIn our current example we only have one predictor, so we will always choose \\(j = 1\\), but in general this will not be the case. Now, after we define the new partitions \\(R_1\\) and \\(R_2\\), and we decide to stop the partitioning, we compute predictors by taking the average of all the observations \\(y\\) for which the associated \\(\\mathbf{x}\\) is in \\(R_1\\) and \\(R_2\\). We refer to these two as \\(\\hat{y}_{R_1}\\) and \\(\\hat{y}_{R_2}\\) respectively.\nBut how do we pick \\(j\\) and \\(s\\)? Basically we find the pair that minimizes the residual sum of squares (RSS):\n\\[\n\\sum_{i:\\, x_i \\in R_1(j,s)} (y_i - \\hat{y}_{R_1})^2 +\n\\sum_{i:\\, x_i \\in R_2(j,s)} (y_i - \\hat{y}_{R_2})^2\n\\]\nThis is then applied recursively to the new regions \\(R_1\\) and \\(R_2\\). We describe how we stop later, but once we are done partitioning the predictor space into regions, in each region a prediction is made using the observations in that region.\nLet’s take a look at what this algorithm does on the 2008 presidential election poll data. We will use the rpart function in the rpart package.\n\nlibrary(rpart)\nfit &lt;- rpart(margin ~ ., data = polls_2008)\n\nHere, there is only one predictor. Thus we do not have to decide which predictor \\(j\\) to split by, we simply have to decide what value \\(s\\) we use to split. We can visually see where the splits were made:\n\nplot(fit, margin = 0.1)\ntext(fit, cex = 0.75)\n\n\n\n\n\n\n\n\n\nThe first split is made on day 39.5. One of those regions is then split at day 86.5. The two resulting new partitions are split on days 49.5 and 117.5, respectively, and so on. We end up with 8 partitions. The final estimate \\(\\hat{f}(x)\\) looks like this:\n\npolls_2008 |&gt; \n  mutate(y_hat = predict(fit)) |&gt; \n  ggplot() +\n  geom_point(aes(day, margin)) +\n  geom_step(aes(day, y_hat), col = \"red\")\n\n\n\n\n\n\n\nNote that the algorithm stopped partitioning at 8. Now we explain how this decision is made.\nFirst we need to define the term complexity parameter (cp). Every time we split and define two new partitions, our training set RSS decreases. This is because with more partitions, our model has more flexibility to adapt to the training data. In fact, if you split until every point is its own partition, then RSS goes all the way down to 0 since the average of one value is that same value. To avoid this, the algorithm sets a minimum for how much the RSS must improve for another partition to be added. This parameter is referred to as the complexity parameter (cp). The RSS must improve by a factor of cp for the new partition to be added. Large values of cp will therefore force the algorithm to stop earlier which results in fewer nodes.\nHowever, cp is not the only parameter used to decide if we should partition a current partition or not. Another common parameter is the minimum number of observations required in a partition before partitioning it further. The argument used in the rpart function is minsplit and the default is 20. The rpart implementation of regression trees also permits users to determine a minimum number of observations in each node. The argument is minbucket and defaults to round(minsplit/3).\nAs expected, if we set cp = 0 and minsplit = 2, then our prediction is as flexible as possible and our predictor is our original data:\n\nfit &lt;- rpart(margin ~ ., data = polls_2008, \n             control = rpart.control(cp = 0, minsplit = 2))\npolls_2008 |&gt; \n  mutate(y_hat = predict(fit)) |&gt; \n  ggplot() +\n  geom_point(aes(day, margin)) +\n  geom_step(aes(day, y_hat), col = \"red\")\n\n\n\n\n\n\n\nIntuitively we know that this is not a good approach as it will generally result in over-training. These cp, minsplit, and minbucket, three parameters can be used to control the variability of the final predictors. The larger these values are the more data is averaged to compute a predictor and thus reduce variability. The drawback is that it restricts flexibility.\nSo how do we pick these parameters? We can use cross validation, described in Chapter Chapter 28, just like with any tuning parameter. Here is an example of using cross validation to choose cp.\n\nlibrary(caret)\ntrain_rpart &lt;- train(margin ~ ., \n                     method = \"rpart\",\n                     tuneGrid = data.frame(cp = seq(0, 0.05, len = 25)),\n                     data = polls_2008)\nggplot(train_rpart)\n\n\n\n\n\n\n\nTo see the resulting tree, we access the finalModel and plot it:\n\nplot(train_rpart$finalModel, margin = 0.1)\ntext(train_rpart$finalModel, cex = 0.75)\n\n\n\n\n\n\n\n\n\nAnd because we only have one predictor, we can actually plot \\(\\hat{f}(x)\\):\n\npolls_2008 |&gt; \n  mutate(y_hat = predict(train_rpart)) |&gt; \n  ggplot() +\n  geom_point(aes(day, margin)) +\n  geom_step(aes(day, y_hat), col = \"red\")\n\n\n\n\n\n\n\nNote that if we already have a tree and want to apply a higher cp value, we can use the prune function. We call this pruning a tree because we are snipping off partitions that do not meet a cp criterion. We previously created a tree that used a cp = 0 and saved it to fit. We can prune it like this:\n\npruned_fit &lt;- prune(fit, cp = 0.01)\n\n\n29.5.4 Classification (decision) trees\nClassification trees, or decision trees, are used in prediction problems where the outcome is categorical. We use the same partitioning principle with some differences to account for the fact that we are now working with a categorical outcome.\nThe first difference is that we form predictions by calculating which class is the most common among the training set observations within the partition, rather than taking the average in each partition (as we can’t take the average of categories).\nThe second is that we can no longer use RSS to choose the partition. While we could use the naive approach of looking for partitions that minimize training error, better performing approaches use more sophisticated metrics. Two of the more popular ones are the Gini Index and Entropy.\nIn a perfect scenario, the outcomes in each of our partitions are all of the same category since this will permit perfect accuracy. The Gini Index is going to be 0 in this scenario, and become larger the more we deviate from this scenario. To define the Gini Index, we define \\(\\hat{p}_{j,k}\\) as the proportion of observations in partition \\(j\\) that are of class \\(k\\). The Gini Index is defined as\n\\[\n\\mbox{Gini}(j) = \\sum_{k = 1}^K \\hat{p}_{j,k}(1-\\hat{p}_{j,k})\n\\]\nIf you study the formula carefully you will see that it is in fact 0 in the perfect scenario described above.\nEntropy is a very similar quantity, defined as\n\\[\n\\mbox{entropy}(j) = -\\sum_{k = 1}^K \\hat{p}_{j,k}\\log(\\hat{p}_{j,k}), \\mbox{ with } 0 \\times \\log(0) \\mbox{ defined as }0\n\\]\nLet us look at how a classification tree performs on the digits example we examined before by using this code to run the algorithm and plot the resulting accuracy:\n\ntrain_rpart &lt;- train(y ~ .,\n                     method = \"rpart\",\n                     tuneGrid = data.frame(cp = seq(0.0, 0.1, len = 25)),\n                     data = mnist_27$train)\nplot(train_rpart)\n\n\n\n\n\n\n\nThe accuracy achieved by this approach is better than what we got with regression, but is not as good as what we achieved with kernel methods:\n\ny_hat &lt;- predict(train_rpart, mnist_27$test)\nconfusionMatrix(y_hat, mnist_27$test$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;     0.82\n\nThe plot of the estimated conditional probability shows us the limitations of classification trees:\n\n\n\n\n\n\n\n\nNote that with decision trees, it is difficult to make the boundaries smooth since each partition creates a discontinuity.\nClassification trees have certain advantages that make them very useful. They are highly interpretable, even more so than linear models. They are easy to visualize (if small enough). Finally, they can model human decision processes and don’t require use of dummy predictors for categorical variables. On the other hand, the approach via recursive partitioning can easily over-train and is therefore a bit harder to train than, for example, linear regression or kNN. Furthermore, in terms of accuracy, it is rarely the best performing method since it is not very flexible and is highly unstable to changes in training data. Random forests, explained next, improve on several of these shortcomings.\n\n29.5.5 Random forests\nRandom forests are a very popular machine learning approach that addresses the shortcomings of decision trees using a clever idea. The goal is to improve prediction performance and reduce instability by averaging multiple decision trees (a forest of trees constructed with randomness). It has two features that help accomplish this.\nThe first step is bootstrap aggregation or bagging. The general idea is to generate many predictors, each using regression or classification trees, and then forming a final prediction based on the average prediction of all these trees. To assure that the individual trees are not the same, we use the bootstrap to induce randomness. These two features combined explain the name: the bootstrap makes the individual trees randomly different, and the combination of trees is the forest. The specific steps are as follows.\n1. Build \\(B\\) decision trees using the training set. We refer to the fitted models as \\(T_1, T_2, \\dots, T_B\\). We later explain how we ensure they are different.\n2. For every observation in the test set, form a prediction \\(\\hat{y}_j\\) using tree \\(T_j\\).\n3. For continuous outcomes, form a final prediction with the average \\(\\hat{y} = \\frac{1}{B} \\sum_{j = 1}^B \\hat{y}_j\\). For categorical data classification, predict \\(\\hat{y}\\) with majority vote (most frequent class among \\(\\hat{y}_1, \\dots, \\hat{y}_T\\)).\nSo how do we get different decision trees from a single training set? For this, we use randomness in two ways which we explain in the steps below. Let \\(N\\) be the number of observations in the training set. To create \\(T_j, \\, j = 1,\\ldots,B\\) from the training set we do the following:\n1. Create a bootstrap training set by sampling \\(N\\) observations from the training set with replacement. This is the first way to induce randomness.\n2. A large number of features is typical in machine learning challenges. Often, many features can be informative but including them all in the model may result in overfitting. The second way random forests induce randomness is by randomly selecting features to be included in the building of each tree. A different random subset is selected for each tree. This reduces correlation between trees in the forest, thereby improving prediction accuracy.\nTo illustrate how the first steps can result in smoother estimates we will demonstrate by fitting a random forest to the 2008 polls data. We will use the randomForest function in the randomForest package:\n\nlibrary(randomForest)\nfit &lt;- randomForest(margin~., data = polls_2008) \n\nNote that if we apply the function plot to the resulting object, stored in fit, we see how the error rate of our algorithm changes as we add trees.\n\nrafalib::mypar()\nplot(fit)\n\n\n\n\n\n\n\n\n\nWe can see that in this case, the accuracy improves as we add more trees until about 30 trees where accuracy stabilizes.\nThe resulting estimate for this random forest can be seen like this:\n\npolls_2008 |&gt;\n  mutate(y_hat = predict(fit, newdata = polls_2008)) |&gt; \n  ggplot() +\n  geom_point(aes(day, margin)) +\n  geom_line(aes(day, y_hat), col = \"red\")\n\n\n\n\n\n\n\nNotice that the random forest estimate is much smoother than what we achieved with the regression tree in the previous section. This is possible because the average of many step functions can be smooth. We can see this by visually examining how the estimate changes as we add more trees. In the following figure you see each of the bootstrap samples for several values of \\(b\\) and for each one we see the tree that is fitted in grey, the previous trees that were fitted in lighter grey, and the result of averaging all the trees estimated up to that point.\n\n\n\n\n\n\n\n\nHere is the random forest fit for our digits example based on two predictors:\n\nlibrary(randomForest)\ntrain_rf &lt;- randomForest(y ~ ., data = mnist_27$train)\n\nconfusionMatrix(predict(train_rf, mnist_27$test),\n                mnist_27$test$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;     0.79\n\nHere is what the conditional probabilities look like:\n\n#&gt; Warning: The following aesthetics were dropped during statistical\n#&gt; transformation: fill\n#&gt; ℹ This can happen when ggplot fails to infer the correct grouping\n#&gt;   structure in the data.\n#&gt; ℹ Did you forget to specify a `group` aesthetic or to convert a\n#&gt;   numerical variable into a factor?\n#&gt; The following aesthetics were dropped during statistical\n#&gt; transformation: fill\n#&gt; ℹ This can happen when ggplot fails to infer the correct grouping\n#&gt;   structure in the data.\n#&gt; ℹ Did you forget to specify a `group` aesthetic or to convert a\n#&gt;   numerical variable into a factor?\n\n\n\n\n\n\n\nVisualizing the estimate shows that, although we obtain high accuracy, it appears that there is room for improvement by making the estimate smoother. This could be achieved by changing the parameter that controls the minimum number of data points in the nodes of the tree. The larger this minimum, the smoother the final estimate will be. We can train the parameters of the random forest. Below, we use the caret package to optimize over the minimum node size. Because, this is not one of the parameters that the caret package optimizes by default we will write our own code:\n\nnodesize &lt;- seq(1, 51, 10)\nacc &lt;- sapply(nodesize, function(ns){\n  train(y ~ ., method = \"rf\", data = mnist_27$train,\n               tuneGrid = data.frame(mtry = 2),\n               nodesize = ns)$results$Accuracy\n})\nqplot(nodesize, acc)\n\n\n\n\n\n\n\nWe can now fit the random forest with the optimized minimun node size to the entire training data and evaluate performance on the test data.\n\ntrain_rf_2 &lt;- randomForest(y ~ ., data = mnist_27$train,\n                           nodesize = nodesize[which.max(acc)])\n\nconfusionMatrix(predict(train_rf_2, mnist_27$test),\n                mnist_27$test$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;     0.83\n\nThe selected model improves accuracy and provides a smoother estimate.\n\n#&gt; Warning: The following aesthetics were dropped during statistical\n#&gt; transformation: fill\n#&gt; ℹ This can happen when ggplot fails to infer the correct grouping\n#&gt;   structure in the data.\n#&gt; ℹ Did you forget to specify a `group` aesthetic or to convert a\n#&gt;   numerical variable into a factor?\n#&gt; The following aesthetics were dropped during statistical\n#&gt; transformation: fill\n#&gt; ℹ This can happen when ggplot fails to infer the correct grouping\n#&gt;   structure in the data.\n#&gt; ℹ Did you forget to specify a `group` aesthetic or to convert a\n#&gt;   numerical variable into a factor?\n\n\n\n\n\n\n\nNote that we can avoid writing our own code by using other random forest implementations as described in the caret manual4.\nRandom forest performs better in all the examples we have considered. However, a disadvantage of random forests is that we lose interpretability. An approach that helps with interpretability is to examine variable importance. To define variable importance we count how often a predictor is used in the individual trees. You can learn more about variable importance in an advanced machine learning book5. The caret package includes the function varImp that extracts variable importance from any model in which the calculation is implemented. We give an example on how we use variable importance in the next section."
  },
  {
    "objectID": "ml/algorithms.html#exercises",
    "href": "ml/algorithms.html#exercises",
    "title": "\n29  Examples of algorithms\n",
    "section": "\n29.6 Exercises",
    "text": "29.6 Exercises\n1. Create a dataset using the following code.\n\nn &lt;- 100\nSigma &lt;- 9*matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2)\ndat &lt;- MASS::mvrnorm(n = 100, c(69, 69), Sigma) |&gt;\n  data.frame() |&gt; setNames(c(\"x\", \"y\"))\n\nUse the caret package to partition into a test and training set of equal size. Train a linear model and report the RMSE. Repeat this exercise 100 times and make a histogram of the RMSEs and report the average and standard deviation. Hint: adapt the code shown earlier like this:\n\ny &lt;- dat$y\ntest_index &lt;- createDataPartition(y, times = 1, p = 0.5, list = FALSE)\ntrain_set &lt;- dat |&gt; slice(-test_index)\ntest_set &lt;- dat |&gt; slice(test_index)\nfit &lt;- lm(y ~ x, data = train_set)\ny_hat &lt;- fit$coef[1] + fit$coef[2]*test_set$x\nsqrt(mean((y_hat - test_set$y)^2))\n\nand put it inside a call to replicate.\n2. Now we will repeat the above but using larger datasets. Repeat exercise 1 but for datasets with n &lt;- c(100, 500, 1000, 5000, 10000). Save the average and standard deviation of RMSE from the 100 repetitions. Hint: use the sapply or map functions.\n3. Describe what you observe with the RMSE as the size of the dataset becomes larger.\n\nOn average, the RMSE does not change much as n gets larger, while the variability of RMSE does decrease.\nBecause of the law of large numbers, the RMSE decreases: more data, more precise estimates.\n\nn = 10000 is not sufficiently large. To see a decrease in RMSE, we need to make it larger.\nThe RMSE is not a random variable.\n\n4. Now repeat exercise 1, but this time make the correlation between x and y larger by changing Sigma like this:\n\nn &lt;- 100\nSigma &lt;- 9*matrix(c(1, 0.95, 0.95, 1), 2, 2)\ndat &lt;- MASS::mvrnorm(n = 100, c(69, 69), Sigma) |&gt;\n  data.frame() |&gt; setNames(c(\"x\", \"y\"))\n\nRepeat the exercise and note what happens to the RMSE now.\n5. Which of the following best explains why the RMSE in exercise 4 is so much lower than exercise 1.\n\nIt is just luck. If we do it again, it will be larger.\nThe Central Limit Theorem tells us the RMSE is normal.\nWhen we increase the correlation between x and y, x has more predictive power and thus provides a better estimate of y. This correlation has a much bigger effect on RMSE than n. Large n simply provide us more precise estimates of the linear model coefficients.\nThese are both examples of regression, so the RMSE has to be the same.\n\n6. Create a dataset using the following code:\n\nn &lt;- 1000\nSigma &lt;- matrix(c(1, 3/4, 3/4, 3/4, 1, 0, 3/4, 0, 1), 3, 3)\ndat &lt;- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) |&gt;\n  data.frame() |&gt; setNames(c(\"y\", \"x_1\", \"x_2\"))\n\nNote that y is correlated with both x_1 and x_2, but the two predictors are independent of each other.\n\ncor(dat)\n\nUse the caret package to partition into a test and training set of equal size. Compare the RMSE when using just x_1, just x_2, and both x_1 and x_2. Train a linear model and report the RMSE.\n7. Repeat exercise 6 but now create an example in which x_1 and x_2 are highly correlated:\n\nn &lt;- 1000\nSigma &lt;- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.95, 0.75, 0.95, 1.0), 3, 3)\ndat &lt;- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) |&gt;\n  data.frame() |&gt; setNames(c(\"y\", \"x_1\", \"x_2\"))\n\nUse the caret package to partition into a test and training set of equal size. Compare the RMSE when using just x_1, just x_2, and both x_1 and x_2 Train a linear model and report the RMSE.\n8. Compare the results in 6 and 7 and choose the statement you agree with:\n\nAdding extra predictors can improve RMSE substantially, but not when they are highly correlated with another predictor.\nAdding extra predictors improves predictions equally in both exercises.\nAdding extra predictors results in over fitting.\nUnless we include all predictors, we have no predicting power.\n\n9. Define the following dataset:\n\nmake_data &lt;- function(n = 1000, p = 0.5, \n                      mu_0 = 0, mu_1 = 2, \n                      sigma_0 = 1,  sigma_1 = 1){\n  y &lt;- rbinom(n, 1, p)\n  f_0 &lt;- rnorm(n, mu_0, sigma_0)\n  f_1 &lt;- rnorm(n, mu_1, sigma_1)\n  x &lt;- ifelse(y == 1, f_1, f_0)\n  test_index &lt;- createDataPartition(y, times = 1, p = 0.5, list = FALSE)\n  list(train = data.frame(x = x, y = as.factor(y)) |&gt; \n         slice(-test_index),\n       test = data.frame(x = x, y = as.factor(y)) |&gt; \n         slice(test_index))\n}\n\nNote that we have defined a variable x that is predictive of a binary outcome y.\n\ndat$train |&gt; ggplot(aes(x, color = y)) + geom_density()\n\nCompare the accuracy of linear regression and logistic regression.\n10. Repeat the simulation from exercise 1 100 times and compare the average accuracy for each method and notice they give practically the same answer.\n11. Generate 25 different datasets changing the difference between the two class: delta &lt;- seq(0, 3, len = 25). Plot accuracy versus delta.\n12. Earlier we used logistic regression to predict sex from height. Use kNN to do the same. Use the code described in this chapter to select the \\(F_1\\) measure and plot it against \\(k\\). Compare to the \\(F_1\\) of about 0.6 we obtained with regression.\n13. Load the following dataset:\nThis dataset includes a matrix x:\n\ndim(tissue_gene_expression$x)\n\nwith the gene expression measured on 500 genes for 189 biological samples representing seven different tissues. The tissue type is stored in y:\n\ntable(tissue_gene_expression$y)\n\nSplit the data in training and test sets, then use kNN to predict tissue type and see what accuracy you obtain. Try it for \\(k = 1, 3, \\dots, 11\\).\n14. We are going to apply LDA and QDA to the tissue_gene_expression dataset. We will start with simple examples based on this dataset and then develop a realistic example.\nCreate a dataset with just the classes “cerebellum” and “hippocampus” (two parts of the brain) and a predictor matrix with 10 randomly selected columns.\n\nset.seed(1993)\ntissues &lt;- c(\"cerebellum\", \"hippocampus\")\nind &lt;- which(tissue_gene_expression$y %in% tissues)\ny &lt;- droplevels(tissue_gene_expression$y[ind])\nx &lt;- tissue_gene_expression$x[ind, ]\nx &lt;- x[, sample(ncol(x), 10)]\n\nUse the train function to estimate the accuracy of LDA.\n15. In this case, LDA fits two 10-dimensional normal distributions. Look at the fitted model by looking at the finalModel component of the result of train. Notice there is a component called means that includes the estimate means of both distributions. Plot the mean vectors against each other and determine which predictors (genes) appear to be driving the algorithm.\n16. Repeat exercises 1 with QDA. Does it have a higher accuracy than LDA?\n17. Are the same predictors (genes) driving the algorithm? Make a plot as in exercise 2.\n18. One thing we see in the previous plot is that the value of predictors correlate in both groups: some predictors are low in both groups while others are high in both groups. The mean value of each predictor, colMeans(x), is not informative or useful for prediction, and often for interpretation purposes it is useful to center or scale each column. This can be achieved with the preProcessing argument in train. Re-run LDA with preProcessing = \"scale\". Note that accuracy does not change but see how it is easier to identify the predictors that differ more between groups in the plot made in exercise 4.\n19. In the previous exercises we saw that both approaches worked well. Plot the predictor values for the two genes with the largest differences between the two groups in a scatterplot to see how they appear to follow a bivariate distribution as assumed by the LDA and QDA approaches. Color the points by the outcome.\n20. Now we are going to increase the complexity of the challenge slightly: we will consider all the tissue types.\n\nset.seed(1993)\ny &lt;- tissue_gene_expression$y\nx &lt;- tissue_gene_expression$x\nx &lt;- x[, sample(ncol(x), 10)]\n\nWhat accuracy do you get with LDA?\n21. We see that the results are slightly worse. Use the confusionMatrix function to learn what type of errors we are making.\n22. Plot an image of the centers of the seven 10-dimensional normal distributions.\n23. Create a simple dataset where the outcome grows 0.75 units on average for every increase in a predictor:\n\nn &lt;- 1000\nsigma &lt;- 0.25\nx &lt;- rnorm(n, 0, 1)\ny &lt;- 0.75 * x + rnorm(n, 0, sigma)\ndat &lt;- data.frame(x = x, y = y)\n\nUse rpart to fit a regression tree and save the result to fit.\n24. Plot the final tree so that you can see where the partitions occurred.\n25. Make a scatterplot of y versus x along with the predicted values based on the fit.\n26. Now model with a random forest instead of a regression tree using randomForest from the randomForest package, and remake the scatterplot with the prediction line.\n27. Use the function plot to see if the random forest has converged or if we need more trees.\n28. It seems that the default values for the random forest result in an estimate that is too flexible (not smooth). Re-run the random forest but this time with nodesize set at 50 and maxnodes set at 25. Remake the plot.\n29. We see that this yields smoother results. Let’s use the train function to help us pick these values. From the caret manual6 we see that we can’t tune the maxnodes parameter or the nodesize argument with randomForest, so we will use the Rborist package and tune the minNode argument. Use the train function to try values minNode &lt;- seq(5, 250, 25). See which value minimizes the estimated RMSE.\n30. Make a scatterplot along with the prediction from the best fitted model.\n31. Use the rpart function to fit a classification tree to the tissue_gene_expression dataset. Use the train function to estimate the accuracy. Try out cp values of seq(0, 0.05, 0.01). Plot the accuracy to report the results of the best model.\n32. Study the confusion matrix for the best fitting classification tree. What do you observe happening for placenta?\n33. Notice that placentas are called endometrium more often than placenta. Note also that the number of placentas is just six, and that, by default, rpart requires 20 observations before splitting a node. Thus it is not possible with these parameters to have a node in which placentas are the majority. Rerun the above analysis but this time permit rpart to split any node by using the argument control = rpart.control(minsplit = 0). Does the accuracy increase? Look at the confusion matrix again.\n34. Plot the tree from the best fitting model obtained in exercise 11.\n35. We can see that with just six genes, we are able to predict the tissue type. Now let’s see if we can do even better with a random forest. Use the train function and the rf method to train a random forest. Try out values of mtry ranging from, at least, seq(50, 200, 25). What mtry value maximizes accuracy? To permit small nodesize to grow as we did with the classification trees, use the following argument: nodesize = 1. This will take several seconds to run. If you want to test it out, try using smaller values with ntree. Set the seed to 1990.\n36. Use the function varImp on the output of train and save it to an object called imp.\n37. The rpart model we ran above produced a tree that used just six predictors. Extracting the predictor names is not straightforward, but can be done. If the output of the call to train was fit_rpart, we can extract the names like this:\n\nind &lt;- !(fit_rpart$finalModel$frame$var == \"&lt;leaf&gt;\")\ntree_terms &lt;- \n  fit_rpart$finalModel$frame$var[ind] |&gt;\n  unique() |&gt;\n  as.character()\ntree_terms\n\nWhat is the variable importance in the random forest call for these predictors? Where do they rank?\n38. Advanced: Extract the top 50 predictors based on importance, take a subset of x with just these predictors and apply the function heatmap to see how these genes behave across the tissues. We will introduce the heatmap function in Chapter Chapter 31."
  },
  {
    "objectID": "ml/algorithms.html#footnotes",
    "href": "ml/algorithms.html#footnotes",
    "title": "\n29  Examples of algorithms\n",
    "section": "",
    "text": "http://www.amazon.com/Mathematical-Statistics-Analysis-Available-Enhanced/dp/0534399428↩︎\nhttps://web.stanford.edu/~hastie/Papers/ESLII.pdf↩︎\nhttps://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID1759289_code1486039.pdf?abstractid = 1759289&mirid = 1&type = 2↩︎\nhttp://topepo.github.io/caret/available-models.html↩︎\nhttps://web.stanford.edu/~hastie/Papers/ESLII.pdf↩︎\nhttps://topepo.github.io/caret/available-models.html↩︎"
  },
  {
    "objectID": "ml/ml-in-practice.html#sec-caret",
    "href": "ml/ml-in-practice.html#sec-caret",
    "title": "\n30  Machine learning in practice\n",
    "section": "\n30.1 The caret package",
    "text": "30.1 The caret package\nWe have already learned about several machine learning algorithms. Many of these algorithms are implemented in R. However, they are distributed via different packages, developed by different authors, and often use different syntax. The caret package tries to consolidate these differences and provide consistency. It currently includes over 200 different methods which are summarized in the caret package manual1. Keep in mind that caret does not include the needed packages and, to implement a package through caret, you still need to install the library. The required packages for each method are described in the package manual. The caret package also provides a function that performs cross validation for us. Here we provide some examples showing how we use this incredibly helpful package. We will use the 2 or 7 example to illustrate and in later sections we use use the package to run algorithms on the larger MNIST datset.\n\n30.1.1 The train functon\nThe caret train function lets us train different algorithms using similar syntax. So, for example, we can type:\n\nlibrary(caret)\n#&gt; Loading required package: lattice\ntrain_glm &lt;- train(y ~ ., method = \"glm\", data = mnist_27$train)\ntrain_knn &lt;- train(y ~ ., method = \"knn\", data = mnist_27$train)\n\nTo make predictions, we can use the output of this function directly without needing to look at the specifics of predict.glm and predict.knn. Instead, we can learn how to obtain predictions from predict.train.\nThe code looks the same for both methods:\n\ny_hat_glm &lt;- predict(train_glm, mnist_27$test, type = \"raw\")\ny_hat_knn &lt;- predict(train_knn, mnist_27$test, type = \"raw\")\n\nThis permits us to quickly compare the algorithms. For example, we can compare the accuracy like this:\n\nfits &lt;- list(glm = y_hat_glm, knn = y_hat_knn)\nsapply(fits, function(fit) confusionMatrix(fit, mnist_27$test$y)$overall[[\"Accuracy\"]])\n#&gt;  glm  knn \n#&gt; 0.75 0.84\n\n\n30.1.2 Cross validation\nWhen an algorithm includes a tuning parameter, train automatically uses cross validation to decide among a few default values. To find out what parameter or parameters are optimized, you can read the manual 2 or study the output of:\n\ngetModelInfo(\"knn\")\n\nWe can also use a quick lookup like this:\n\nmodelLookup(\"knn\")\n\nIf we run it with default values:\n\ntrain_knn &lt;- train(y ~ ., method = \"knn\", data = mnist_27$train)\n\nyou can quickly see the results of the cross validation using the ggplot function. The argument highlight highlights the max:\n\nggplot(train_knn, highlight = TRUE)\n\n\n\n\n\n\n\nBy default, the cross validation is performed by taking 25 bootstrap samples comprised of 25% of the observations. For the kNN method, the default is to try \\(k=5,7,9\\). We change this using the tuneGrid parameter. The grid of values must be supplied by a data frame with the parameter names as specified in the modelLookup output.\nHere, we present an example where we try out 30 values between 9 and 67. To do this with caret, we need to define a column named k, so we use this: data.frame(k = seq(9, 67, 2)). Note that when running this code, we are fitting 30 versions of kNN to 25 bootstrapped samples. Since we are fitting \\(30 \\times 25 = 750\\) kNN models, running this code will take several seconds. We set the seed because cross validation is a random procedure and we want to make sure the result here is reproducible.\n\nset.seed(2008)\ntrain_knn &lt;- train(y ~ ., method = \"knn\", \n                   data = mnist_27$train,\n                   tuneGrid = data.frame(k = seq(9, 71, 2)))\nggplot(train_knn, highlight = TRUE)\n\n\n\n\n\n\n\nTo access the parameter that maximized the accuracy, you can use this:\n\ntrain_knn$bestTune\n#&gt;     k\n#&gt; 10 27\n\nand the best performing model like this:\n\ntrain_knn$finalModel\n#&gt; 27-nearest neighbor model\n#&gt; Training set outcome distribution:\n#&gt; \n#&gt;   2   7 \n#&gt; 379 421\n\nThe function predict will use this best performing model. Here is the accuracy of the best model when applied to the test set, which we have not used at all yet because the cross validation was done on the training set:\n\nconfusionMatrix(predict(train_knn, mnist_27$test, type = \"raw\"),\n                mnist_27$test$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.835\n\nIf we want to change how we perform cross validation, we can use the trainControl function. We can make the code above go a bit faster by using, for example, 10-fold cross validation. This means we have 10 samples using 10% of the observations each. We accomplish this using the following code:\n\ncontrol &lt;- trainControl(method = \"cv\", number = 10, p = .9)\ntrain_knn_cv &lt;- train(y ~ ., method = \"knn\", \n                   data = mnist_27$train,\n                   tuneGrid = data.frame(k = seq(9, 71, 2)),\n                   trControl = control)\nggplot(train_knn_cv, highlight = TRUE)\n\n\n\n\n\n\n\nWe notice that the accuracy estimates are more variable, which is expected since we changed the number of samples used to estimate accuracy.\nNote that results component of the train output includes several summary statistics related to the variability of the cross validation estimates:\n\nnames(train_knn$results)\n#&gt; [1] \"k\"          \"Accuracy\"   \"Kappa\"      \"AccuracySD\" \"KappaSD\"\n\nWe have only covered the basics. To caret package manual 3 includes many more details."
  },
  {
    "objectID": "ml/ml-in-practice.html#preprocessing",
    "href": "ml/ml-in-practice.html#preprocessing",
    "title": "\n30  Machine learning in practice\n",
    "section": "\n30.2 Preprocessing",
    "text": "30.2 Preprocessing\nIn machine learning, we often transform predictors before running the machine algorithm. We also remove predictors that are clearly not useful. We call these steps preprocessing.\nExamples of preprocessing include standardizing the predictors, taking the log transform of some predictors, removing predictors that are highly correlated with others, and removing predictors with very few non-unique values or close to zero variation. We show an example below.\nWe can run the nearZero function from the caret package to see that several features do not vary much from observation to observation. We can see that there is a large number of features with 0 variability:\n\nlibrary(matrixStats)\nsds &lt;- colSds(x)\nhist(sds, breaks = 256)\n\n\n\n\n\n\n\nThis is expected because there are parts of the image that rarely contain writing (dark pixels).\nThe caret packages includes a function that recommends features to be removed due to near zero variance:\n\nnzv &lt;- nearZeroVar(x)\n\nWe can see the columns recommended for removal:\n\nimage(matrix(1:784 %in% nzv, 28, 28))\n\n\nrafalib::mypar()\nimage(matrix(1:784 %in% nzv, 28, 28))\n\n\n\n\n\n\n\nSo we end up keeping this number of columns:\n\ncol_index &lt;- setdiff(1:ncol(x), nzv)\nlength(col_index)\n#&gt; [1] 252\n\nNow we are ready to fit some models. Before we start, we need to add column names to the feature matrices as these are required by caret:\n\ncolnames(x) &lt;- 1:ncol(mnist$train$images)\ncolnames(x_test) &lt;- colnames(x)"
  },
  {
    "objectID": "ml/ml-in-practice.html#k-nearest-neighbors",
    "href": "ml/ml-in-practice.html#k-nearest-neighbors",
    "title": "\n30  Machine learning in practice\n",
    "section": "\n30.3 k-nearest neighbors",
    "text": "30.3 k-nearest neighbors\nBefore starting this section, be warned that the first two calls to the train function in the code below can take several hours to run. This is common challenge when training machine learning algorithms since we have to run the algorithm for each cross validation split and each set of tuning parameter being considered. In the next section, we will provide some suggestion on how to predict the duration of the process and ways to reduce.\nThe first step is to optimize for \\(k\\).\n\ntrain_knn &lt;- train(x[ ,col_index], y, \n                   method = \"knn\", \n                   tuneGrid = data.frame(k = seq(3, 13, 2)))\n\nOnce we optimize our algorithm, we can fit it to the entire dataset:\n\nfit_knn &lt;- knn3(x[, col_index], y,  k = train_knn$bestTune)\n\nWe achieve a high accuracy:\n\ny_hat_knn &lt;- predict(fit_knn, x_test[, col_index], type = \"class\")\nconfusionMatrix(y_hat_knn, factor(y_test))$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.945\n\nAn alternative to removing low variance columns directly, is to use dimension reduction on the feature matrix before applying the algorithms. It is important that we not use the test set when finding the PCs nor any summary of the data, as this could result in overtraining. So we start by applying prcomp to the training data:\n\ncol_means &lt;- colMeans(x)\npca &lt;- prcomp(sweep(x, 2, col_means), center = FALSE)\n\nNext, and run knn on just a small number of dimensions. We try 36 dimensions since this explains about 80% of the data.\n\nk &lt;- 36\nx_train &lt;- pca$x[,1:k]\ntrain_knn &lt;- train(x_train, y, \n                   method = \"knn\", \n                   tuneGrid = data.frame(k = seq(3, 13, 2)))\nfit &lt;- knn3(x_train, y, k = train_knn$bestTune)\n\nNow we apply the transformation we learned with the training data to the test data, reduce the dimension, and then run predict. Note that we used the rotation and column means estimated from the training data.\n\ny_hat &lt;- predict(fit, sweep(x_test, 2, col_means) %*% pca$rotation[,1:k], type = \"class\")\nconfusionMatrix(y_hat, factor(y_test))$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.962\n\nWith obtain an improvement in accuracy, while using only 36 dimensions."
  },
  {
    "objectID": "ml/ml-in-practice.html#random-forest",
    "href": "ml/ml-in-practice.html#random-forest",
    "title": "\n30  Machine learning in practice\n",
    "section": "\n30.4 Random Forest",
    "text": "30.4 Random Forest\nWith the random forest algorithm several parameters can be optimized, but the main one is mtry, the number of predictors that are randomly selected for each tree. This is also the only tuning parameter that the caret function train permits when using the default implementation from the randomForest package.\n\nlibrary(randomForest)\ngrid &lt;- data.frame(mtry = seq(3, 24, 3))\ntrain_rf &lt;-  train(x[, col_index], y, \n                   method = \"rf\", \n                   tuneGrid = grid)\n\nNow that we have optimized our algorithm, we are ready to fit our final model:\n\nfit_rf &lt;- randomForest(x[, col_index], y, mtry = train_rf$bestTune$mtry)\n\n\n#&gt; randomForest 4.7-1.1\n#&gt; Type rfNews() to see new features/changes/bug fixes.\n#&gt; \n#&gt; Attaching package: 'randomForest'\n#&gt; The following object is masked from 'package:ggplot2':\n#&gt; \n#&gt;     margin\n\nAs with kNN, we also achieve high accuracy:\n\ny_hat_rf &lt;- predict(fit_rf, x_test[ ,col_index])\nconfusionMatrix(y_hat_rf, y_test)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.954\n\nBy optimizing some of the other algorithm parameters we can achieve even higher accuracy."
  },
  {
    "objectID": "ml/ml-in-practice.html#testing-and-improving-computation-time",
    "href": "ml/ml-in-practice.html#testing-and-improving-computation-time",
    "title": "\n30  Machine learning in practice\n",
    "section": "\n30.5 Testing and improving computation time",
    "text": "30.5 Testing and improving computation time\nThe default method for estimating accuracy used by the train function is to test prediction on 25 bootstrap samples. This can result in long compute times. For examples, if we are considering several values, say 10, of the tuning parameters, we will fit the algorithm 250 times. We can use the system.time function to estimate the how long it takes to run the algorithm once\n\nsystem.time(fit_rf &lt;- randomForest(x[, col_index], y,  mtry = 9))\n#&gt;    user  system elapsed \n#&gt;   60.61    0.59   61.25\n\nand use this to estimate the total time for the 250 iterations. In this case it will be several hours.\nOne way to reduce run time is to use k-fold cross validation with a smaller number of test sets. A popular choice is leaving out 10 test sets with 10% of the data:\n\ncontrol &lt;- trainControl(method = \"cv\", number = 10, p = .9)\n\nand re-running the train function with this choice specified via the trControl argument:\n\ntrain_rf &lt;-  train(x[, col_index], y, \n                   method = \"rf\", \n                   tuneGrid = grid,\n                   trControl = control)\n\nFor random forest we can also speed up the training step by running less trees per fit. After running the algorithm once, we can use the plot function to see how the error rate changes as the number of trees grows. Here we\n\nplot(fit_rf)\n\n\n\n\n\n\n\nWe can see that error rate stabilizes after about 200 trees. We can use this finding to speed up the cross validation procedure. Specifically, because the default is 500, by adding the argument ntree = 200 to the call to train above, the procedure will finish 2.5 times faster."
  },
  {
    "objectID": "ml/ml-in-practice.html#variable-importance",
    "href": "ml/ml-in-practice.html#variable-importance",
    "title": "\n30  Machine learning in practice\n",
    "section": "\n30.6 Variable importance",
    "text": "30.6 Variable importance\nThe following function computes the importance of each feature:\n\nimp &lt;- importance(fit_rf)\n\nWe can see which features are being used most by plotting an image:\n\nmat &lt;- rep(0, ncol(x))\nmat[col_index] &lt;- imp\nimage(matrix(mat, 28, 28))\n\n\nrafalib::mypar()\nmat &lt;- rep(0, ncol(x))\nmat[col_index] &lt;- imp\nimage(matrix(mat, 28, 28))"
  },
  {
    "objectID": "ml/ml-in-practice.html#visual-assessments",
    "href": "ml/ml-in-practice.html#visual-assessments",
    "title": "\n30  Machine learning in practice\n",
    "section": "\n30.7 Visual assessments",
    "text": "30.7 Visual assessments\nAn important part of data analysis is visualizing results to determine why we are failing. How we do this depends on the application. Below we show the images of digits for which we made an incorrect prediction. Here are some errors for the random forest:\n\n\n\n\n\n\n\n\nBy examining errors like this we often find specific weaknesses to algorithms or parameter choices and can try to correct them."
  },
  {
    "objectID": "ml/ml-in-practice.html#ensembles",
    "href": "ml/ml-in-practice.html#ensembles",
    "title": "\n30  Machine learning in practice\n",
    "section": "\n30.8 Ensembles",
    "text": "30.8 Ensembles\nThe idea of an ensemble is similar to the idea of combining data from different pollsters to obtain a better estimate of the true support for each candidate.\nIn machine learning, one can usually greatly improve the final results by combining the results of different algorithms.\nHere is a simple example where we compute new class probabilities by taking the average of random forest and kNN. We can see that the accuracy improves to 0.96:\n\np_rf &lt;- predict(fit_rf, x_test[,col_index], type = \"prob\")  \np_rf &lt;- p_rf / rowSums(p_rf)\np_knn  &lt;- predict(fit_knn, x_test[,col_index])\np &lt;- (p_rf + p_knn)/2\ny_pred &lt;- factor(apply(p, 1, which.max) - 1)\nconfusionMatrix(y_pred, y_test)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.954\n\nIn the exercises we are going to build several machine learning models for the mnist_27 dataset and then build an ensemble."
  },
  {
    "objectID": "ml/ml-in-practice.html#exercises",
    "href": "ml/ml-in-practice.html#exercises",
    "title": "\n30  Machine learning in practice\n",
    "section": "\n30.9 Exercises",
    "text": "30.9 Exercises\n1. Previously in the book, we have compared conditional probability give two predictors \\(p(x_1,x_2)\\) to the fit \\(\\hat{p}(x_1,x_2)\\) obtained with a machine learning algorithm by making image plots. The following code can be used to make these images and include a curve at the values of \\(x_1\\) and \\(x_2\\) for which the function is \\(0.5\\).\n\nplot_cond_prob &lt;- function(x_1, x_2, p){\n  data.frame(x_1 = x_1, x_2 = x_2, p = p) |&gt;\n    ggplot(aes(x_1, x_2)) +\n    geom_raster(aes(fill = p), show.legend = FALSE) +\n    stat_contour(aes(z = p), breaks = 0.5, color = \"black\") +\n    scale_fill_gradientn(colors = c(\"#F8766D\", \"white\", \"#00BFC4\"))\n}\n\nWe can see the true conditional probability for the 2 or 7 example like this:\n\nwith(mnist_27$true_p, plot_cond_prob(x_1, x_2, p))\n\nFit a kNN model and make this plot for the estimated conditional probability. Hint: Use the argument newdata=mnist27$train to obtain predictions for a grid points.\n2. Notice that, in the plot made in exercise 1, the boundary is somewhat wiggly. This is because kNN, like the basic bin smoother, does not use a kernel. To improve this we could try loess. By reading through the available models part of the manual4 we see that we can use the gamLoess method. In the manual5 we also see that we need to install the gam package if we have not done so already. We see that we have two parameters to optimize:\n\nmodelLookup(\"gamLoess\")\n#&gt;      model parameter  label forReg forClass probModel\n#&gt; 1 gamLoess      span   Span   TRUE     TRUE      TRUE\n#&gt; 2 gamLoess    degree Degree   TRUE     TRUE      TRUE\n\nUse cross-validation to pick a span between 0.15 and 0.75. Keep degree = 1. What span does cross-validation select?\n3. Show an image plot of the estimate \\(\\hat{p}(x,y)\\) resulting from the model fit in the exercise 2. How does the accuracy compare to that of kNN? Comment on the difference between the estimate obtained with kNN.\n4. Use the mnist_27 training set to build a model with several of the models available from the caret package. For example, you can try these:\n\nmodels &lt;- c(\"glm\", \"lda\",  \"naive_bayes\",  \"svmLinear\", \"gamboost\",  \n            \"gamLoess\", \"qda\", \"knn\", \"kknn\", \"loclda\", \"gam\", \"rf\", \n            \"ranger\",\"wsrf\", \"Rborist\", \"avNNet\", \"mlp\", \"monmlp\", \"gbm\", \n            \"adaboost\", \"svmRadial\", \"svmRadialCost\", \"svmRadialSigma\")\n\nWe have not explained many of these, but apply them anyway using train with all the default parameters. Keep the results in a list. You might need to install some packages. Keep in mind that you will likely get some warnings.\n5. Now that you have all the trained models in a list, use sapply or map to create a matrix of predictions for the test set. You should end up with a matrix with length(mnist_27$test$y) rows and length(models) columns.\n6. Now compute accuracy for each model on the test set.\n7. Now build an ensemble prediction by majority vote and compute its accuracy.\n8. Earlier we computed the accuracy of each method on the training set and noticed they varied. Which individual methods do better than the ensemble?\n9. It is tempting to remove the methods that do not perform well and re-do the ensemble. The problem with this approach is that we are using the test data to make a decision. However, we could use the accuracy estimates obtained from cross validation with the training data. Obtain these estimates and save them in an object.\n10. Now let’s only consider the methods with an estimated accuracy of 0.8 when constructing the ensemble. What is the accuracy now?\n11. Advanced: If two methods give results that are the same, ensembling them will not change the results at all. For each pair of metrics compare the percent of time they call the same thing. Then use the heatmap function to visualize the results. Hint: use the method = \"binary\" argument in the dist function.\n12. Advanced: Note that each method can also produce an estimated conditional probability. Instead of majority vote we can take the average of these estimated conditional probabilities. For most methods, we can the use the type = \"prob\" in the train function. However, some of the methods require you to use the argument trControl=trainControl(classProbs=TRUE) when calling train. Also these methods do not work if classes have numbers as names. Hint: change the levels like this:\n\ndat$train$y &lt;- recode_factor(dat$train$y, \"2\"=\"two\", \"7\"=\"seven\")\ndat$test$y &lt;- recode_factor(dat$test$y, \"2\"=\"two\", \"7\"=\"seven\")\n\n13. In this chapter, we illustrated a couple of machine learning algorithms on a subset of the MNIST dataset. Try fitting a model to the entire dataset."
  },
  {
    "objectID": "ml/ml-in-practice.html#footnotes",
    "href": "ml/ml-in-practice.html#footnotes",
    "title": "\n30  Machine learning in practice\n",
    "section": "",
    "text": "https://topepo.github.io/caret/available-models.html↩︎\nhttp://topepo.github.io/caret/available-models.html↩︎\nhttps://topepo.github.io/caret/available-models.html↩︎\nhttps://topepo.github.io/caret/available-models.html↩︎\nhttps://topepo.github.io/caret/train-models-by-tag.html↩︎"
  },
  {
    "objectID": "ml/clustering.html#hierarchical-clustering",
    "href": "ml/clustering.html#hierarchical-clustering",
    "title": "31  Clustering",
    "section": "\n31.1 Hierarchical clustering",
    "text": "31.1 Hierarchical clustering\nWith the distance between each pair of movies computed, we need an algorithm to define groups from these. Hierarchical clustering starts by defining each observation as a separate group, then the two closest groups are joined into a group iteratively until there is just one group including all the observations. The hclust function implements this algorithm and it takes a distance as input.\n\nh &lt;- hclust(d)\n\nWe can see the resulting groups using a dendrogram.\n\nplot(h, cex = 0.65, main = \"\", xlab = \"\")\n\n\n\n\n\n\n\n\n\nThis graph gives us an approximation between the distance between any two movies. To find this distance we find the first location, from top to bottom, where these movies split into two different groups. The height of this location is the distance between these two groups. So, for example, the distance between the three Star Wars movies is 8 or less, while the distance between Raiders of the Lost of Ark and Silence of the Lambs is about 17.\nTo generate actual groups we can do one of two things: 1) decide on a minimum distance needed for observations to be in the same group or 2) decide on the number of groups you want and then find the minimum distance that achieves this. The function cutree can be applied to the output of hclust to perform either of these two operations and generate groups.\n\ngroups &lt;- cutree(h, k = 10)\n\nNote that the clustering provides some insights into types of movies. Group 4 appears to be blockbusters:\n\nnames(groups)[groups == 4]\n#&gt; [1] \"Braveheart\"        \"Godfather, The\"    \"Good Will Hunting\"\n\nAnd group 9 appears to be nerd movies:\n\nnames(groups)[groups == 9]\n#&gt; [1] \"True Lies\"            \"Fugitive, The\"        \"Groundhog Day\"       \n#&gt; [4] \"Men in Black (a.k...\"\n\nWe can change the size of the group by either making k larger or h smaller. We can also explore the data to see if there are clusters of movie raters.\n\nh_2 &lt;- dist(t(x)) |&gt; hclust()"
  },
  {
    "objectID": "ml/clustering.html#k-means",
    "href": "ml/clustering.html#k-means",
    "title": "31  Clustering",
    "section": "\n31.2 k-means",
    "text": "31.2 k-means\nTo use the k-means clustering algorithm we have to pre-define \\(k\\), the number of clusters we want to define. The k-means algorithm is iterative. The first step is to define \\(k\\) centers. Then each observation is assigned to the cluster with the closest center to that observation. In a second step the centers are redefined using the observation in each cluster: the column means are used to define a centroid. We repeat these two steps until the centers converge.\nThe kmeans function included in R-base does not handle NAs. For illustrative purposes we will fill out the NAs with 0s. In general, the choice of how to fill in missing data, or if one should do it at all, should be made with care.\n\nx_0 &lt;- x\nx_0[is.na(x_0)] &lt;- 0\nk &lt;- kmeans(x_0, centers = 10)\n\nThe cluster assignments are in the cluster component:\n\ngroups &lt;- k$cluster\n\nNote that because the first center is chosen at random, the final clusters are random. We impose some stability by repeating the entire function several times and averaging the results. The number of random starting values to use can be assigned through the nstart argument.\n\nk &lt;- kmeans(x_0, centers = 10, nstart = 25)"
  },
  {
    "objectID": "ml/clustering.html#heatmaps",
    "href": "ml/clustering.html#heatmaps",
    "title": "31  Clustering",
    "section": "\n31.3 Heatmaps",
    "text": "31.3 Heatmaps\nA powerful visualization tool for discovering clusters or patterns in your data is the heatmap. The idea is simple: plot an image of your data matrix with colors used as the visual cue and both the columns and rows ordered according to the results of a clustering algorithm. We will demonstrate this with the tissue_gene_expression dataset. We will scale the rows of the gene expression matrix.\nThe first step is compute:\n\nx &lt;- sweep(tissue_gene_expression$x, 2, colMeans(tissue_gene_expression$x))\nh_1 &lt;- hclust(dist(x))\nh_2 &lt;- hclust(dist(t(x)))\n\nNow we can use the results of this clustering to order the rows and columns.\n\nimage(x[h_1$order, h_2$order])\n\nBut there is heatmap function that does it for us:\n\nheatmap(x, col = RColorBrewer::brewer.pal(11, \"Spectral\"))\n\nWe do not show the results of the heatmap function because there are too many features for the plot to be useful. We will therefore filter some columns and remake the plots."
  },
  {
    "objectID": "ml/clustering.html#filtering-features",
    "href": "ml/clustering.html#filtering-features",
    "title": "31  Clustering",
    "section": "\n31.4 Filtering features",
    "text": "31.4 Filtering features\nIf the information about clusters is included in just a few features, including all the features can add enough noise that detecting clusters becomes challenging. One simple approach to try to remove features with no information is to only include those with high variance. In the movie example, a user with low variance in their ratings is not really informative: all the movies seem about the same to them. Here is an example of how we can include only the features with high variance.\n\nlibrary(matrixStats)\nsds &lt;- colSds(x, na.rm = TRUE)\no &lt;- order(sds, decreasing = TRUE)[1:25]\nheatmap(x[,o], col = RColorBrewer::brewer.pal(11, \"Spectral\"))"
  },
  {
    "objectID": "ml/clustering.html#exercises",
    "href": "ml/clustering.html#exercises",
    "title": "31  Clustering",
    "section": "\n31.5 Exercises",
    "text": "31.5 Exercises\n1. Load the tissue_gene_expression dataset. Remove the row means and compute the distance between each observation. Store the result in d.\n2. Make a hierarchical clustering plot and add the tissue types as labels.\n3. Run a k-means clustering on the data with \\(K=7\\). Make a table comparing the identified clusters to the actual tissue types. Run the algorithm several times to see how the answer changes.\n4. Select the 50 most variable genes. Make sure the observations show up in the columns, that the predictors are centered, and add a color bar to show the different tissue types. Hint: use the ColSideColors argument to assign colors. Also, use col = RColorBrewer::brewer.pal(11, \"RdBu\") for a better use of colors."
  },
  {
    "objectID": "highdim/dimension-reduction.html#examples",
    "href": "highdim/dimension-reduction.html#examples",
    "title": "\n21  Dimension reduction\n",
    "section": "\n21.6 Examples",
    "text": "21.6 Examples\n\n21.6.1 Iris example\nThe iris data is a widely used example in data analysis courses. It includes four botanical measurements related to three flower species:\n\nnames(iris)\n#&gt; [1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\" \n#&gt; [5] \"Species\"\n\nIf you print iris$Species you will see that the data is ordered by the species.\nIf we visualize the distances we can clearly see the three species with one species very different from the other two:\n\nx &lt;- iris[,1:4] |&gt; as.matrix()\nd &lt;- dist(x)\nimage(as.matrix(d), col = rev(RColorBrewer::brewer.pal(9, \"RdBu\")))\n\n\n\n\n\n\n\n\n\nOur features matrix has four dimensions, but three are very correlated:\n\ncor(x)\n#&gt;              Sepal.Length Sepal.Width Petal.Length Petal.Width\n#&gt; Sepal.Length        1.000      -0.118        0.872       0.818\n#&gt; Sepal.Width        -0.118       1.000       -0.428      -0.366\n#&gt; Petal.Length        0.872      -0.428        1.000       0.963\n#&gt; Petal.Width         0.818      -0.366        0.963       1.000\n\nIf we apply PCA, we should be able to approximate this distance with just two dimensions, compressing the highly correlated dimensions. Using the summary function we can see the variability explained by each PC:\n\npca &lt;- prcomp(x)\nsummary(pca)\n#&gt; Importance of components:\n#&gt;                          PC1    PC2    PC3     PC4\n#&gt; Standard deviation     2.056 0.4926 0.2797 0.15439\n#&gt; Proportion of Variance 0.925 0.0531 0.0171 0.00521\n#&gt; Cumulative Proportion  0.925 0.9777 0.9948 1.00000\n\nThe first two dimensions account for almot 98% of the variability. Thus we should be able to approximate the distance very well with two dimensions. We confirm this by computing the distance from first two dimensions and comparing to the original:\n\nd_approx &lt;- dist(pca$x[, 1:2])\nplot(d, d_approx); abline(0, 1, col = \"red\")\n\n\n\n\n\n\n\n\n\nA useful application of this result is that we can now visualize the distance between each observation with a two-dimensional plot.\n\ndata.frame(pca$x[,1:2], Species = iris$Species) |&gt;\n  ggplot(aes(PC1, PC2, fill = Species)) +\n  geom_point(cex = 3, pch = 21) +\n  coord_fixed(ratio = 1)\n\n\n\n\n\n\n\nWe color the the observation by their labels and notice that with these two dimensions we achieve almost perfect separation.\nLooking more closely at the resulting PCs and rotaions:\n\n\n\n\n\n\n\n\nwe learn that the first PC is obtained by taking a weighted avarge of sepal length, petal length, and petal width, since these are red in first column, and subtracts a weighted sepal width, since this is blue. The second PC is a weighted average of weighted average of petal length and petal width minus a weighted average of sepal length and petal width.\n\n21.6.2 MNIST example\nThe written digits example has 784 features. Is there any room for data reduction? We will use PCA to answer this.\nLet’s load the data if not already loaded:\n\nlibrary(dslabs)\nif (!exists(\"mnist\")) mnist &lt;- read_mnist()\n\nBecause the pixels are so small, we expect pixels close to each other on the grid to be correlated, meaning that dimension reduction should be possible.\nLet’s compute the PCs. This will take a few seconds as it is a rather large matrix.\n\npca &lt;- prcomp(mnist$train$images)\n\n\n\n\n\n\n\n\n\n\nplot(pca$sdev^2/sum(pca$sdev^2), xlab = \"PC\", ylab = \"Variance explained\")\n\nWe can see that the first few PCs already explain a large percent of the variability:\nAnd just by looking at the first two PCs we already see information about the labels. Here is a random sample of 500 digits:\n\ndata.frame(PC1 = pca$x[,1], PC2 = pca$x[,2], label = factor(mnist$train$label)) |&gt;\n  sample_n(500) |&gt;\n  ggplot(aes(PC1, PC2, fill = label)) +\n  geom_point(cex = 3, pch = 21)\n\n\n\n\n\n\n\nWe can also see the linear combinations on the grid to get an idea of how pixels are getting to compute the first four principal components:\n\n\n\n\n\n\n\n\nWe can clearly see that first PC appears to be separating the 1s (red) from the 0s (blue). We can kind of make out numbers in the other three PCs as well. By looking at the PCs stratified by digit we get futher insights. For example, we see that the second PC separates 4s, 7s, and 9s from the rest:\n\n\n\n\n\n\n\n\nWe can also confirm that the lower variance PCs appear related to unimportant variability, mainly smudges in the corners:"
  },
  {
    "objectID": "highdim/matrices-in-R.html#dimensions-of-a-matrix",
    "href": "highdim/matrices-in-R.html#dimensions-of-a-matrix",
    "title": "\n19  Matrices in R\n",
    "section": "\n19.2 Dimensions of a matrix",
    "text": "19.2 Dimensions of a matrix\nThe dimension of a matrix isan important characteristic needed to assure that certain linear algebra operations can be performed. The dimension, is a two-number summary defined as the number of rows \\(\\times\\) the number of columns.\nThe nrow function tells us how how many rows tha matrix has:\n\nnrow(x)\n#&gt; [1] 60000\n\nand ncol tells us how many columns:\n\nncol(x)\n#&gt; [1] 784\n\nWe learn that our dataset contains 60,000 observations (images) and 784 features (pixels).\nThe dim function returns the rows and columns:\n\ndim(x)\n#&gt; [1] 60000   784"
  },
  {
    "objectID": "highdim/matrices-in-R.html#creating-a-a-matrix",
    "href": "highdim/matrices-in-R.html#creating-a-a-matrix",
    "title": "\n19  Matrices in R\n",
    "section": "\n19.3 Creating a a matrix",
    "text": "19.3 Creating a a matrix\nIn R we can create a matrix using the matrix function. The first argument is a vector containing the elements that will fill up the matrix. The second and third arguments determine the number of row and columns, respectively. So a typical way to create a matrix is to first obtain a vector of numbers containing the elements of the matrix and feeding it to the matrix function. For example, to create a \\(100 \\times 2\\) matrix of normally distributed random variables we write:\n\nz &lt;- matrix(rnorm(100*2), 100, 2)\n\nNote that by default the matrix is filled in column by column:\n\nmatrix(1:15, 3, 5)\n#&gt;      [,1] [,2] [,3] [,4] [,5]\n#&gt; [1,]    1    4    7   10   13\n#&gt; [2,]    2    5    8   11   14\n#&gt; [3,]    3    6    9   12   15\n\nTo fill the matrix row by row we can use byrow argument:\n\nmatrix(1:15, 3, 5, byrow = TRUE)\n#&gt;      [,1] [,2] [,3] [,4] [,5]\n#&gt; [1,]    1    2    3    4    5\n#&gt; [2,]    6    7    8    9   10\n#&gt; [3,]   11   12   13   14   15\n\nThe function as.vector converts a matrix back into a vector:\n\nas.vector(matrix(1:15, 3, 5))\n#&gt;  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n\n\nIf the product of columns and rows does not match the length of the vector provided in the first argument, matrix recycles values. If the length of the vector is a sub-multiple or multiple of the number of rows this happens without warning:\n\nmatrix(1:3, 3, 5)\n#&gt;      [,1] [,2] [,3] [,4] [,5]\n#&gt; [1,]    1    1    1    1    1\n#&gt; [2,]    2    2    2    2    2\n#&gt; [3,]    3    3    3    3    3"
  },
  {
    "objectID": "highdim/matrices-in-R.html#subsetting",
    "href": "highdim/matrices-in-R.html#subsetting",
    "title": "\n19  Matrices in R\n",
    "section": "\n19.4 Subsetting",
    "text": "19.4 Subsetting\nWe can extract a specific entry from matrix, for example the 300th row and 100th column, we use write:\n\nx[300,100]\n\nWe can extract subsets of the matrices by using vectors of indexes. As an example, we can extract the first 100 pixles from the first 300 observations like this: and rows like this:\n\nx[1:300,1:100]\n\nTo extract an entire row or subset of rows, we leave the column dimension blank. So the following code returns all the pixes for the first 300 observations:\n\nx[1:300,]\n\nSimilarly, we can subset any number of columns by keeping the first dimension blank. Here is the code to extract the first 100 pixels:\n\nx[,1:100]\n\n\n\n\n\n\n\nIf we subset just one row or just one column, the resulting object is no longer a matrix. Here is an example:\n\ndim(x[300,])\n#&gt; NULL\n\nTo avoid this we can use the drop argument:\n\ndim(x[100,,drop = FALSE])\n#&gt; [1]   1 784\n\n\n\n\nTask 1: Visualize the original image\nAs an example, let’s try to visualize the third observation. From the label we know this is a:\n\nmnist$train$label[3]\n#&gt; [1] 4\n\nThe third row of the matrix x[3,] contains the 784 pixels intesities. We can assume these were entered in order and convert them back to a \\(28 \\times 28\\) matrix using:\n\ngrid &lt;- matrix(x[3,], 28, 28)\n\nTo visualize the data we can use image, which shows an image of its third argument, with the first two arguments to determine the position on the x and y axes, respectively. Because the top of this plot is pixel 1, which is shown at the bottom, the image is flipped. To code below includes code showing how to flip it back:\n\nimage(1:28, 1:28, grid)\nimage(1:28, 1:28, grid[, 28:1])"
  },
  {
    "objectID": "highdim/matrices-in-R.html#the-transpose",
    "href": "highdim/matrices-in-R.html#the-transpose",
    "title": "\n19  Matrices in R\n",
    "section": "\n19.6 The transpose",
    "text": "19.6 The transpose\nA common operation when working with matrices is the transpose. We use the transpose to understand several concepts described in the next several sections. This operation simply converts the rows of a matrix into columns. We use the symbols \\(\\top\\) or \\('\\) next to the bold upper case letter to denote the transpose:\n\\[\n\\text{if } \\,\n\\mathbf{X} =\n\\begin{bmatrix}\n  x_{1,1}&\\dots & x_{1,p} \\\\\n  x_{2,1}&\\dots & x_{2,p} \\\\\n  \\vdots & \\ddots & \\vdots & \\\\\n  x_{n,1}&\\dots & x_{n,p}\n  \\end{bmatrix} \\text{ then }\\,\n\\mathbf{X}^\\top =\n\\begin{bmatrix}\n  x_{1,1}&x_{2,1}&\\dots & x_{n,1} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  x_{1,p}&x_{2,p}&\\dots & x_{n,p}\n  \\end{bmatrix}\n\\]\nIn R we compute the transpose using the function t\n\ndim(x)\n#&gt; [1] 60000   784\ndim(t(x))\n#&gt; [1]   784 60000\n\nOne use of the transpose is that we can write the matrix \\(\\mathbf{X}\\) as rows of the column vectors representing the features for each individual observation in the following way:\n\\[\n\\mathbf{X} =\n\\begin{bmatrix}\n\\mathbf{x}_1^\\top\\\\\n\\mathbf{x}_2^\\top\\\\\n\\vdots\\\\\n\\mathbf{x}_n^\\top\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "highdim/matrices-in-R.html#conditional-filtering",
    "href": "highdim/matrices-in-R.html#conditional-filtering",
    "title": "\n19  Matrices in R\n",
    "section": "\n19.8 Conditional filtering",
    "text": "19.8 Conditional filtering\nOne of the advantages of matrices operations over tidyverse operations, is that we can easily select columns based on summaries of the columns.\nNote that logical filters can be used to subset matrices in a similar way in which they can be used to subset vectors. Here is a simple examples subsetting columns with logicals:\n\nmatrix(1:15, 3, 5)[,c(FALSE, TRUE, TRUE, FALSE, TRUE)]\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    4    7   13\n#&gt; [2,]    5    8   14\n#&gt; [3,]    6    9   15\n\nThis implies that we can select rows with conditional expression. Here is practical example that removes all observations containing at least one NA:\n\nx[apply(!is.na(x), 1, all),]\n\nThis being a common operation, we have a matrixStats function to do it faster:\n\nx[!rowAnyNAs(x),]\n\nTask 3: Are some pixels uninformative?\nWe can use these ideas to remove columns associated with pixels that don’t change much and thus do not informing digit classification. We will quantify the variation of each pixel with its standard deviation across all entries. Since each column represents a pixel, we use the colSds function from the matrixStats package:\n\nsds &lt;- colSds(x)\n\nA quick look at the distribution of these values shows that some pixels have very low entry to entry variability:\n\n\n\n\n\n\n\n\n\nhist(sds, breaks = 30, main = \"SDs\")\n\nThis makes sense since we don’t write in some parts of the box. Here is the variance plotted by location:\n\nimage(1:28, 1:28, matrix(sds, 28, 28)[, 28:1])\n\n\n\n\n\n\n\n\n\nWe see that there is little variation in the corners.\nWe could remove features that have no variation since these can’t help us predict.\nSo if we wanted to remove uninformative predictors from our matrix, we could write this one line of code:\n\nnew_x &lt;- x[,colSds(x) &gt; 60]\ndim(new_x)\n#&gt; [1] 60000   322\n\nOnly the columns for which the standard deviation is above 60 are kept, which removes over half the predictors."
  }
]